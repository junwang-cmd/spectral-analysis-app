# ====================================
# å¯¼å…¥å¿…è¦çš„åº“
# ====================================

import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import base64
import traceback
import time
from io import BytesIO
from sklearn.decomposition import PCA, NMF
from sklearn.cluster import KMeans, AgglomerativeClustering
from sklearn.ensemble import IsolationForest
from sklearn.neighbors import LocalOutlierFactor
from sklearn.metrics import silhouette_score
from scipy.spatial.distance import mahalanobis

# æœºå™¨å­¦ä¹ ç›¸å…³
from sklearn.model_selection import train_test_split, KFold, LeaveOneOut, cross_val_score
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.decomposition import PCA
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.svm import SVR
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.neural_network import MLPRegressor
from sklearn.cross_decomposition import PLSRegression
from sklearn.multioutput import MultiOutputRegressor
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error

# ç‰¹å¾é€‰æ‹©ç›¸å…³
from sklearn.feature_selection import (
    VarianceThreshold, SelectKBest, f_regression, mutual_info_regression,
    RFE, SelectFromModel
)

# ä¿¡å·å¤„ç†ç›¸å…³
from scipy.signal import savgol_filter
from scipy import sparse
from scipy.sparse.linalg import spsolve
# éœ€è¦é¢å¤–å¯¼å…¥çš„åº“
from sklearn.decomposition import PCA, FastICA, NMF
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
from sklearn.manifold import TSNE
from scipy.signal import find_peaks
from scipy.stats import pearsonr, spearmanr
from sklearn.preprocessing import MinMaxScaler
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import seaborn as sns
from scipy import stats
from datetime import datetime, timedelta
# è®¾ç½®matplotlibä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False



# ====================================
# 1. å…¨å±€é…ç½®å’Œå¸¸é‡
# ====================================

MODEL_NAMES = {
    'linear': 'çº¿æ€§å›å½’',
    'ridge': 'å²­å›å½’', 
    'lasso': 'Lassoå›å½’',
    'svr': 'æ”¯æŒå‘é‡å›å½’',
    'rf': 'éšæœºæ£®æ—',
    'gbr': 'æ¢¯åº¦æå‡å›å½’',
    'mlp': 'å¤šå±‚æ„ŸçŸ¥æœº',
    'pls': 'åæœ€å°äºŒä¹˜å›å½’',
    'xgb': 'XGBoost'
}


# ====================================
# 2. å·¥å…·å‡½æ•°
# ====================================

def set_page_style():
    """è®¾ç½®é¡µé¢æ ·å¼"""
    st.markdown("""
    <style>
    .main {
        padding-top: 1rem;
    }
    .section-header {
        font-size: 2rem;
        font-weight: bold;
        color: #1f77b4;
        margin-bottom: 1rem;
        text-align: center;
        border-bottom: 2px solid #1f77b4;
        padding-bottom: 0.5rem;
    }
    .info-box {
        background-color: #e7f3ff;
        border-left: 5px solid #1f77b4;
        padding: 1rem;
        margin: 1rem 0;
        border-radius: 5px;
    }
    .metric-card {
        background-color: #f0f2f6;
        padding: 1rem;
        border-radius: 10px;
        border: 1px solid #ddd;
        margin: 0.5rem 0;
    }
    </style>
    """, unsafe_allow_html=True)


def init_session_state():
    """åˆå§‹åŒ–ä¼šè¯çŠ¶æ€"""
    if 'data_loaded' not in st.session_state:
        st.session_state.data_loaded = False
    if 'preprocessing_done' not in st.session_state:
        st.session_state.preprocessing_done = False
    if 'feature_selected' not in st.session_state:
        st.session_state.feature_selected = False


def check_data_prerequisites(need_labels=False, need_preprocessing=True):
    """
    æ£€æŸ¥æ•°æ®å‰ç½®æ¡ä»¶
    
    Args:
        need_labels: æ˜¯å¦éœ€è¦æ ‡ç­¾æ•°æ®
        need_preprocessing: æ˜¯å¦éœ€è¦é¢„å¤„ç†å®Œæˆ
    
    Returns:
        bool: æ˜¯å¦æ»¡è¶³æ¡ä»¶
    """
    # æ£€æŸ¥æ•°æ®æ˜¯å¦åŠ è½½
    if not hasattr(st.session_state, 'data_loaded') or not st.session_state.data_loaded:
        show_status_message("è¯·å…ˆåŠ è½½æ•°æ®", "warning")
        return False
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦æ ‡ç­¾æ•°æ®
    if need_labels:
        if not hasattr(st.session_state, 'y') or st.session_state.y is None:
            show_status_message("æ­¤åŠŸèƒ½éœ€è¦æ ‡ç­¾æ•°æ®ï¼Œè¯·åœ¨æ•°æ®åŠ è½½é¡µé¢è¾“å…¥æ ‡ç­¾", "warning")
            return False
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦é¢„å¤„ç†å®Œæˆ
    if need_preprocessing:
        if not hasattr(st.session_state, 'preprocessing_done') or not st.session_state.preprocessing_done:
            show_status_message("è¯·å…ˆå®Œæˆæ•°æ®é¢„å¤„ç†", "warning")
            return False
    
    return True


def get_current_data():
    """
    è·å–å½“å‰ä½¿ç”¨çš„æ•°æ®
    
    Returns:
        tuple: (X, wavenumbers, info_message)
    """
    if hasattr(st.session_state, 'feature_selected') and st.session_state.feature_selected:
        X = st.session_state.X_final
        wavenumbers = st.session_state.wavenumbers_final
        info = f"âœ… ä½¿ç”¨ç‰¹å¾é€‰æ‹©åçš„æ•°æ®ï¼Œç‰¹å¾æ•°é‡: {X.shape[1]}"
    elif hasattr(st.session_state, 'preprocessing_done') and st.session_state.preprocessing_done:
        X = st.session_state.X_preprocessed
        wavenumbers = st.session_state.wavenumbers_preprocessed
        info = f"â„¹ï¸ ä½¿ç”¨é¢„å¤„ç†åçš„å…¨éƒ¨ç‰¹å¾ï¼Œç‰¹å¾æ•°é‡: {X.shape[1]}"
    else:
        X = st.session_state.X
        wavenumbers = st.session_state.wavenumbers
        info = f"âš ï¸ ä½¿ç”¨åŸå§‹æ•°æ®ï¼Œç‰¹å¾æ•°é‡: {X.shape[1]}"
    
    return X, wavenumbers, info


def show_status_message(message, message_type="info"):
    """
    æ˜¾ç¤ºçŠ¶æ€æ¶ˆæ¯
    
    Args:
        message: æ¶ˆæ¯å†…å®¹
        message_type: æ¶ˆæ¯ç±»å‹ ("info", "success", "warning", "error")
    """
    if message_type == "info":
        st.info(message)
    elif message_type == "success":
        st.success(message)
    elif message_type == "warning":
        st.warning(message)
    elif message_type == "error":
        st.error(message)
    else:
        st.write(message)


def safe_execute(func, error_message="æ“ä½œå¤±è´¥"):
    """
    å®‰å…¨æ‰§è¡Œå‡½æ•°ï¼Œæ•è·å¼‚å¸¸
    
    Args:
        func: è¦æ‰§è¡Œçš„å‡½æ•°
        error_message: é”™è¯¯æ¶ˆæ¯
    
    Returns:
        å‡½æ•°æ‰§è¡Œç»“æœæˆ–None
    """
    try:
        return func()
    except Exception as e:
        st.error(f"{error_message}: {str(e)}")
        with st.expander("æŸ¥çœ‹è¯¦ç»†é”™è¯¯ä¿¡æ¯"):
            st.code(traceback.format_exc())
        return None


# ====================================
# 3. åŸºçº¿æ ¡æ­£ç±»
# ====================================
class SpectrumBaselineCorrector:
    """å…‰è°±åŸºçº¿æ ¡æ­£å™¨ - å®Œæ•´å®ç°"""
    
    def __init__(self):
        self.available_methods = {
            'als': self.als_baseline,
            'arPLS': self.arpls_baseline,
            'polynomial': self.polynomial_baseline,
            'linear': self.linear_baseline,
            'rolling_ball': self.rolling_ball_baseline
        }
    
    def correct_baseline(self, spectrum, method='als', **params):
        """
        æ‰§è¡ŒåŸºçº¿æ ¡æ­£
        
        Args:
            spectrum: å…‰è°±æ•°æ® (1D array)
            method: æ ¡æ­£æ–¹æ³• ('als', 'arPLS', 'polynomial', 'linear', 'rolling_ball')
            **params: æ–¹æ³•ç‰¹å®šå‚æ•°
        
        Returns:
            tuple: (baseline, corrected_spectrum)
        """
        # æ•°æ®é¢„æ£€æŸ¥
        spectrum = self._validate_spectrum(spectrum)
        
        if method not in self.available_methods:
            raise ValueError(f"ä¸æ”¯æŒçš„åŸºçº¿æ ¡æ­£æ–¹æ³•: {method}")
        
        try:
            baseline = self.available_methods[method](spectrum, **params)
            corrected = spectrum - baseline
            return baseline, corrected
        except Exception as e:
            # è¯¦ç»†é”™è¯¯ä¿¡æ¯
            print(f"åŸºçº¿æ ¡æ­£å¤±è´¥ - æ–¹æ³•: {method}, é”™è¯¯: {str(e)}")
            print(f"å…‰è°±å½¢çŠ¶: {spectrum.shape}")
            print(f"å…‰è°±èŒƒå›´: [{np.min(spectrum):.3f}, {np.max(spectrum):.3f}]")
            print(f"æ˜¯å¦åŒ…å«NaN: {np.isnan(spectrum).any()}")
            print(f"æ˜¯å¦åŒ…å«æ— ç©·å€¼: {np.isinf(spectrum).any()}")
            raise e
    
    def _validate_spectrum(self, spectrum):
        """éªŒè¯å’Œæ¸…ç†å…‰è°±æ•°æ®"""
        spectrum = np.asarray(spectrum, dtype=float)
        
        # æ£€æŸ¥ç»´åº¦
        if spectrum.ndim != 1:
            raise ValueError(f"å…‰è°±å¿…é¡»æ˜¯1Dæ•°ç»„ï¼Œå½“å‰ç»´åº¦: {spectrum.ndim}")
        
        # æ£€æŸ¥é•¿åº¦
        if len(spectrum) < 10:
            raise ValueError(f"å…‰è°±æ•°æ®ç‚¹å¤ªå°‘: {len(spectrum)}")
        
        # å¤„ç†NaNå’Œæ— ç©·å€¼
        if np.isnan(spectrum).any():
            print("è­¦å‘Š: å…‰è°±åŒ…å«NaNå€¼ï¼Œå°†ç”¨é‚»è¿‘å€¼å¡«å……")
            spectrum = self._interpolate_nan(spectrum)
        
        if np.isinf(spectrum).any():
            print("è­¦å‘Š: å…‰è°±åŒ…å«æ— ç©·å€¼ï¼Œå°†ç”¨æœ‰é™å€¼æ›¿æ¢")
            spectrum[np.isinf(spectrum)] = np.nanmedian(spectrum[np.isfinite(spectrum)])
        
        return spectrum
    
    def _interpolate_nan(self, spectrum):
        """æ’å€¼å¡«å……NaNå€¼"""
        mask = np.isfinite(spectrum)
        if not mask.any():
            raise ValueError("å…‰è°±æ•°æ®å…¨éƒ¨ä¸ºNaNæˆ–æ— ç©·å€¼")
        
        indices = np.arange(len(spectrum))
        spectrum[~mask] = np.interp(indices[~mask], indices[mask], spectrum[mask])
        return spectrum
    
    def als_baseline(self, spectrum, lam=1e5, p=0.01, niter=10):
        """
        éå¯¹ç§°æœ€å°äºŒä¹˜åŸºçº¿æ ¡æ­£ (Asymmetric Least Squares)
        
        Args:
            spectrum: å…‰è°±æ•°æ®
            lam: å¹³æ»‘å‚æ•° (è¶Šå¤§è¶Šå¹³æ»‘)
            p: éå¯¹ç§°å‚æ•° (0-1, è¶Šå°è¶Šåå‘è°·åº•)
            niter: è¿­ä»£æ¬¡æ•°
        """
        spectrum = np.asarray(spectrum, dtype=float)
        L = len(spectrum)
        
        # æ„å»ºå·®åˆ†çŸ©é˜µ
        D = sparse.diags([1, -2, 1], [0, -1, -2], shape=(L, L-2))
        D = lam * D.dot(D.transpose())
        
        # åˆå§‹åŒ–æƒé‡
        w = np.ones(L)
        W = sparse.spdiags(w, 0, L, L)
        
        for i in range(niter):
            try:
                # æ±‚è§£åŸºçº¿
                W.setdiag(w)
                Z = W + D
                baseline = spsolve(Z, w * spectrum)
                
                # æ›´æ–°æƒé‡
                w = p * (spectrum > baseline) + (1 - p) * (spectrum < baseline)
                
            except Exception as e:
                print(f"ALSè¿­ä»£ {i+1} å¤±è´¥: {e}")
                if i == 0:
                    # å¦‚æœç¬¬ä¸€æ¬¡è¿­ä»£å°±å¤±è´¥ï¼Œè¿”å›ç®€å•åŸºçº¿
                    return self.linear_baseline(spectrum)
                else:
                    # ä½¿ç”¨ä¸Šä¸€æ¬¡çš„ç»“æœ
                    break
        
        return baseline
    
    def arpls_baseline(self, spectrum, lam=1e5, ratio=0.01, niter=10):
        """
        éå¯¹ç§°é‡åŠ æƒæƒ©ç½šæœ€å°äºŒä¹˜ - æ•°å€¼ç¨³å®šç‰ˆæœ¬
        """
        try:
            spectrum = np.asarray(spectrum, dtype=float).flatten()
            L = len(spectrum)
            
            # éªŒè¯è¾“å…¥
            if L < 3:
                print("å…‰è°±é•¿åº¦å¤ªçŸ­ï¼Œä½¿ç”¨çº¿æ€§åŸºçº¿")
                return self.linear_baseline(spectrum)
            
            # æ„å»ºäºŒé˜¶å·®åˆ†çŸ©é˜µ
            diags = np.ones(L-2)
            D1 = sparse.spdiags([-diags, 2*diags, -diags], [0, 1, 2], L-2, L)
            D2 = D1.T @ D1
            
            # åˆå§‹åŒ–
            w = np.ones(L)
            baseline = spectrum.copy()
            
            for i in range(niter):
                try:
                    # æ„å»ºæƒé‡çŸ©é˜µ
                    W = sparse.diags(w, format='csr')
                    
                    # æ„å»ºç³»ç»ŸçŸ©é˜µ
                    A = W + lam * D2
                    
                    # å³ä¾§å‘é‡
                    b = W @ spectrum
                    
                    # æ±‚è§£çº¿æ€§ç³»ç»Ÿ
                    try:
                        baseline_new = spsolve(A, b)
                    except:
                        A_dense = A.toarray()
                        baseline_new = np.linalg.solve(A_dense, b)
                    
                    # ç¡®ä¿ç»“æœæ˜¯ä¸€ç»´æ•°ç»„
                    baseline_new = np.asarray(baseline_new).flatten()
                    
                    # è®¡ç®—æ®‹å·®
                    residual = spectrum - baseline_new
                    negative_residual = residual[residual < 0]
                    
                    if len(negative_residual) == 0:
                        baseline = baseline_new
                        break
                    
                    # è®¡ç®—ç»Ÿè®¡é‡
                    mean_neg = np.mean(negative_residual)
                    std_neg = np.std(negative_residual)
                    
                    if std_neg == 0:
                        baseline = baseline_new
                        break
                    
                    # â­ æ•°å€¼ç¨³å®šçš„æƒé‡æ›´æ–° - ä¿®å¤æº¢å‡ºé—®é¢˜ â­
                    threshold = 2 * std_neg - mean_neg
                    exp_arg = 2.0 * (residual - threshold) / std_neg
                    
                    # é™åˆ¶æŒ‡æ•°å‚æ•°èŒƒå›´ï¼Œé¿å…æº¢å‡º
                    exp_arg = np.clip(exp_arg, -50, 50)  # é™åˆ¶åœ¨åˆç†èŒƒå›´å†…
                    
                    # ä½¿ç”¨æ•°å€¼ç¨³å®šçš„sigmoidå‡½æ•°
                    w_new = np.where(exp_arg > 0, 
                                    1.0 / (1.0 + np.exp(-exp_arg)),  # æ­£æ•°æƒ…å†µ
                                    np.exp(exp_arg) / (1.0 + np.exp(exp_arg)))  # è´Ÿæ•°æƒ…å†µ
                    
                    # æ£€æŸ¥æ”¶æ•›
                    weight_change = np.linalg.norm(w_new - w) / (np.linalg.norm(w) + 1e-10)
                    if weight_change < ratio:
                        baseline = baseline_new
                        break
                    
                    # é˜»å°¼æ›´æ–°ï¼Œæé«˜ç¨³å®šæ€§
                    w = 0.7 * w + 0.3 * w_new
                    baseline = baseline_new
                    
                except Exception as iter_error:
                    print(f"arPLSè¿­ä»£ {i+1} å¤±è´¥: {iter_error}")
                    if i == 0:
                        print("arPLSå¤±è´¥ï¼Œåˆ‡æ¢åˆ°ALSæ–¹æ³•")
                        return self.als_baseline(spectrum, lam=lam, p=0.01, niter=niter)
                    else:
                        break
            
            return baseline
            
        except Exception as e:
            print(f"arPLSå®Œå…¨å¤±è´¥: {e}")
            print("è‡ªåŠ¨åˆ‡æ¢åˆ°ALSæ–¹æ³•")
            return self.als_baseline(spectrum, lam=lam, p=0.01, niter=niter)
    
    def polynomial_baseline(self, spectrum, degree=2):
        """å¤šé¡¹å¼åŸºçº¿æ ¡æ­£"""
        try:
            x = np.arange(len(spectrum))
            coeffs = np.polyfit(x, spectrum, degree)
            baseline = np.polyval(coeffs, x)
            return baseline
        except Exception as e:
            print(f"å¤šé¡¹å¼åŸºçº¿æ ¡æ­£å¤±è´¥: {e}")
            return self.linear_baseline(spectrum)
    
    def linear_baseline(self, spectrum):
        """çº¿æ€§åŸºçº¿æ ¡æ­£"""
        x = np.arange(len(spectrum))
        slope = (spectrum[-1] - spectrum[0]) / (len(spectrum) - 1)
        baseline = spectrum[0] + slope * x
        return baseline
    
    def rolling_ball_baseline(self, spectrum, radius=100):
        """æ»šçƒåŸºçº¿æ ¡æ­£"""
        try:
            from scipy.ndimage import minimum_filter1d
            
            # ä½¿ç”¨æœ€å°å€¼æ»¤æ³¢å™¨æ¨¡æ‹Ÿæ»šçƒ
            baseline = minimum_filter1d(spectrum, size=radius*2+1, mode='mirror')
            
            # å¹³æ»‘åŸºçº¿
            baseline = savgol_filter(baseline, min(len(baseline)//10*2+1, 51), 3)
            
            return baseline
        except Exception as e:
            print(f"æ»šçƒåŸºçº¿æ ¡æ­£å¤±è´¥: {e}")
            return self.linear_baseline(spectrum)


# åŸºçº¿æ ¡æ­£é—®é¢˜è¯Šæ–­å‡½æ•°
def diagnose_baseline_correction_issue(spectrum_data, wavenumbers, method='als', **params):
    """
    è¯Šæ–­åŸºçº¿æ ¡æ­£é—®é¢˜
    
    Args:
        spectrum_data: å…‰è°±æ•°æ®çŸ©é˜µ (n_samples x n_features)
        wavenumbers: æ³¢æ•°æ•°ç»„
        method: åŸºçº¿æ ¡æ­£æ–¹æ³•
        **params: æ–¹æ³•å‚æ•°
    """
    print("=== åŸºçº¿æ ¡æ­£é—®é¢˜è¯Šæ–­ ===")
    
    corrector = SpectrumBaselineCorrector()
    
    # æ£€æŸ¥ç¬¬ä¸€ä¸ªæ ·æœ¬
    first_spectrum = spectrum_data[0]
    
    print(f"æ ·æœ¬å½¢çŠ¶: {spectrum_data.shape}")
    print(f"ç¬¬ä¸€ä¸ªå…‰è°±å½¢çŠ¶: {first_spectrum.shape}")
    print(f"å…‰è°±æ•°å€¼èŒƒå›´: [{np.min(first_spectrum):.3f}, {np.max(first_spectrum):.3f}]")
    print(f"æ˜¯å¦åŒ…å«NaN: {np.isnan(first_spectrum).any()}")
    print(f"æ˜¯å¦åŒ…å«æ— ç©·å€¼: {np.isinf(first_spectrum).any()}")
    print(f"æ³¢æ•°èŒƒå›´: [{np.min(wavenumbers):.1f}, {np.max(wavenumbers):.1f}]")
    
    # å°è¯•æ ¡æ­£ç¬¬ä¸€ä¸ªæ ·æœ¬
    try:
        baseline, corrected = corrector.correct_baseline(first_spectrum, method, **params)
        print("âœ… ç¬¬ä¸€ä¸ªæ ·æœ¬åŸºçº¿æ ¡æ­£æˆåŠŸ")
        
        # å¯è§†åŒ–ç»“æœ
        plt.figure(figsize=(12, 8))
        
        plt.subplot(2, 2, 1)
        plt.plot(wavenumbers, first_spectrum, 'b-', label='åŸå§‹å…‰è°±')
        plt.plot(wavenumbers, baseline, 'r--', label='åŸºçº¿')
        plt.xlabel('æ³¢æ•° (cmâ»Â¹)')
        plt.ylabel('å¼ºåº¦')
        plt.title('åŸå§‹å…‰è°±ä¸åŸºçº¿')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        plt.subplot(2, 2, 2)
        plt.plot(wavenumbers, corrected, 'g-', label='æ ¡æ­£åå…‰è°±')
        plt.xlabel('æ³¢æ•° (cmâ»Â¹)')
        plt.ylabel('å¼ºåº¦')
        plt.title('åŸºçº¿æ ¡æ­£åå…‰è°±')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        plt.subplot(2, 2, 3)
        plt.hist(first_spectrum, bins=30, alpha=0.7, label='åŸå§‹')
        plt.hist(corrected, bins=30, alpha=0.7, label='æ ¡æ­£å')
        plt.xlabel('å¼ºåº¦')
        plt.ylabel('é¢‘æ•°')
        plt.title('å¼ºåº¦åˆ†å¸ƒ')
        plt.legend()
        
        plt.subplot(2, 2, 4)
        residuals = first_spectrum - baseline - corrected
        plt.plot(wavenumbers, residuals, 'k-')
        plt.xlabel('æ³¢æ•° (cmâ»Â¹)')
        plt.ylabel('æ®‹å·®')
        plt.title('æ‹Ÿåˆæ®‹å·®')
        plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()
        
        return True, baseline, corrected
        
    except Exception as e:
        print(f"âŒ ç¬¬ä¸€ä¸ªæ ·æœ¬åŸºçº¿æ ¡æ­£å¤±è´¥: {str(e)}")
        print(f"é”™è¯¯ç±»å‹: {type(e).__name__}")
        
        # æä¾›è§£å†³å»ºè®®
        print("\n=== è§£å†³å»ºè®® ===")
        
        if np.isnan(first_spectrum).any():
            print("1. å…‰è°±åŒ…å«NaNå€¼ï¼Œå»ºè®®é¢„å¤„ç†æ—¶æ£€æŸ¥æ•°æ®è´¨é‡")
        
        if np.isinf(first_spectrum).any():
            print("2. å…‰è°±åŒ…å«æ— ç©·å€¼ï¼Œå»ºè®®æ£€æŸ¥æ•°æ®é¢„å¤„ç†æ­¥éª¤")
        
        if len(first_spectrum) < 100:
            print("3. å…‰è°±æ•°æ®ç‚¹å¤ªå°‘ï¼Œå¯èƒ½å½±å“åŸºçº¿æ ¡æ­£æ•ˆæœ")
        
        if method == 'als' and 'lam' in params:
            if params['lam'] > 1e7:
                print("4. ALSæ–¹æ³•çš„lambdaå‚æ•°å¤ªå¤§ï¼Œå»ºè®®é™ä½åˆ°1e5-1e6")
        
        print("5. å»ºè®®å°è¯•æ›´ç®€å•çš„åŸºçº¿æ ¡æ­£æ–¹æ³•ï¼Œå¦‚'linear'æˆ–'polynomial'")
        print("6. æ£€æŸ¥å…‰è°±æ•°æ®çš„ç‰©ç†åˆç†æ€§")
        
        return False, None, None


# ä¿®å¤å»ºè®®çš„å‚æ•°è®¾ç½®
def get_robust_baseline_params():
    """è·å–ç¨³å¥çš„åŸºçº¿æ ¡æ­£å‚æ•°"""
    return {
        'als': {'lam': 1e5, 'p': 0.01, 'niter': 10},
        'arPLS': {'lam': 1e5, 'ratio': 0.01, 'niter': 10}, 
        'polynomial': {'degree': 2},
        'linear': {},
        'rolling_ball': {'radius': 50}
    }


# åœ¨Streamlitåº”ç”¨ä¸­çš„ä½¿ç”¨ç¤ºä¾‹
def safe_baseline_correction(X, method='als', **params):
    """
    å®‰å…¨çš„åŸºçº¿æ ¡æ­£ï¼Œå¸¦é”™è¯¯å¤„ç†å’Œé™çº§ç­–ç•¥
    """
    corrector = SpectrumBaselineCorrector()
    X_corrected = np.zeros_like(X)
    failed_samples = []
    
    for i in range(X.shape[0]):
        try:
            baseline, corrected = corrector.correct_baseline(X[i], method, **params)
            X_corrected[i] = corrected
        except Exception as e:
            # è®°å½•å¤±è´¥çš„æ ·æœ¬
            failed_samples.append(i+1)
            
            # é™çº§ç­–ç•¥ï¼šä½¿ç”¨çº¿æ€§åŸºçº¿æ ¡æ­£
            try:
                baseline, corrected = corrector.correct_baseline(X[i], 'linear')
                X_corrected[i] = corrected
                print(f"æ ·æœ¬ {i+1}: {method}æ ¡æ­£å¤±è´¥ï¼Œæ”¹ç”¨çº¿æ€§æ ¡æ­£")
            except Exception as e2:
                # æœ€åçš„é™çº§ï¼šä¸æ ¡æ­£
                X_corrected[i] = X[i]
                print(f"æ ·æœ¬ {i+1}: æ‰€æœ‰åŸºçº¿æ ¡æ­£æ–¹æ³•å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æ•°æ®")
    
    if failed_samples:
        print(f"åŸºçº¿æ ¡æ­£å¤±è´¥çš„æ ·æœ¬: {failed_samples}")
        print("å»ºè®®æ£€æŸ¥è¿™äº›æ ·æœ¬çš„æ•°æ®è´¨é‡")
    
    return X_corrected, failed_samples



# ====================================
# 4. æ¨¡å‹ç›¸å…³å‡½æ•°
# ====================================

def create_model_instance(model_name, params, is_multioutput):
    """
    åˆ›å»ºæ¨¡å‹å®ä¾‹
    
    Args:
        model_name: æ¨¡å‹åç§°
        params: æ¨¡å‹å‚æ•°
        is_multioutput: æ˜¯å¦ä¸ºå¤šè¾“å‡ºé—®é¢˜
        
    Returns:
        tuple: (model, use_scaler)
    """
    use_scaler = False
    
    if model_name == 'linear':
        model = LinearRegression()
        use_scaler = True
    elif model_name == 'ridge':
        model = Ridge(alpha=params['alpha'], random_state=params.get('random_state', 42))
        use_scaler = True
    elif model_name == 'lasso':
        model = Lasso(alpha=params['alpha'], random_state=params.get('random_state', 42))
        use_scaler = True
    elif model_name == 'svr':
        base_model = SVR(**params)
        model = MultiOutputRegressor(base_model) if is_multioutput else base_model
        use_scaler = True
    elif model_name == 'rf':
        model = RandomForestRegressor(**params)
    elif model_name == 'gbr':
        base_model = GradientBoostingRegressor(**params)
        model = MultiOutputRegressor(base_model) if is_multioutput else base_model
    elif model_name == 'mlp':
        model = MLPRegressor(**params)
        use_scaler = True
    elif model_name == 'pls':
        model = PLSRegression(**params)
    elif model_name == 'xgb':
        try:
            import xgboost as xgb
            base_model = xgb.XGBRegressor(**params)
            model = MultiOutputRegressor(base_model) if is_multioutput else base_model
        except ImportError:
            raise ImportError("XGBoostæœªå®‰è£…ï¼Œè¯·å…ˆå®‰è£…: pip install xgboost")
    else:
        raise ValueError(f"æœªçŸ¥çš„æ¨¡å‹ç±»å‹: {model_name}")
    
    return model, use_scaler


def setup_model_parameters_ui(model_name, index):
    """
    è®¾ç½®æ¨¡å‹å‚æ•°UI
    
    Args:
        model_name: æ¨¡å‹åç§°
        index: ç´¢å¼•ï¼ˆç”¨äºåŒºåˆ†ä¸åŒçš„UIç»„ä»¶ï¼‰
        
    Returns:
        dict: æ¨¡å‹å‚æ•°å­—å…¸
    """
    if model_name == 'linear':
        return {}  # çº¿æ€§å›å½’æ— å‚æ•°
    elif model_name == 'ridge':
        return setup_ridge_params(index)
    elif model_name == 'lasso':
        return setup_lasso_params(index)
    elif model_name == 'svr':
        return setup_svr_params(index)
    elif model_name == 'rf':
        return setup_rf_params(index)
    elif model_name == 'gbr':
        return setup_gbr_params(index)
    elif model_name == 'mlp':
        return setup_mlp_params(index)
    elif model_name == 'pls':
        return setup_pls_params(index)
    elif model_name == 'xgb':
        return setup_xgb_params(index)
    else:
        return {}


def setup_ridge_params(index):
    """å²­å›å½’å‚æ•°è®¾ç½®"""
    alpha = st.selectbox(
        "æ­£åˆ™åŒ–å¼ºåº¦", 
        [0.1, 1.0, 10.0, 100.0], 
        index=1, 
        key=f"ridge_alpha_{index}"
    )
    random_state = st.number_input(
        "éšæœºç§å­", 
        value=42, 
        key=f"ridge_seed_{index}"
    )
    
    return {
        'alpha': alpha,
        'random_state': random_state
    }


def setup_lasso_params(index):
    """Lassoå›å½’å‚æ•°è®¾ç½®"""
    alpha = st.selectbox(
        "æ­£åˆ™åŒ–å¼ºåº¦", 
        [0.01, 0.1, 1.0, 10.0], 
        index=1, 
        key=f"lasso_alpha_{index}"
    )
    random_state = st.number_input(
        "éšæœºç§å­", 
        value=42, 
        key=f"lasso_seed_{index}"
    )
    
    return {
        'alpha': alpha,
        'random_state': random_state
    }


def setup_svr_params(index):
    """æ”¯æŒå‘é‡å›å½’å‚æ•°è®¾ç½®"""
    col1, col2 = st.columns(2)
    
    with col1:
        C = st.selectbox(
            "æƒ©ç½šå‚æ•°C", 
            [0.1, 1.0, 10.0, 100.0], 
            index=2, 
            key=f"svr_C_{index}"
        )
        kernel = st.selectbox(
            "æ ¸å‡½æ•°", 
            ['rbf', 'linear', 'poly'], 
            index=0, 
            key=f"svr_kernel_{index}"
        )
    
    with col2:
        gamma = st.selectbox(
            "Gamma", 
            ['scale', 'auto', 0.001, 0.01, 0.1], 
            index=0, 
            key=f"svr_gamma_{index}"
        )
        epsilon = st.selectbox(
            "Epsilon", 
            [0.01, 0.1, 0.2], 
            index=1, 
            key=f"svr_epsilon_{index}"
        )
    
    return {
        'C': C,
        'kernel': kernel,
        'gamma': gamma,
        'epsilon': epsilon
    }


def setup_rf_params(index):
    """éšæœºæ£®æ—å‚æ•°è®¾ç½®"""
    col1, col2 = st.columns(2)
    
    with col1:
        n_estimators = st.slider(
            "æ ‘çš„æ•°é‡", 
            50, 500, 100, 
            key=f"rf_trees_{index}"
        )
        max_depth = st.selectbox(
            "æœ€å¤§æ·±åº¦", 
            [None, 5, 10, 15, 20], 
            index=0, 
            key=f"rf_depth_{index}"
        )
    
    with col2:
        min_samples_split = st.slider(
            "åˆ†è£‚æœ€å°æ ·æœ¬æ•°", 
            2, 10, 2, 
            key=f"rf_split_{index}"
        )
        min_samples_leaf = st.slider(
            "å¶èŠ‚ç‚¹æœ€å°æ ·æœ¬æ•°", 
            1, 5, 1, 
            key=f"rf_leaf_{index}"
        )
    
    random_state = st.number_input(
        "éšæœºç§å­", 
        value=42, 
        key=f"rf_seed_{index}"
    )
    
    return {
        'n_estimators': n_estimators,
        'max_depth': max_depth,
        'min_samples_split': min_samples_split,
        'min_samples_leaf': min_samples_leaf,
        'random_state': random_state
    }


def setup_gbr_params(index):
    """æ¢¯åº¦æå‡å‚æ•°è®¾ç½®"""
    col1, col2 = st.columns(2)
    
    with col1:
        n_estimators = st.slider(
            "æå‡é˜¶æ®µæ•°", 
            50, 500, 100, 
            key=f"gbr_stages_{index}"
        )
        learning_rate = st.selectbox(
            "å­¦ä¹ ç‡", 
            [0.01, 0.05, 0.1, 0.2], 
            index=2, 
            key=f"gbr_lr_{index}"
        )
    
    with col2:
        max_depth = st.slider(
            "æœ€å¤§æ·±åº¦", 
            2, 10, 3, 
            key=f"gbr_depth_{index}"
        )
        subsample = st.slider(
            "å­é‡‡æ ·æ¯”ä¾‹", 
            0.5, 1.0, 1.0, 
            step=0.1, 
            key=f"gbr_subsample_{index}"
        )
    
    random_state = st.number_input(
        "éšæœºç§å­", 
        value=42, 
        key=f"gbr_seed_{index}"
    )
    
    return {
        'n_estimators': n_estimators,
        'learning_rate': learning_rate,
        'max_depth': max_depth,
        'subsample': subsample,
        'random_state': random_state
    }


def setup_pls_params(index):
    """PLSå‚æ•°è®¾ç½®"""
    n_components = st.slider(
        "ä¸»æˆåˆ†æ•°é‡", 
        1, min(20, st.session_state.X_train.shape[1]), 
        5, 
        key=f"pls_components_{index}"
    )
    scale = st.checkbox(
        "æ ‡å‡†åŒ–", 
        value=True, 
        key=f"pls_scale_{index}"
    )
    
    return {
        'n_components': n_components,
        'scale': scale
    }


def setup_mlp_params(index):
    """MLPå‚æ•°è®¾ç½®"""
    col1, col2 = st.columns(2)
    
    with col1:
        layer_option = st.selectbox(
            "éšè—å±‚ç»“æ„", 
            ["ä¸€å±‚", "ä¸¤å±‚", "ä¸‰å±‚"], 
            index=1, 
            key=f"mlp_layers_{index}"
        )
        
        if layer_option == "ä¸€å±‚":
            layer1_size = st.slider(
                "éšè—å±‚ç¥ç»å…ƒæ•°", 
                10, 200, 50, 
                key=f"mlp_l1_{index}"
            )
            hidden_layer_sizes = (layer1_size,)
        elif layer_option == "ä¸¤å±‚":
            layer1_size = st.slider(
                "ç¬¬ä¸€å±‚ç¥ç»å…ƒæ•°", 
                10, 200, 100, 
                key=f"mlp_l1_{index}"
            )
            layer2_size = st.slider(
                "ç¬¬äºŒå±‚ç¥ç»å…ƒæ•°", 
                10, 100, 50, 
                key=f"mlp_l2_{index}"
            )
            hidden_layer_sizes = (layer1_size, layer2_size)
        else:  # ä¸‰å±‚
            layer1_size = st.slider(
                "ç¬¬ä¸€å±‚ç¥ç»å…ƒæ•°", 
                10, 200, 100, 
                key=f"mlp_l1_{index}"
            )
            layer2_size = st.slider(
                "ç¬¬äºŒå±‚ç¥ç»å…ƒæ•°", 
                10, 100, 50, 
                key=f"mlp_l2_{index}"
            )
            layer3_size = st.slider(
                "ç¬¬ä¸‰å±‚ç¥ç»å…ƒæ•°", 
                10, 50, 25, 
                key=f"mlp_l3_{index}"
            )
            hidden_layer_sizes = (layer1_size, layer2_size, layer3_size)
        
        activation = st.selectbox(
            "æ¿€æ´»å‡½æ•°", 
            ['relu', 'tanh', 'logistic'], 
            index=0, 
            key=f"mlp_activation_{index}"
        )
    
    with col2:
        solver = st.selectbox(
            "ä¼˜åŒ–ç®—æ³•", 
            ['adam', 'lbfgs', 'sgd'], 
            index=0, 
            key=f"mlp_solver_{index}"
        )
        learning_rate_init = st.selectbox(
            "åˆå§‹å­¦ä¹ ç‡", 
            [0.0001, 0.001, 0.01], 
            index=1, 
            key=f"mlp_lr_{index}"
        )
        max_iter = st.slider(
            "æœ€å¤§è¿­ä»£æ¬¡æ•°", 
            100, 1000, 500, 
            key=f"mlp_iter_{index}"
        )
        alpha = st.selectbox(
            "L2æ­£åˆ™åŒ–å‚æ•°", 
            [0.0001, 0.001, 0.01], 
            index=0, 
            key=f"mlp_alpha_{index}"
        )
    
    random_state = st.number_input(
        "éšæœºç§å­", 
        value=42, 
        key=f"mlp_seed_{index}"
    )
    
    return {
        'hidden_layer_sizes': hidden_layer_sizes,
        'activation': activation,
        'solver': solver,
        'learning_rate_init': learning_rate_init,
        'max_iter': max_iter,
        'alpha': alpha,
        'random_state': random_state
    }


def setup_xgb_params(index):
    """XGBoostå‚æ•°è®¾ç½®"""
    col1, col2 = st.columns(2)
    
    with col1:
        n_estimators = st.slider(
            "æå‡è½®æ•°", 
            50, 500, 100, 
            key=f"xgb_trees_{index}"
        )
        learning_rate = st.selectbox(
            "å­¦ä¹ ç‡", 
            [0.01, 0.05, 0.1, 0.2], 
            index=2, 
            key=f"xgb_lr_{index}"
        )
        max_depth = st.slider(
            "æœ€å¤§æ·±åº¦", 
            2, 10, 6, 
            key=f"xgb_depth_{index}"
        )
    
    with col2:
        subsample = st.slider(
            "å­é‡‡æ ·æ¯”ä¾‹", 
            0.5, 1.0, 1.0, 
            step=0.1, 
            key=f"xgb_subsample_{index}"
        )
        colsample_bytree = st.slider(
            "ç‰¹å¾é‡‡æ ·æ¯”ä¾‹", 
            0.5, 1.0, 1.0, 
            step=0.1, 
            key=f"xgb_colsample_{index}"
        )
        reg_alpha = st.selectbox(
            "L1æ­£åˆ™åŒ–", 
            [0, 0.01, 0.1], 
            index=0, 
            key=f"xgb_alpha_{index}"
        )
    
    random_state = st.number_input(
        "éšæœºç§å­", 
        value=42, 
        key=f"xgb_seed_{index}"
    )
    
    return {
        'n_estimators': n_estimators,
        'learning_rate': learning_rate,
        'max_depth': max_depth,
        'subsample': subsample,
        'colsample_bytree': colsample_bytree,
        'reg_alpha': reg_alpha,
        'random_state': random_state
    }


# ====================================
# 5. é¡µé¢å‡½æ•°
# ====================================

def show_data_loading_page():
    """æ•°æ®åŠ è½½ä¸æ ‡ç­¾è¾“å…¥é¡µé¢"""
    st.markdown("<h1 class='section-header'>æ•°æ®åŠ è½½ä¸æ ‡ç­¾è¾“å…¥</h1>", unsafe_allow_html=True)
    
    st.markdown("""
    <div class="info-box">
    è¯·ä¸Šä¼ å…‰è°±æ•°æ®æ–‡ä»¶ï¼Œæ”¯æŒ CSV å’Œ Excel æ ¼å¼ã€‚ç³»ç»Ÿä¼šè‡ªåŠ¨è¯†åˆ«å…‰è°±æ•°æ®å’Œæ ‡ç­¾æ•°æ®ã€‚
    </div>
    """, unsafe_allow_html=True)
    
    # åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
    if 'data_format_confirmed' not in st.session_state:
        st.session_state.data_format_confirmed = False
    if 'label_setup_completed' not in st.session_state:
        st.session_state.label_setup_completed = False
    
    # æ–‡ä»¶ä¸Šä¼ 
    uploaded_file = st.file_uploader("é€‰æ‹©æ•°æ®æ–‡ä»¶", type=["csv", "xlsx", "xls"])
    
    if uploaded_file is not None:
        try:
            # è¯»å–æ•°æ®
            if uploaded_file.name.endswith('.csv'):
                df = pd.read_csv(uploaded_file)
            else:
                df = pd.read_excel(uploaded_file)
            
            # ä¿å­˜åŸå§‹æ•°æ®åˆ°ä¼šè¯çŠ¶æ€
            if 'uploaded_df' not in st.session_state or st.session_state.uploaded_df is None:
                st.session_state.uploaded_df = df
                st.session_state.data_format_confirmed = False
                st.session_state.label_setup_completed = False
            
            st.success(f"æ–‡ä»¶åŠ è½½æˆåŠŸï¼æ•°æ®å½¢çŠ¶: {df.shape}")
            
            # æ˜¾ç¤ºæ•°æ®é¢„è§ˆ
            st.subheader("ğŸ“Š æ•°æ®é¢„è§ˆ")
            st.dataframe(df.head(), use_container_width=True)
            
            # æ­¥éª¤1ï¼šæ•°æ®æ ¼å¼è®¾ç½®ï¼ˆåªåœ¨æœªç¡®è®¤æ—¶æ˜¾ç¤ºï¼‰
            if not st.session_state.data_format_confirmed:
                st.subheader("âš™ï¸ æ­¥éª¤1: æ•°æ®æ ¼å¼è®¾ç½®")
                
                col1, col2 = st.columns(2)
                
                with col1:
                    st.write("**é€‰æ‹©å…‰è°±æ•°æ®èµ·å§‹åˆ—ï¼š**")
                    start_col_options = list(range(1, min(df.shape[1], 11)))
                    
                    selected_start_col = st.selectbox(
                        "å…‰è°±æ³¢æ•°æ•°æ®ä»ç¬¬å‡ åˆ—å¼€å§‹ï¼Ÿ",
                        start_col_options,
                        index=2 if len(start_col_options) > 2 else 0,
                        format_func=lambda x: f"ç¬¬{x}åˆ— ({df.columns[x-1]})",
                        key="start_col_select"
                    )
                
                with col2:
                    st.write("**æ ‡ç­¾æ•°æ®è®¾ç½®ï¼š**")
                    has_labels = st.radio("æ•°æ®ä¸­æ˜¯å¦åŒ…å«æ ‡ç­¾ï¼ˆç›®æ ‡å˜é‡ï¼‰ï¼Ÿ", ["æ˜¯", "å¦"], key="has_labels_radio")
                
                # ç¡®è®¤æ•°æ®æ ¼å¼
                if st.button("ç¡®è®¤æ•°æ®æ ¼å¼", key="confirm_format_btn"):
                    with st.spinner("æ­£åœ¨å¤„ç†å…‰è°±æ•°æ®..."):
                        try:
                            # è¯†åˆ«æ³¢æ•°åˆ—
                            potential_wavenumbers = df.columns[selected_start_col-1:]
                            numeric_columns = []
                            
                            for col in potential_wavenumbers:
                                try:
                                    float(col)
                                    numeric_columns.append(col)
                                except ValueError:
                                    continue
                            
                            if len(numeric_columns) < 10:
                                st.error("æ£€æµ‹åˆ°çš„æ³¢æ•°åˆ—æ•°é‡ä¸è¶³ï¼Œè¯·æ£€æŸ¥æ•°æ®æ ¼å¼")
                                return
                            
                            # æå–å…‰è°±æ•°æ®å’Œæ³¢æ•°
                            wavenumbers = pd.Series(numeric_columns).astype(float)
                            X = df[numeric_columns].values.astype(float)
                            
                            # ä¿å­˜åˆ°ä¼šè¯çŠ¶æ€
                            st.session_state.X = X
                            st.session_state.wavenumbers = wavenumbers
                            st.session_state.original_df = df
                            st.session_state.numeric_columns = numeric_columns
                            st.session_state.has_labels_choice = has_labels
                            st.session_state.data_format_confirmed = True
                            
                            st.success(f"âœ… å…‰è°±æ•°æ®å¤„ç†æˆåŠŸï¼")
                            st.info(f"ğŸ“Š å…‰è°±æ•°æ®å½¢çŠ¶: {X.shape}")
                            st.info(f"ğŸ“ æ³¢æ•°èŒƒå›´: {wavenumbers.min():.1f} ~ {wavenumbers.max():.1f} cmâ»Â¹")
                            
                            # å¦‚æœé€‰æ‹©æ— æ ‡ç­¾ï¼Œç›´æ¥å®Œæˆè®¾ç½®
                            if has_labels == "å¦":
                                st.session_state.y = None
                                st.session_state.selected_cols = []
                                st.session_state.label_setup_completed = True
                                st.session_state.data_loaded = True
                                st.info("â„¹ï¸ æœªè®¾ç½®æ ‡ç­¾æ•°æ®ï¼Œå¯è¿›è¡Œè¶‹åŠ¿åˆ†æç­‰æ— ç›‘ç£åˆ†æ")
                                st.rerun()  # åˆ·æ–°é¡µé¢æ˜¾ç¤ºä¸‹ä¸€æ­¥
                            else:
                                st.info("ğŸ‘‡ è¯·ç»§ç»­è¿›è¡Œæ ‡ç­¾æ•°æ®è®¾ç½®")
                                st.rerun()  # åˆ·æ–°é¡µé¢æ˜¾ç¤ºæ ‡ç­¾è®¾ç½®
                            
                        except Exception as e:
                            st.error(f"æ•°æ®å¤„ç†å‡ºé”™: {e}")
                            st.error(traceback.format_exc())
            
            # æ­¥éª¤2ï¼šæ ‡ç­¾æ•°æ®è®¾ç½®ï¼ˆåªåœ¨æ ¼å¼ç¡®è®¤åä¸”é€‰æ‹©æœ‰æ ‡ç­¾æ—¶æ˜¾ç¤ºï¼‰
            elif st.session_state.data_format_confirmed and not st.session_state.label_setup_completed:
                if st.session_state.has_labels_choice == "æ˜¯":
                    st.subheader("ğŸ·ï¸ æ­¥éª¤2: æ ‡ç­¾æ•°æ®è®¾ç½®")
                    
                    # æ˜¾ç¤ºå…‰è°±æ•°æ®ä¿¡æ¯
                    st.info(f"âœ… å…‰è°±æ•°æ®å·²ç¡®è®¤ - å½¢çŠ¶: {st.session_state.X.shape}")
                    
                    # ä»éæ³¢æ•°åˆ—ä¸­é€‰æ‹©æ ‡ç­¾åˆ—
                    label_candidates = [col for col in df.columns if col not in st.session_state.numeric_columns]
                    
                    if label_candidates:
                        st.write("**å¯é€‰æ‹©çš„æ ‡ç­¾åˆ—ï¼š**")
                        
                        # æ˜¾ç¤ºå€™é€‰æ ‡ç­¾åˆ—çš„é¢„è§ˆ
                        preview_df = df[label_candidates].head()
                        st.dataframe(preview_df, use_container_width=True)
                        
                        selected_label_cols = st.multiselect(
                            "é€‰æ‹©æ ‡ç­¾åˆ—ï¼ˆç›®æ ‡å˜é‡ï¼‰",
                            label_candidates,
                            help="å¯ä»¥é€‰æ‹©å¤šä¸ªç›®æ ‡å˜é‡è¿›è¡Œå¤šè¾“å‡ºé¢„æµ‹",
                            key="label_cols_select"
                        )
                        
                        if selected_label_cols:
                            # æ˜¾ç¤ºé€‰ä¸­æ ‡ç­¾çš„ç»Ÿè®¡ä¿¡æ¯
                            st.write("**é€‰ä¸­æ ‡ç­¾çš„ç»Ÿè®¡ä¿¡æ¯ï¼š**")
                            label_stats = df[selected_label_cols].describe()
                            st.dataframe(label_stats, use_container_width=True)
                            
                            # ç¡®è®¤æ ‡ç­¾è®¾ç½®
                            if st.button("ç¡®è®¤æ ‡ç­¾è®¾ç½®", key="confirm_labels_btn"):
                                try:
                                    y = df[selected_label_cols].values
                                    if len(selected_label_cols) == 1:
                                        y = y.ravel()
                                    
                                    st.session_state.y = y
                                    st.session_state.selected_cols = selected_label_cols
                                    st.session_state.label_setup_completed = True
                                    st.session_state.data_loaded = True
                                    
                                    st.success(f"âœ… æ ‡ç­¾æ•°æ®è®¾ç½®æˆåŠŸï¼")
                                    st.info(f"ğŸ¯ ç›®æ ‡å˜é‡: {', '.join(selected_label_cols)}")
                                    st.info(f"ğŸ“Š æ ‡ç­¾æ•°æ®å½¢çŠ¶: {y.shape}")
                                    
                                    st.rerun()  # åˆ·æ–°é¡µé¢æ˜¾ç¤ºå®ŒæˆçŠ¶æ€
                                    
                                except Exception as e:
                                    st.error(f"æ ‡ç­¾æ•°æ®å¤„ç†å‡ºé”™: {e}")
                        else:
                            st.warning("è¯·é€‰æ‹©è‡³å°‘ä¸€ä¸ªæ ‡ç­¾åˆ—")
                            
                            # æä¾›è·³è¿‡é€‰é¡¹
                            if st.button("è·³è¿‡æ ‡ç­¾è®¾ç½®ï¼ˆä»…è¿›è¡Œæ— ç›‘ç£åˆ†æï¼‰", key="skip_labels_btn"):
                                st.session_state.y = None
                                st.session_state.selected_cols = []
                                st.session_state.label_setup_completed = True
                                st.session_state.data_loaded = True
                                st.info("â„¹ï¸ å·²è·³è¿‡æ ‡ç­¾è®¾ç½®ï¼Œå¯è¿›è¡Œè¶‹åŠ¿åˆ†æç­‰æ— ç›‘ç£åˆ†æ")
                                st.rerun()
                    else:
                        st.warning("æœªæ‰¾åˆ°å¯ç”¨çš„æ ‡ç­¾åˆ—ï¼Œæ‰€æœ‰åˆ—éƒ½è¢«è¯†åˆ«ä¸ºæ³¢æ•°æ•°æ®")
                        if st.button("ç¡®è®¤æ— æ ‡ç­¾æ•°æ®", key="confirm_no_labels_btn"):
                            st.session_state.y = None
                            st.session_state.selected_cols = []
                            st.session_state.label_setup_completed = True
                            st.session_state.data_loaded = True
                            st.info("â„¹ï¸ ç¡®è®¤æ— æ ‡ç­¾æ•°æ®ï¼Œå¯è¿›è¡Œè¶‹åŠ¿åˆ†æç­‰æ— ç›‘ç£åˆ†æ")
                            st.rerun()
            
            # æ­¥éª¤3ï¼šæ˜¾ç¤ºæœ€ç»ˆå®ŒæˆçŠ¶æ€
            elif st.session_state.data_format_confirmed and st.session_state.label_setup_completed:
                st.subheader("âœ… æ•°æ®åŠ è½½å®Œæˆ")
                
                # æ˜¾ç¤ºæ•°æ®æ‘˜è¦
                col1, col2 = st.columns(2)
                
                with col1:
                    st.success("**å…‰è°±æ•°æ®**")
                    st.info(f"ğŸ“Š æ•°æ®å½¢çŠ¶: {st.session_state.X.shape}")
                    st.info(f"ğŸ“ æ³¢æ•°èŒƒå›´: {st.session_state.wavenumbers.min():.1f} ~ {st.session_state.wavenumbers.max():.1f} cmâ»Â¹")
                
                with col2:
                    if st.session_state.y is not None:
                        st.success("**æ ‡ç­¾æ•°æ®**")
                        st.info(f"ğŸ¯ ç›®æ ‡å˜é‡: {', '.join(st.session_state.selected_cols)}")
                        st.info(f"ğŸ“Š æ ‡ç­¾å½¢çŠ¶: {st.session_state.y.shape}")
                    else:
                        st.info("**æ— æ ‡ç­¾æ•°æ®**")
                        st.info("ğŸ” é€‚ç”¨äºæ— ç›‘ç£åˆ†æ")
                
                # æä¾›é‡æ–°è®¾ç½®é€‰é¡¹
                if st.button("ğŸ”„ é‡æ–°è®¾ç½®æ•°æ®", key="reset_data_btn"):
                    # æ¸…é™¤ç›¸å…³ä¼šè¯çŠ¶æ€
                    keys_to_clear = [
                        'data_format_confirmed', 'label_setup_completed', 'data_loaded',
                        'X', 'y', 'wavenumbers', 'selected_cols', 'numeric_columns', 
                        'has_labels_choice', 'uploaded_df'
                    ]
                    for key in keys_to_clear:
                        if key in st.session_state:
                            del st.session_state[key]
                    st.rerun()
                
                # æ˜¾ç¤ºè¯¦ç»†ç»Ÿè®¡ä¿¡æ¯
                with st.expander("ğŸ“Š æŸ¥çœ‹è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯"):
                    if st.session_state.y is not None:
                        if len(st.session_state.selected_cols) == 1:
                            st.write("**æ ‡ç­¾ç»Ÿè®¡ä¿¡æ¯ï¼š**")
                            y = st.session_state.y
                            stats_df = pd.DataFrame({
                                'ç»Ÿè®¡é‡': ['æ ·æœ¬æ•°', 'å‡å€¼', 'æ ‡å‡†å·®', 'æœ€å°å€¼', 'æœ€å¤§å€¼'],
                                'æ•°å€¼': [
                                    f"{len(y)}",
                                    f"{np.mean(y):.4f}",
                                    f"{np.std(y):.4f}",
                                    f"{np.min(y):.4f}",
                                    f"{np.max(y):.4f}"
                                ]
                            })
                            st.dataframe(stats_df, use_container_width=True)
                        else:
                            st.write("**å¤šç›®æ ‡æ ‡ç­¾ç»Ÿè®¡ä¿¡æ¯ï¼š**")
                            stats_data = []
                            for i, col in enumerate(st.session_state.selected_cols):
                                stats_data.append({
                                    'ç›®æ ‡å˜é‡': col,
                                    'æ ·æœ¬æ•°': f"{st.session_state.y.shape[0]}",
                                    'å‡å€¼': f"{np.mean(st.session_state.y[:, i]):.4f}",
                                    'æ ‡å‡†å·®': f"{np.std(st.session_state.y[:, i]):.4f}",
                                    'æœ€å°å€¼': f"{np.min(st.session_state.y[:, i]):.4f}",
                                    'æœ€å¤§å€¼': f"{np.max(st.session_state.y[:, i]):.4f}"
                                })
                            stats_df = pd.DataFrame(stats_data)
                            st.dataframe(stats_df, use_container_width=True)
                    
                    # å…‰è°±æ•°æ®ç»Ÿè®¡
                    st.write("**å…‰è°±æ•°æ®ç»Ÿè®¡ï¼š**")
                    X = st.session_state.X
                    spectrum_stats = pd.DataFrame({
                        'ç»Ÿè®¡é‡': ['æ ·æœ¬æ•°', 'ç‰¹å¾æ•°', 'æ•°æ®èŒƒå›´æœ€å°å€¼', 'æ•°æ®èŒƒå›´æœ€å¤§å€¼', 'å¹³å‡å…‰è°±å¼ºåº¦'],
                        'æ•°å€¼': [
                            f"{X.shape[0]}",
                            f"{X.shape[1]}",
                            f"{X.min():.4f}",
                            f"{X.max():.4f}",
                            f"{np.mean(X):.4f}"
                        ]
                    })
                    st.dataframe(spectrum_stats, use_container_width=True)
        
        except Exception as e:
            st.error(f"æ–‡ä»¶è¯»å–å¤±è´¥: {e}")
    
    else:
        st.info("è¯·ä¸Šä¼ æ•°æ®æ–‡ä»¶å¼€å§‹åˆ†æ")
        
        # æ˜¾ç¤ºä½¿ç”¨è¯´æ˜
        st.subheader("ğŸ“‹ ä½¿ç”¨è¯´æ˜")
        st.markdown("""
        **æ”¯æŒçš„æ–‡ä»¶æ ¼å¼ï¼š**
        - CSV æ–‡ä»¶ (.csv)
        - Excel æ–‡ä»¶ (.xlsx, .xls)
        
        **æ•°æ®æ ¼å¼è¦æ±‚ï¼š**
        1. æ¯è¡Œä»£è¡¨ä¸€ä¸ªæ ·æœ¬
        2. å‰å‡ åˆ—å¯ä»¥æ˜¯æ ·æœ¬æ ‡è¯†ä¿¡æ¯
        3. å…‰è°±æ•°æ®åˆ—çš„åˆ—ååº”ä¸ºæ³¢æ•°å€¼ï¼ˆå¦‚ 4000, 3999.5, 3999, ...ï¼‰
        4. å¦‚æœæœ‰æ ‡ç­¾æ•°æ®ï¼Œåº”åœ¨éæ³¢æ•°åˆ—ä¸­
        
        **æ•°æ®åŠ è½½æµç¨‹ï¼š**
        1. ä¸Šä¼ æ–‡ä»¶å¹¶é¢„è§ˆæ•°æ®
        2. è®¾ç½®å…‰è°±æ•°æ®èµ·å§‹åˆ—å’Œæ ‡ç­¾é€‰é¡¹
        3. å¦‚æœ‰æ ‡ç­¾ï¼Œé€‰æ‹©å…·ä½“çš„æ ‡ç­¾åˆ—
        4. å®Œæˆæ•°æ®åŠ è½½è®¾ç½®
        
        **æ ‡ç­¾æ•°æ®ï¼š**
        - æœ‰æ ‡ç­¾ï¼šé€‚ç”¨äºå®šé‡é¢„æµ‹å»ºæ¨¡
        - æ— æ ‡ç­¾ï¼šé€‚ç”¨äºè¶‹åŠ¿åˆ†æã€PCAåˆ†æç­‰æ¢ç´¢æ€§åˆ†æ
        """)


def show_preprocessing_page():
    """æ•°æ®é¢„å¤„ç†é¡µé¢ - é€‚é…å®Œæ•´åŸºçº¿æ ¡æ­£å™¨"""
    st.markdown("<h1 class='section-header'>æ•°æ®é¢„å¤„ç†</h1>", unsafe_allow_html=True)
    
    # æ£€æŸ¥å‰ç½®æ¡ä»¶
    if not check_data_prerequisites(need_labels=False, need_preprocessing=False):
        return
    
    st.markdown("""
    <div class="info-box">
    å¯¹å…‰è°±æ•°æ®è¿›è¡Œé¢„å¤„ç†ï¼ŒåŒ…æ‹¬æ³¢æ•°æˆªå–ã€å¹³æ»‘ã€åŸºçº¿æ ¡æ­£ã€å½’ä¸€åŒ–ç­‰æ­¥éª¤ã€‚
    </div>
    """, unsafe_allow_html=True)
    
    X = st.session_state.X
    wavenumbers = st.session_state.wavenumbers
    
    st.info(f"ğŸ“Š åŸå§‹æ•°æ®å½¢çŠ¶: {X.shape}")
    st.info(f"ğŸ“ æ³¢æ•°èŒƒå›´: {wavenumbers.min():.1f} ~ {wavenumbers.max():.1f} cmâ»Â¹")
    
    # é¢„å¤„ç†å‚æ•°è®¾ç½®
    st.subheader("âš™ï¸ é¢„å¤„ç†å‚æ•°è®¾ç½®")
    
    # 1. æ³¢æ•°æˆªå–
    st.write("**1. æ³¢æ•°èŒƒå›´æˆªå–**")
    col1, col2 = st.columns(2)
    
    with col1:
        start_wavenumber = st.number_input(
            "èµ·å§‹æ³¢æ•° (cmâ»Â¹)", 
            min_value=float(wavenumbers.min()),
            max_value=float(wavenumbers.max()),
            value=float(wavenumbers.min()),
            step=0.5
        )
    
    with col2:
        end_wavenumber = st.number_input(
            "ç»“æŸæ³¢æ•° (cmâ»Â¹)", 
            min_value=float(wavenumbers.min()),
            max_value=float(wavenumbers.max()),
            value=float(wavenumbers.max()),
            step=0.5
        )
    
    # 2. å¹³æ»‘å¤„ç†
    st.write("**2. Savitzky-Golay å¹³æ»‘**")
    apply_smooth = st.checkbox("å¯ç”¨å¹³æ»‘å¤„ç†", value=True)
    
    if apply_smooth:
        col1, col2 = st.columns(2)
        with col1:
            smooth_window = st.slider("çª—å£å¤§å°", 5, 21, 9, step=2)
        with col2:
            smooth_poly = st.slider("å¤šé¡¹å¼é˜¶æ•°", 1, 5, 3)
    
    # 3. åŸºçº¿æ ¡æ­£ - æ›´æ–°ä¸ºæ”¯æŒæ–°çš„æ–¹æ³•
    st.write("**3. åŸºçº¿æ ¡æ­£**")
    apply_baseline = st.checkbox("å¯ç”¨åŸºçº¿æ ¡æ­£", value=True)
    
    if apply_baseline:
        baseline_method = st.selectbox(
            "åŸºçº¿æ ¡æ­£æ–¹æ³•", 
            ['als', 'arPLS', 'polynomial', 'linear', 'rolling_ball'],
            format_func=lambda x: {
                'als': 'ALS (éå¯¹ç§°æœ€å°äºŒä¹˜)',
                'arPLS': 'arPLS (éå¯¹ç§°é‡åŠ æƒæƒ©ç½šæœ€å°äºŒä¹˜)',
                'polynomial': 'å¤šé¡¹å¼æ‹Ÿåˆ',
                'linear': 'çº¿æ€§åŸºçº¿',
                'rolling_ball': 'æ»šçƒåŸºçº¿æ ¡æ­£'
            }[x]
        )
        
        # åŸºçº¿æ ¡æ­£å‚æ•° - æ ¹æ®ä¸åŒæ–¹æ³•æ˜¾ç¤ºä¸åŒå‚æ•°
        baseline_params = {}
        
        if baseline_method == 'als':
            st.write("*ALS å‚æ•°è®¾ç½®*")
            col1, col2, col3 = st.columns(3)
            with col1:
                baseline_params['lam'] = st.selectbox(
                    "Î» (å¹³æ»‘åº¦)", 
                    [1e3, 1e4, 1e5, 1e6, 1e7], 
                    index=2,
                    help="æ•°å€¼è¶Šå¤§ï¼ŒåŸºçº¿è¶Šå¹³æ»‘"
                )
            with col2:
                baseline_params['p'] = st.selectbox(
                    "p (ä¸å¯¹ç§°åº¦)", 
                    [0.001, 0.01, 0.1, 0.5], 
                    index=1,
                    help="æ•°å€¼è¶Šå°ï¼ŒåŸºçº¿è¶Šåå‘è°·åº•"
                )
            with col3:
                baseline_params['niter'] = st.slider(
                    "è¿­ä»£æ¬¡æ•°", 
                    5, 50, 10,
                    help="è¿­ä»£æ¬¡æ•°ï¼Œé€šå¸¸10-20æ¬¡è¶³å¤Ÿ"
                )
        
        elif baseline_method == 'arPLS':
            st.write("*arPLS å‚æ•°è®¾ç½®*")
            col1, col2, col3 = st.columns(3)
            with col1:
                baseline_params['lam'] = st.selectbox(
                    "Î» (å¹³æ»‘åº¦)", 
                    [1e3, 1e4, 1e5, 1e6, 1e7], 
                    index=2,
                    help="æ•°å€¼è¶Šå¤§ï¼ŒåŸºçº¿è¶Šå¹³æ»‘"
                )
            with col2:
                baseline_params['ratio'] = st.selectbox(
                    "æ”¶æ•›æ¯”ä¾‹", 
                    [0.001, 0.01, 0.1], 
                    index=1,
                    help="æ”¶æ•›åˆ¤æ–­é˜ˆå€¼"
                )
            with col3:
                baseline_params['niter'] = st.slider(
                    "æœ€å¤§è¿­ä»£æ¬¡æ•°", 
                    5, 50, 10,
                    help="æœ€å¤§è¿­ä»£æ¬¡æ•°"
                )
        
        elif baseline_method == 'polynomial':
            st.write("*å¤šé¡¹å¼å‚æ•°è®¾ç½®*")
            baseline_params['degree'] = st.slider(
                "å¤šé¡¹å¼é˜¶æ•°", 
                1, 8, 2,
                help="é˜¶æ•°è¶Šé«˜ï¼ŒåŸºçº¿è¶Šçµæ´»ï¼Œä½†å¯èƒ½è¿‡æ‹Ÿåˆ"
            )
        
        elif baseline_method == 'rolling_ball':
            st.write("*æ»šçƒå‚æ•°è®¾ç½®*")
            baseline_params['radius'] = st.slider(
                "çƒåŠå¾„", 
                10, 500, 100,
                help="çƒåŠå¾„è¶Šå¤§ï¼ŒåŸºçº¿è¶Šå¹³æ»‘"
            )
        
        # linearæ–¹æ³•æ— éœ€é¢å¤–å‚æ•°
    
    # 4. å½’ä¸€åŒ–
    st.write("**4. å½’ä¸€åŒ–**")
    apply_normalize = st.checkbox("å¯ç”¨å½’ä¸€åŒ–", value=True)
    
    if apply_normalize:
        normalize_method = st.selectbox(
            "å½’ä¸€åŒ–æ–¹æ³•",
            ['area', 'max', 'vector', 'minmax', 'std'],
            format_func=lambda x: {
                'area': 'é¢ç§¯å½’ä¸€åŒ–',
                'max': 'æœ€å¤§å€¼å½’ä¸€åŒ–',
                'vector': 'å‘é‡å½’ä¸€åŒ– (L2)',
                'minmax': 'æœ€å°-æœ€å¤§å½’ä¸€åŒ–',
                'std': 'æ ‡å‡†åŒ– (é›¶å‡å€¼å•ä½æ–¹å·®)'
            }[x]
        )
    
    # 5. SNVå˜æ¢
    st.write("**5. æ ‡å‡†æ­£æ€å˜é‡å˜æ¢ (SNV)**")
    apply_snv = st.checkbox("å¯ç”¨SNVå˜æ¢", value=False)
    
    # é¢„è§ˆè®¾ç½®
    if st.checkbox("é¢„è§ˆåŸºçº¿æ ¡æ­£æ•ˆæœ", value=False):
        if apply_baseline:
            try:
                # é€‰æ‹©ä¸€ä¸ªæ ·æœ¬è¿›è¡Œé¢„è§ˆ
                preview_idx = st.selectbox("é€‰æ‹©é¢„è§ˆæ ·æœ¬", range(min(5, X.shape[0])), format_func=lambda x: f"æ ·æœ¬ {x+1}")
                
                # è·å–æˆªå–åçš„æ•°æ®
                start_idx = np.argmin(np.abs(wavenumbers - start_wavenumber))
                end_idx = np.argmin(np.abs(wavenumbers - end_wavenumber))
                if start_idx > end_idx:
                    start_idx, end_idx = end_idx, start_idx
                
                wavenumbers_crop = wavenumbers[start_idx:end_idx+1]
                spectrum_crop = X[preview_idx, start_idx:end_idx+1]
                
                # åº”ç”¨å¹³æ»‘
                if apply_smooth:
                    spectrum_smooth = savgol_filter(spectrum_crop, smooth_window, smooth_poly)
                else:
                    spectrum_smooth = spectrum_crop
                
                # åŸºçº¿æ ¡æ­£é¢„è§ˆ
                corrector = SpectrumBaselineCorrector()
                baseline, corrected = corrector.correct_baseline(
                    spectrum_smooth, 
                    baseline_method, 
                    **baseline_params
                )
                
                # ç»˜åˆ¶é¢„è§ˆå›¾
                fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))
                
                # åŸå§‹å…‰è°±å’ŒåŸºçº¿
                ax1.plot(wavenumbers_crop, spectrum_smooth, 'b-', label='å¹³æ»‘åå…‰è°±', alpha=0.8)
                ax1.plot(wavenumbers_crop, baseline, 'r--', label='ä¼°è®¡åŸºçº¿', linewidth=2)
                ax1.set_title(f'æ ·æœ¬ {preview_idx+1} - åŸºçº¿æ ¡æ­£é¢„è§ˆ')
                ax1.set_xlabel('æ³¢æ•° (cmâ»Â¹)')
                ax1.set_ylabel('å¼ºåº¦')
                ax1.legend()
                ax1.grid(True, alpha=0.3)
                
                # æ ¡æ­£åå…‰è°±
                ax2.plot(wavenumbers_crop, corrected, 'g-', label='åŸºçº¿æ ¡æ­£å', alpha=0.8)
                ax2.set_title('åŸºçº¿æ ¡æ­£åå…‰è°±')
                ax2.set_xlabel('æ³¢æ•° (cmâ»Â¹)')
                ax2.set_ylabel('æ ¡æ­£åå¼ºåº¦')
                ax2.legend()
                ax2.grid(True, alpha=0.3)
                
                plt.tight_layout()
                st.pyplot(fig)
                
            except Exception as e:
                st.error(f"é¢„è§ˆå¤±è´¥: {e}")
    
    # å¼€å§‹é¢„å¤„ç†
    if st.button("ğŸš€ å¼€å§‹é¢„å¤„ç†", type="primary"):
        with st.spinner("æ­£åœ¨è¿›è¡Œæ•°æ®é¢„å¤„ç†..."):
            try:
                progress_bar = st.progress(0)
                progress_text = st.empty()
                
                # ä¿å­˜é¢„å¤„ç†å‚æ•°
                preprocessing_params = {
                    'start_wavenumber': start_wavenumber,
                    'end_wavenumber': end_wavenumber,
                    'apply_smooth': apply_smooth,
                    'smooth_window': smooth_window if apply_smooth else None,
                    'smooth_poly': smooth_poly if apply_smooth else None,
                    'apply_baseline': apply_baseline,
                    'baseline_method': baseline_method if apply_baseline else None,
                    'baseline_params': baseline_params if apply_baseline else None,
                    'apply_normalize': apply_normalize,
                    'normalize_method': normalize_method if apply_normalize else None,
                    'apply_snv': apply_snv
                }
                
                st.session_state.preprocessing_params = preprocessing_params
                
                # 1. æ³¢æ•°æˆªå–
                progress_text.text("æ­¥éª¤ 1/5: æ³¢æ•°æˆªå–...")
                start_idx = np.argmin(np.abs(wavenumbers - start_wavenumber))
                end_idx = np.argmin(np.abs(wavenumbers - end_wavenumber))
                
                if start_idx > end_idx:
                    start_idx, end_idx = end_idx, start_idx
                
                wavenumbers_crop = wavenumbers[start_idx:end_idx+1]
                X_crop = X[:, start_idx:end_idx+1]
                progress_bar.progress(0.2)
                
                # 2. å¹³æ»‘å¤„ç†
                progress_text.text("æ­¥éª¤ 2/5: å¹³æ»‘å¤„ç†...")
                if apply_smooth:
                    X_smooth = np.zeros_like(X_crop)
                    for i in range(X_crop.shape[0]):
                        X_smooth[i] = savgol_filter(X_crop[i], smooth_window, smooth_poly)
                else:
                    X_smooth = X_crop.copy()
                progress_bar.progress(0.4)
                
                # 3. åŸºçº¿æ ¡æ­£ - ä½¿ç”¨å®Œæ•´çš„åŸºçº¿æ ¡æ­£å™¨
                progress_text.text("æ­¥éª¤ 3/5: åŸºçº¿æ ¡æ­£...")
                if apply_baseline:
                    corrector = SpectrumBaselineCorrector()
                    X_corrected = np.zeros_like(X_smooth)
                    failed_samples = []
                    
                    for i in range(X_smooth.shape[0]):
                        try:
                            baseline, corrected = corrector.correct_baseline(
                                X_smooth[i], 
                                baseline_method, 
                                **baseline_params
                            )
                            X_corrected[i] = corrected
                        except Exception as e:
                            failed_samples.append(i+1)
                            X_corrected[i] = X_smooth[i]  # ä½¿ç”¨å¹³æ»‘åçš„åŸå§‹æ•°æ®
                            # ä¸åœ¨å¾ªç¯ä¸­æ˜¾ç¤ºè­¦å‘Šï¼Œé¿å…ç•Œé¢æ··ä¹±
                    
                    # ç»Ÿä¸€æ˜¾ç¤ºå¤±è´¥ä¿¡æ¯
                    if failed_samples:
                        if len(failed_samples) <= 5:
                            st.warning(f"æ ·æœ¬ {', '.join(map(str, failed_samples))} åŸºçº¿æ ¡æ­£å¤±è´¥ï¼Œä½¿ç”¨å¹³æ»‘åæ•°æ®")
                        else:
                            st.warning(f"å…±æœ‰ {len(failed_samples)} ä¸ªæ ·æœ¬åŸºçº¿æ ¡æ­£å¤±è´¥ï¼Œä½¿ç”¨å¹³æ»‘åæ•°æ®")
                else:
                    X_corrected = X_smooth.copy()
                progress_bar.progress(0.6)
                
                # 4. å½’ä¸€åŒ–
                progress_text.text("æ­¥éª¤ 4/5: å½’ä¸€åŒ–...")
                if apply_normalize:
                    X_normalized = np.zeros_like(X_corrected)
                    
                    for i in range(X_corrected.shape[0]):
                        spectrum = X_corrected[i]
                        
                        if normalize_method == 'area':
                            # ä¿®æ­£ï¼šä½¿ç”¨ç»å¯¹å€¼è®¡ç®—é¢ç§¯ï¼Œé¿å…è´Ÿé¢ç§¯é—®é¢˜
                            total_area = np.trapz(np.abs(spectrum), wavenumbers_crop)
                            if total_area < 1e-12:
                                X_normalized[i] = spectrum
                            else:
                                X_normalized[i] = spectrum / total_area
                        
                        elif normalize_method == 'max':
                            max_abs_val = np.max(np.abs(spectrum))
                            if max_abs_val < 1e-12:
                                X_normalized[i] = spectrum
                            else:
                                X_normalized[i] = spectrum / max_abs_val
                        
                        elif normalize_method == 'vector':
                            norm_val = np.linalg.norm(spectrum)
                            if norm_val < 1e-12:
                                X_normalized[i] = spectrum
                            else:
                                X_normalized[i] = spectrum / norm_val
                        
                        elif normalize_method == 'minmax':
                            min_val = np.min(spectrum)
                            max_val = np.max(spectrum)
                            if abs(max_val - min_val) < 1e-12:
                                X_normalized[i] = spectrum
                            else:
                                X_normalized[i] = (spectrum - min_val) / (max_val - min_val)
                        
                        elif normalize_method == 'std':
                            mean_val = np.mean(spectrum)
                            std_val = np.std(spectrum)
                            if std_val < 1e-12:
                                X_normalized[i] = spectrum - mean_val
                            else:
                                X_normalized[i] = (spectrum - mean_val) / std_val
                        
                        else:
                            X_normalized[i] = spectrum
                else:
                    X_normalized = X_corrected.copy()
                progress_bar.progress(0.8)
                
                # 5. SNVå˜æ¢
                progress_text.text("æ­¥éª¤ 5/5: SNVå˜æ¢...")
                if apply_snv:
                    X_final = np.zeros_like(X_normalized)
                    for i, spectrum in enumerate(X_normalized):
                        mean_val = np.mean(spectrum)
                        std_val = np.std(spectrum)
                        if std_val < 1e-12:
                            X_final[i] = spectrum
                        else:
                            X_final[i] = (spectrum - mean_val) / std_val
                else:
                    X_final = X_normalized
                progress_bar.progress(1.0)
                
                # ä¿å­˜ç»“æœ
                st.session_state.X_preprocessed = X_final
                st.session_state.wavenumbers_preprocessed = wavenumbers_crop
                st.session_state.preprocessing_done = True
                
                progress_text.text("é¢„å¤„ç†å®Œæˆï¼")
                
                st.success("ğŸ‰ æ•°æ®é¢„å¤„ç†å®Œæˆï¼")
                st.info(f"ğŸ“Š é¢„å¤„ç†åæ•°æ®å½¢çŠ¶: {X_final.shape}")
                st.info(f"ğŸ“ æ³¢æ•°èŒƒå›´: {wavenumbers_crop.min():.1f} ~ {wavenumbers_crop.max():.1f} cmâ»Â¹")
                
                # æ˜¾ç¤ºé¢„å¤„ç†æ­¥éª¤æ€»ç»“
                with st.expander("æŸ¥çœ‹é¢„å¤„ç†æ­¥éª¤æ€»ç»“"):
                    steps_summary = []
                    steps_summary.append(f"âœ… æ³¢æ•°æˆªå–: {start_wavenumber:.1f} ~ {end_wavenumber:.1f} cmâ»Â¹")
                    
                    if apply_smooth:
                        steps_summary.append(f"âœ… Savitzky-Golayå¹³æ»‘: çª—å£={smooth_window}, å¤šé¡¹å¼é˜¶æ•°={smooth_poly}")
                    else:
                        steps_summary.append("â­• å¹³æ»‘å¤„ç†: æœªå¯ç”¨")
                    
                    if apply_baseline:
                        method_name = {
                            'als': 'ALS (éå¯¹ç§°æœ€å°äºŒä¹˜)',
                            'arPLS': 'arPLS (éå¯¹ç§°é‡åŠ æƒæƒ©ç½šæœ€å°äºŒä¹˜)',
                            'polynomial': 'å¤šé¡¹å¼æ‹Ÿåˆ',
                            'linear': 'çº¿æ€§åŸºçº¿',
                            'rolling_ball': 'æ»šçƒåŸºçº¿æ ¡æ­£'
                        }[baseline_method]
                        steps_summary.append(f"âœ… åŸºçº¿æ ¡æ­£: {method_name}")
                    else:
                        steps_summary.append("â­• åŸºçº¿æ ¡æ­£: æœªå¯ç”¨")
                    
                    if apply_normalize:
                        method_name = {
                            'area': 'é¢ç§¯å½’ä¸€åŒ–',
                            'max': 'æœ€å¤§å€¼å½’ä¸€åŒ–',
                            'vector': 'å‘é‡å½’ä¸€åŒ– (L2)',
                            'minmax': 'æœ€å°-æœ€å¤§å½’ä¸€åŒ–',
                            'std': 'æ ‡å‡†åŒ– (é›¶å‡å€¼å•ä½æ–¹å·®)'
                        }[normalize_method]
                        steps_summary.append(f"âœ… å½’ä¸€åŒ–: {method_name}")
                    else:
                        steps_summary.append("â­• å½’ä¸€åŒ–: æœªå¯ç”¨")
                    
                    if apply_snv:
                        steps_summary.append("âœ… SNVå˜æ¢: å·²å¯ç”¨")
                    else:
                        steps_summary.append("â­• SNVå˜æ¢: æœªå¯ç”¨")
                    
                    for step in steps_summary:
                        st.write(step)
                
                # æ˜¾ç¤ºé¢„å¤„ç†å‰åå¯¹æ¯”
                show_preprocessing_comparison(X, X_final, wavenumbers, wavenumbers_crop)
                
            except Exception as e:
                st.error(f"é¢„å¤„ç†è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
                st.error(traceback.format_exc())


def show_preprocessing_comparison(X_original, X_processed, wavenumbers_original, wavenumbers_processed):
    """æ˜¾ç¤ºé¢„å¤„ç†å‰åå¯¹æ¯”"""
    st.subheader("ğŸ“ˆ é¢„å¤„ç†æ•ˆæœå¯¹æ¯”")
    
    # é€‰æ‹©è¦æ˜¾ç¤ºçš„æ ·æœ¬
    n_samples = min(5, X_original.shape[0])
    sample_indices = np.linspace(0, X_original.shape[0]-1, n_samples, dtype=int)
    
    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))
    
    # åŸå§‹å…‰è°±
    for i in sample_indices:
        ax1.plot(wavenumbers_original, X_original[i], alpha=0.7, label=f'æ ·æœ¬ {i+1}')
    ax1.set_title('åŸå§‹å…‰è°±')
    ax1.set_xlabel('æ³¢æ•° (cmâ»Â¹)')
    ax1.set_ylabel('å¼ºåº¦')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # é¢„å¤„ç†åå…‰è°±
    for i in sample_indices:
        ax2.plot(wavenumbers_processed, X_processed[i], alpha=0.7, label=f'æ ·æœ¬ {i+1}')
    ax2.set_title('é¢„å¤„ç†åå…‰è°±')
    ax2.set_xlabel('æ³¢æ•° (cmâ»Â¹)')
    ax2.set_ylabel('å¤„ç†åå¼ºåº¦')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    plt.tight_layout()
    st.pyplot(fig)


def show_feature_extraction_page():
    """ç‰¹å¾æå–ä¸å¯è§†åŒ–é¡µé¢"""
    st.markdown("<h1 class='section-header'>ç‰¹å¾æå–ä¸å¯è§†åŒ–</h1>", unsafe_allow_html=True)
    
    # æ£€æŸ¥å‰ç½®æ¡ä»¶
    if not check_data_prerequisites(need_labels=False, need_preprocessing=True):
        return
    
    st.markdown("""
    <div class="info-box">
    è¿›è¡Œç‰¹å¾é€‰æ‹©ä»¥æå–æœ€é‡è¦çš„å…‰è°±ç‰¹å¾ï¼Œæé«˜æ¨¡å‹æ€§èƒ½å¹¶å‡å°‘è®¡ç®—å¤æ‚åº¦ã€‚
    </div>
    """, unsafe_allow_html=True)
    
    X = st.session_state.X_preprocessed
    wavenumbers = st.session_state.wavenumbers_preprocessed
    
    st.info(f"ğŸ“Š é¢„å¤„ç†åæ•°æ®å½¢çŠ¶: {X.shape}")
    
    # ç‰¹å¾é€‰æ‹©æ–¹æ³•
    st.subheader("ğŸ” ç‰¹å¾é€‰æ‹©")
    
    feature_method = st.selectbox(
        "é€‰æ‹©ç‰¹å¾é€‰æ‹©æ–¹æ³•",
        ["ä¸è¿›è¡Œç‰¹å¾é€‰æ‹©", "æ–¹å·®è¿‡æ»¤", "å•å˜é‡é€‰æ‹©", "é€’å½’ç‰¹å¾æ¶ˆé™¤", "éšæœºæ£®æ—é‡è¦æ€§"],
        help="ä¸åŒçš„ç‰¹å¾é€‰æ‹©æ–¹æ³•é€‚ç”¨äºä¸åŒåœºæ™¯"
    )
    
    if feature_method != "ä¸è¿›è¡Œç‰¹å¾é€‰æ‹©":
        # æ£€æŸ¥æ˜¯å¦æœ‰æ ‡ç­¾æ•°æ®
        if not hasattr(st.session_state, 'y') or st.session_state.y is None:
            st.warning("å¤§éƒ¨åˆ†ç‰¹å¾é€‰æ‹©æ–¹æ³•éœ€è¦æ ‡ç­¾æ•°æ®ï¼Œå½“å‰ä»…å¯ä½¿ç”¨æ–¹å·®è¿‡æ»¤")
            if feature_method != "æ–¹å·®è¿‡æ»¤":
                st.stop()
        
        # ç‰¹å¾é€‰æ‹©å‚æ•°
        if feature_method == "æ–¹å·®è¿‡æ»¤":
            threshold = st.slider("æ–¹å·®é˜ˆå€¼", 0.0, 1.0, 0.01, 0.01)
        
        elif feature_method == "å•å˜é‡é€‰æ‹©":
            k_features = st.slider("é€‰æ‹©ç‰¹å¾æ•°é‡", 10, min(X.shape[1], 1000), min(100, X.shape[1]//2))
        
        elif feature_method == "é€’å½’ç‰¹å¾æ¶ˆé™¤":
            n_features = st.slider("ç›®æ ‡ç‰¹å¾æ•°é‡", 10, min(X.shape[1], 500), min(50, X.shape[1]//4))
        
        elif feature_method == "éšæœºæ£®æ—é‡è¦æ€§":
            n_features = st.slider("é€‰æ‹©ç‰¹å¾æ•°é‡", 10, min(X.shape[1], 500), min(100, X.shape[1]//2))
            threshold = st.slider("é‡è¦æ€§é˜ˆå€¼", 0.0, 0.01, 0.001, 0.0001)
    
    # æ‰§è¡Œç‰¹å¾é€‰æ‹©
    if st.button("ğŸš€ æ‰§è¡Œç‰¹å¾é€‰æ‹©"):
        if feature_method == "ä¸è¿›è¡Œç‰¹å¾é€‰æ‹©":
            st.session_state.X_final = X
            st.session_state.wavenumbers_final = wavenumbers
            st.session_state.selected_features = np.arange(X.shape[1])
            st.session_state.feature_selection_method = feature_method
            st.session_state.feature_selected = True
            
            st.success("âœ… ä½¿ç”¨å…¨éƒ¨é¢„å¤„ç†åçš„ç‰¹å¾")
            st.info(f"ğŸ“Š æœ€ç»ˆç‰¹å¾æ•°é‡: {X.shape[1]}")
        
        else:
            with st.spinner("æ­£åœ¨è¿›è¡Œç‰¹å¾é€‰æ‹©..."):
                try:
                    if feature_method == "æ–¹å·®è¿‡æ»¤":
                        from sklearn.feature_selection import VarianceThreshold
                        
                        selector = VarianceThreshold(threshold=threshold)
                        X_selected = selector.fit_transform(X)
                        selected_features = selector.get_support(indices=True)
                    
                    elif feature_method == "å•å˜é‡é€‰æ‹©":
                        from sklearn.feature_selection import SelectKBest, f_regression
                        
                        y = st.session_state.y
                        if y.ndim > 1 and y.shape[1] > 1:
                            # å¤šè¾“å‡ºæƒ…å†µï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªç›®æ ‡å˜é‡
                            y_temp = y[:, 0]
                        else:
                            y_temp = y.ravel() if y.ndim > 1 else y
                        
                        selector = SelectKBest(score_func=f_regression, k=k_features)
                        X_selected = selector.fit_transform(X, y_temp)
                        selected_features = selector.get_support(indices=True)
                    
                    elif feature_method == "é€’å½’ç‰¹å¾æ¶ˆé™¤":
                        from sklearn.feature_selection import RFE
                        from sklearn.linear_model import LinearRegression
                        
                        y = st.session_state.y
                        if y.ndim > 1 and y.shape[1] > 1:
                            y_temp = y[:, 0]
                        else:
                            y_temp = y.ravel() if y.ndim > 1 else y
                        
                        estimator = LinearRegression()
                        selector = RFE(estimator, n_features_to_select=n_features)
                        X_selected = selector.fit_transform(X, y_temp)
                        selected_features = selector.get_support(indices=True)
                    
                    elif feature_method == "éšæœºæ£®æ—é‡è¦æ€§":
                        from sklearn.ensemble import RandomForestRegressor
                        
                        y = st.session_state.y
                        if y.ndim > 1 and y.shape[1] > 1:
                            y_temp = y[:, 0]
                        else:
                            y_temp = y.ravel() if y.ndim > 1 else y
                        
                        rf = RandomForestRegressor(n_estimators=100, random_state=42)
                        rf.fit(X, y_temp)
                        
                        importances = rf.feature_importances_
                        
                        # æ–¹æ³•1ï¼šæŒ‰é‡è¦æ€§é˜ˆå€¼é€‰æ‹©
                        mask1 = importances > threshold
                        
                        # æ–¹æ³•2ï¼šé€‰æ‹©top-kç‰¹å¾
                        indices = np.argsort(importances)[::-1][:n_features]
                        mask2 = np.zeros(X.shape[1], dtype=bool)
                        mask2[indices] = True
                        
                        # å–å¹¶é›†
                        selected_features = np.where(mask1 | mask2)[0]
                        X_selected = X[:, selected_features]
                    
                    # ä¿å­˜ç»“æœ
                    st.session_state.X_final = X_selected
                    st.session_state.wavenumbers_final = wavenumbers.iloc[selected_features] if isinstance(wavenumbers, pd.Series) else wavenumbers[selected_features]
                    st.session_state.selected_features = selected_features
                    st.session_state.feature_selection_method = feature_method
                    st.session_state.feature_selected = True
                    
                    st.success("ğŸ‰ ç‰¹å¾é€‰æ‹©å®Œæˆï¼")
                    st.info(f"ğŸ“Š åŸå§‹ç‰¹å¾æ•°é‡: {X.shape[1]}")
                    st.info(f"ğŸ“Š é€‰æ‹©ç‰¹å¾æ•°é‡: {X_selected.shape[1]}")
                    st.info(f"ğŸ“ˆ ç‰¹å¾å‹ç¼©æ¯”: {X_selected.shape[1]/X.shape[1]:.2%}")
                    
                    # æ˜¾ç¤ºç‰¹å¾é€‰æ‹©ç»“æœ
                    show_feature_selection_results(X, X_selected, wavenumbers, selected_features, feature_method)
                    
                except Exception as e:
                    st.error(f"ç‰¹å¾é€‰æ‹©è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
                    st.error(traceback.format_exc())
    
    # å¦‚æœå·²ç»å®Œæˆç‰¹å¾é€‰æ‹©ï¼Œæ˜¾ç¤ºå¯è§†åŒ–
    if hasattr(st.session_state, 'feature_selected') and st.session_state.feature_selected:
        show_feature_visualization()


def show_feature_selection_results(X_original, X_selected, wavenumbers, selected_features, method):
    """æ˜¾ç¤ºç‰¹å¾é€‰æ‹©ç»“æœ"""
    st.subheader("ğŸ“Š ç‰¹å¾é€‰æ‹©ç»“æœ")
    
    # ç‰¹å¾é‡è¦æ€§å¯è§†åŒ–
    if method == "éšæœºæ£®æ—é‡è¦æ€§":
        # æ˜¾ç¤ºç‰¹å¾é‡è¦æ€§å›¾
        from sklearn.ensemble import RandomForestRegressor
        
        y = st.session_state.y
        if y.ndim > 1 and y.shape[1] > 1:
            y_temp = y[:, 0]
        else:
            y_temp = y.ravel() if y.ndim > 1 else y
        
        rf = RandomForestRegressor(n_estimators=100, random_state=42)
        rf.fit(X_original, y_temp)
        
        importances = rf.feature_importances_
        
        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))
        
        # å…¨éƒ¨ç‰¹å¾é‡è¦æ€§
        ax1.plot(wavenumbers, importances, alpha=0.7)
        ax1.set_title('ç‰¹å¾é‡è¦æ€§åˆ†å¸ƒ')
        ax1.set_xlabel('æ³¢æ•° (cmâ»Â¹)')
        ax1.set_ylabel('é‡è¦æ€§')
        ax1.grid(True, alpha=0.3)
        
        # é€‰æ‹©çš„ç‰¹å¾
        selected_wavenumbers = wavenumbers.iloc[selected_features] if isinstance(wavenumbers, pd.Series) else wavenumbers[selected_features]
        selected_importances = importances[selected_features]
        
        ax2.scatter(selected_wavenumbers, selected_importances, c='red', alpha=0.7)
        ax2.set_title('é€‰æ‹©çš„ç‰¹å¾åŠå…¶é‡è¦æ€§')
        ax2.set_xlabel('æ³¢æ•° (cmâ»Â¹)')
        ax2.set_ylabel('é‡è¦æ€§')
        ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        st.pyplot(fig)
    
    # ç‰¹å¾åˆ†å¸ƒå¯¹æ¯”
    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))
    
    # åŸå§‹å…‰è°±ï¼ˆå–å‰å‡ ä¸ªæ ·æœ¬ï¼‰
    n_samples = min(3, X_original.shape[0])
    for i in range(n_samples):
        ax1.plot(wavenumbers, X_original[i], alpha=0.7, label=f'æ ·æœ¬ {i+1}')
    ax1.set_title('åŸå§‹é¢„å¤„ç†åå…‰è°±ï¼ˆå…¨éƒ¨ç‰¹å¾ï¼‰')
    ax1.set_xlabel('æ³¢æ•° (cmâ»Â¹)')
    ax1.set_ylabel('å¼ºåº¦')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # é€‰æ‹©ç‰¹å¾çš„å…‰è°±
    selected_wavenumbers = wavenumbers.iloc[selected_features] if isinstance(wavenumbers, pd.Series) else wavenumbers[selected_features]
    for i in range(n_samples):
        ax2.plot(selected_wavenumbers, X_selected[i], alpha=0.7, label=f'æ ·æœ¬ {i+1}')
    ax2.set_title('ç‰¹å¾é€‰æ‹©åå…‰è°±')
    ax2.set_xlabel('æ³¢æ•° (cmâ»Â¹)')
    ax2.set_ylabel('å¼ºåº¦')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    plt.tight_layout()
    st.pyplot(fig)


def show_feature_visualization():
    """æ˜¾ç¤ºç‰¹å¾å¯è§†åŒ–"""
    st.subheader("ğŸ“ˆ æ•°æ®å¯è§†åŒ–")
    
    X = st.session_state.X_final
    wavenumbers = st.session_state.wavenumbers_final
    
    # ç»Ÿä¸€è½¬æ¢ä¸ºnumpyæ•°ç»„ä»¥é¿å…ç´¢å¼•é—®é¢˜
    if isinstance(wavenumbers, pd.Series):
        wavenumbers = wavenumbers.values
    elif not isinstance(wavenumbers, np.ndarray):
        wavenumbers = np.array(wavenumbers)
    
    tab1, tab2, tab3 = st.tabs(["å…‰è°±å›¾", "ç»Ÿè®¡åˆ†æ", "ç›¸å…³æ€§åˆ†æ"])
    
    with tab1:
        st.write("### ğŸŒˆ å…‰è°±å›¾å¯è§†åŒ–")
        
        # æ˜¾ç¤ºé€‰é¡¹
        col1, col2 = st.columns(2)
        
        with col1:
            n_samples_show = st.slider("æ˜¾ç¤ºæ ·æœ¬æ•°é‡", 1, min(20, X.shape[0]), min(5, X.shape[0]))
        
        with col2:
            sample_selection = st.selectbox("æ ·æœ¬é€‰æ‹©æ–¹å¼", ["å‡åŒ€åˆ†å¸ƒ", "éšæœºé€‰æ‹©", "å‰Nä¸ªæ ·æœ¬"])
        
        # é€‰æ‹©æ ·æœ¬
        if sample_selection == "å‡åŒ€åˆ†å¸ƒ":
            indices = np.linspace(0, X.shape[0]-1, n_samples_show, dtype=int)
        elif sample_selection == "éšæœºé€‰æ‹©":
            np.random.seed(42)
            indices = np.random.choice(X.shape[0], n_samples_show, replace=False)
        else:  # å‰Nä¸ªæ ·æœ¬
            indices = np.arange(min(n_samples_show, X.shape[0]))
        
        # ç»˜åˆ¶å…‰è°±å›¾
        fig, ax = plt.subplots(figsize=(12, 6))
        
        for i, idx in enumerate(indices):
            ax.plot(wavenumbers, X[idx], alpha=0.7, label=f'æ ·æœ¬ {idx+1}')
        
        ax.set_title('å…‰è°±å›¾')
        ax.set_xlabel('æ³¢æ•° (cmâ»Â¹)')
        ax.set_ylabel('å¼ºåº¦')
        if len(indices) <= 10:
            ax.legend()
        ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        st.pyplot(fig)
    
    with tab2:
        st.write("### ğŸ“Š ç»Ÿè®¡åˆ†æ")
        
        # è®¡ç®—ç»Ÿè®¡é‡
        mean_spectrum = np.mean(X, axis=0)
        std_spectrum = np.std(X, axis=0)
        min_spectrum = np.min(X, axis=0)
        max_spectrum = np.max(X, axis=0)
        
        # ç»˜åˆ¶ç»Ÿè®¡å›¾
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))
        
        # å‡å€¼å…‰è°±
        ax1.plot(wavenumbers, mean_spectrum, color='blue')
        ax1.fill_between(wavenumbers, mean_spectrum - std_spectrum, mean_spectrum + std_spectrum, 
                        alpha=0.3, color='blue')
        ax1.set_title('å¹³å‡å…‰è°± Â± æ ‡å‡†å·®')
        ax1.set_xlabel('æ³¢æ•° (cmâ»Â¹)')
        ax1.set_ylabel('å¼ºåº¦')
        ax1.grid(True, alpha=0.3)
        
        # æ ‡å‡†å·®
        ax2.plot(wavenumbers, std_spectrum, color='red')
        ax2.set_title('æ ‡å‡†å·®åˆ†å¸ƒ')
        ax2.set_xlabel('æ³¢æ•° (cmâ»Â¹)')
        ax2.set_ylabel('æ ‡å‡†å·®')
        ax2.grid(True, alpha=0.3)
        
        # æœ€å€¼èŒƒå›´
        ax3.fill_between(wavenumbers, min_spectrum, max_spectrum, alpha=0.5, color='green')
        ax3.plot(wavenumbers, mean_spectrum, color='black', linewidth=2, label='å‡å€¼')
        ax3.set_title('æ•°æ®èŒƒå›´ (æœ€å°å€¼-æœ€å¤§å€¼)')
        ax3.set_xlabel('æ³¢æ•° (cmâ»Â¹)')
        ax3.set_ylabel('å¼ºåº¦')
        ax3.legend()
        ax3.grid(True, alpha=0.3)
        
        # ç®±çº¿å›¾ï¼ˆé€‰æ‹©å‡ ä¸ªä»£è¡¨æ€§æ³¢æ•°ï¼‰
        n_wavenumber_samples = min(20, len(wavenumbers))
        wn_indices = np.linspace(0, len(wavenumbers)-1, n_wavenumber_samples, dtype=int)
        
        box_data = [X[:, i] for i in wn_indices]
        box_labels = [f'{wavenumbers[i]:.0f}' for i in wn_indices]
        
        ax4.boxplot(box_data, labels=box_labels)
        ax4.set_title('å¼ºåº¦åˆ†å¸ƒç®±çº¿å›¾ï¼ˆéƒ¨åˆ†æ³¢æ•°ï¼‰')
        ax4.set_xlabel('æ³¢æ•° (cmâ»Â¹)')
        ax4.set_ylabel('å¼ºåº¦')
        ax4.tick_params(axis='x', rotation=45)
        ax4.grid(True, alpha=0.3)
        
        plt.tight_layout()
        st.pyplot(fig)
        
        # æ˜¾ç¤ºæ•°å€¼ç»Ÿè®¡
        st.write("**æ•°å€¼ç»Ÿè®¡æ‘˜è¦ï¼š**")
        stats_df = pd.DataFrame({
            'ç»Ÿè®¡é‡': ['æ ·æœ¬æ•°é‡', 'ç‰¹å¾æ•°é‡', 'å¹³å‡å¼ºåº¦å‡å€¼', 'å¹³å‡å¼ºåº¦æ ‡å‡†å·®', 'æœ€å°å¼ºåº¦', 'æœ€å¤§å¼ºåº¦'],
            'æ•°å€¼': [
                X.shape[0],
                X.shape[1],
                f"{np.mean(mean_spectrum):.6f}",
                f"{np.mean(std_spectrum):.6f}",
                f"{np.min(X):.6f}",
                f"{np.max(X):.6f}"
            ]
        })
        st.dataframe(stats_df, use_container_width=True)
    
    with tab3:
        st.write("### ğŸ”— ç›¸å…³æ€§åˆ†æ")
        
        if hasattr(st.session_state, 'y') and st.session_state.y is not None:
            y = st.session_state.y
            
            # è®¡ç®—ç›¸å…³ç³»æ•°
            if y.ndim == 1:
                correlations = np.array([np.corrcoef(X[:, i], y)[0, 1] for i in range(X.shape[1])])
                
                # ç»˜åˆ¶ç›¸å…³æ€§å›¾
                fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))
                
                # ç›¸å…³ç³»æ•°æ›²çº¿
                ax1.plot(wavenumbers, correlations, color='purple')
                ax1.axhline(y=0, color='black', linestyle='--', alpha=0.5)
                ax1.set_title('å…‰è°±ç‰¹å¾ä¸ç›®æ ‡å˜é‡çš„ç›¸å…³æ€§')
                ax1.set_xlabel('æ³¢æ•° (cmâ»Â¹)')
                ax1.set_ylabel('ç›¸å…³ç³»æ•°')
                ax1.grid(True, alpha=0.3)
                
                # ç›¸å…³ç³»æ•°åˆ†å¸ƒç›´æ–¹å›¾
                ax2.hist(correlations, bins=30, alpha=0.7, color='purple')
                ax2.set_title('ç›¸å…³ç³»æ•°åˆ†å¸ƒ')
                ax2.set_xlabel('ç›¸å…³ç³»æ•°')
                ax2.set_ylabel('é¢‘æ•°')
                ax2.grid(True, alpha=0.3)
                
                plt.tight_layout()
                st.pyplot(fig)
                
                # æ˜¾ç¤ºé«˜ç›¸å…³æ€§ç‰¹å¾
                high_corr_indices = np.where(np.abs(correlations) > 0.5)[0]
                if len(high_corr_indices) > 0:
                    st.write("**é«˜ç›¸å…³æ€§ç‰¹å¾ (|r| > 0.5)ï¼š**")
                    high_corr_df = pd.DataFrame({
                        'æ³¢æ•°': [wavenumbers[i] if i < len(wavenumbers) else f"ç‰¹å¾{i}" for i in high_corr_indices],
                        'ç›¸å…³ç³»æ•°': correlations[high_corr_indices].round(4)
                    })
                    high_corr_df = high_corr_df.sort_values('ç›¸å…³ç³»æ•°', key=abs, ascending=False)
                    st.dataframe(high_corr_df, use_container_width=True)
                else:
                    st.info("æœªå‘ç°é«˜ç›¸å…³æ€§ç‰¹å¾ (|r| > 0.5)")
            
            else:
                # å¤šç›®æ ‡æƒ…å†µ
                st.write("**å¤šç›®æ ‡å˜é‡ç›¸å…³æ€§åˆ†æï¼š**")
                target_names = st.session_state.selected_cols
                
                selected_target = st.selectbox("é€‰æ‹©ç›®æ ‡å˜é‡æŸ¥çœ‹ç›¸å…³æ€§", range(len(target_names)), 
                                             format_func=lambda x: target_names[x])
                
                correlations = np.array([np.corrcoef(X[:, i], y[:, selected_target])[0, 1] 
                                       for i in range(X.shape[1])])
                
                fig, ax = plt.subplots(figsize=(12, 6))
                ax.plot(wavenumbers, correlations, color='purple')
                ax.axhline(y=0, color='black', linestyle='--', alpha=0.5)
                ax.set_title(f'å…‰è°±ç‰¹å¾ä¸ {target_names[selected_target]} çš„ç›¸å…³æ€§')
                ax.set_xlabel('æ³¢æ•° (cmâ»Â¹)')
                ax.set_ylabel('ç›¸å…³ç³»æ•°')
                ax.grid(True, alpha=0.3)
                
                plt.tight_layout()
                st.pyplot(fig)
        
        else:
            st.info("éœ€è¦æ ‡ç­¾æ•°æ®æ‰èƒ½è¿›è¡Œç›¸å…³æ€§åˆ†æ")


def show_trend_analysis_page():
    """è¶‹åŠ¿åˆ†æé¡µé¢"""
    st.markdown("<h1 class='section-header'>è¶‹åŠ¿åˆ†æ</h1>", unsafe_allow_html=True)
    
    # æ£€æŸ¥å‰ç½®æ¡ä»¶
    if not check_data_prerequisites(need_labels=False, need_preprocessing=True):
        return
    
    st.markdown("""
    <div class="info-box">
    è¿›è¡Œå…‰è°±æ•°æ®çš„è¶‹åŠ¿åˆ†æï¼ŒåŒ…æ‹¬PCAé™ç»´ã€æˆåˆ†åˆ†è§£ã€æ—¶é—´è¶‹åŠ¿ç­‰å¤šç§åˆ†ææ–¹æ³•ã€‚
    é€‚ç”¨äºæœ‰æ ‡ç­¾æˆ–æ— æ ‡ç­¾æ•°æ®çš„æ¢ç´¢æ€§åˆ†æã€‚
    </div>
    """, unsafe_allow_html=True)
    
    X, wavenumbers, data_info = get_current_data()
    show_status_message(data_info, "info")
    
    # åˆ†ææ–¹æ³•é€‰æ‹©
    analysis_tabs = st.tabs([
        "PCAåˆ†æ", 
        "æˆåˆ†åˆ†è§£", 
        "æ—¶é—´è¶‹åŠ¿", 
        "èšç±»åˆ†æ", 
        "å¼‚å¸¸æ£€æµ‹",
        "ç»¼åˆæŠ¥å‘Š"
    ])
    
    with analysis_tabs[0]:
        show_pca_analysis(X, wavenumbers)
    
    with analysis_tabs[1]:
        show_component_decomposition(X, wavenumbers)
    
    with analysis_tabs[2]:
        show_time_trend_analysis(X, wavenumbers)
    
    with analysis_tabs[3]:
        show_clustering_analysis(X, wavenumbers)
    
    with analysis_tabs[4]:
        show_anomaly_detection(X, wavenumbers)
    
    with analysis_tabs[5]:
        show_comprehensive_report(X, wavenumbers)


def show_pca_analysis(X, wavenumbers):
    """PCAåˆ†æ"""
    st.write("### ğŸ” ä¸»æˆåˆ†åˆ†æ (PCA)")
     # è½¬æ¢ä¸ºnumpyæ•°ç»„ä»¥é¿å…ç´¢å¼•é—®é¢˜
    if isinstance(wavenumbers, pd.Series):
        wavenumbers = wavenumbers.values
    
    from sklearn.decomposition import PCA
    from sklearn.preprocessing import StandardScaler
    
    # PCAå‚æ•°è®¾ç½®
    col1, col2 = st.columns(2)
    
    with col1:
        n_components = st.slider(
            "ä¸»æˆåˆ†æ•°é‡", 
            2, min(10, X.shape[1], X.shape[0]), 
            min(5, X.shape[1], X.shape[0])
        )
    
    with col2:
        standardize = st.checkbox("æ•°æ®æ ‡å‡†åŒ–", value=True)
    
    if st.button("æ‰§è¡ŒPCAåˆ†æ"):
        with st.spinner("æ­£åœ¨è¿›è¡ŒPCAåˆ†æ..."):
            try:
                # æ•°æ®é¢„å¤„ç†
                if standardize:
                    scaler = StandardScaler()
                    X_scaled = scaler.fit_transform(X)
                else:
                    X_scaled = X
                
                # æ‰§è¡ŒPCA
                pca = PCA(n_components=n_components)
                X_pca = pca.fit_transform(X_scaled)
                
                # ç»“æœå±•ç¤º
                st.success(f"âœ… PCAåˆ†æå®Œæˆï¼è§£é‡Šæ–¹å·®æ¯”: {pca.explained_variance_ratio_.sum():.2%}")
                
                # 1. è§£é‡Šæ–¹å·®æ¯”å›¾
                fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))
                
                # å„ä¸»æˆåˆ†è§£é‡Šæ–¹å·®æ¯”
                ax1.bar(range(1, n_components+1), pca.explained_variance_ratio_, alpha=0.7)
                ax1.set_title('å„ä¸»æˆåˆ†è§£é‡Šæ–¹å·®æ¯”')
                ax1.set_xlabel('ä¸»æˆåˆ†')
                ax1.set_ylabel('è§£é‡Šæ–¹å·®æ¯”')
                ax1.grid(True, alpha=0.3)
                
                # ç´¯ç§¯è§£é‡Šæ–¹å·®æ¯”
                cumsum_var = np.cumsum(pca.explained_variance_ratio_)
                ax2.plot(range(1, n_components+1), cumsum_var, 'o-', color='red')
                ax2.set_title('ç´¯ç§¯è§£é‡Šæ–¹å·®æ¯”')
                ax2.set_xlabel('ä¸»æˆåˆ†æ•°é‡')
                ax2.set_ylabel('ç´¯ç§¯è§£é‡Šæ–¹å·®æ¯”')
                ax2.grid(True, alpha=0.3)
                
                # ç¬¬ä¸€ä¸»æˆåˆ†è½½è·
                ax3.plot(wavenumbers, pca.components_[0], color='blue')
                ax3.set_title('ç¬¬ä¸€ä¸»æˆåˆ†è½½è·')
                ax3.set_xlabel('æ³¢æ•° (cmâ»Â¹)')
                ax3.set_ylabel('è½½è·')
                ax3.grid(True, alpha=0.3)
                
                # å‰ä¸¤ä¸ªä¸»æˆåˆ†å¾—åˆ†æ•£ç‚¹å›¾
                ax4.scatter(X_pca[:, 0], X_pca[:, 1], alpha=0.6)
                ax4.set_title('å‰ä¸¤ä¸ªä¸»æˆåˆ†å¾—åˆ†å›¾')
                ax4.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})')
                ax4.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})')
                ax4.grid(True, alpha=0.3)
                
                plt.tight_layout()
                st.pyplot(fig)
                
                # 2. ä¸»æˆåˆ†è½½è·çƒ­å›¾
                if n_components >= 3:
                    st.write("### ğŸ“Š ä¸»æˆåˆ†è½½è·çƒ­å›¾")
                    
                    fig, ax = plt.subplots(figsize=(12, 6))
                    
                    # é€‰æ‹©æ˜¾ç¤ºçš„æ³¢æ•°ç‚¹ï¼ˆå¦‚æœå¤ªå¤šçš„è¯ï¼‰
                    if len(wavenumbers) > 200:
                        step = len(wavenumbers) // 200
                        wn_indices = slice(None, None, step)
                        display_wavenumbers = wavenumbers[wn_indices]
                        display_components = pca.components_[:, wn_indices]
                    else:
                        display_wavenumbers = wavenumbers
                        display_components = pca.components_
                    
                    im = ax.imshow(display_components, aspect='auto', cmap='RdBu_r')
                    ax.set_title('ä¸»æˆåˆ†è½½è·çƒ­å›¾')
                    ax.set_xlabel('æ³¢æ•° (cmâ»Â¹)')
                    ax.set_ylabel('ä¸»æˆåˆ†')
                    
                    # è®¾ç½®xè½´æ ‡ç­¾
                    n_ticks = 10
                    tick_indices = np.linspace(0, len(display_wavenumbers)-1, n_ticks, dtype=int)
                    ax.set_xticks(tick_indices)
                    ax.set_xticklabels([f'{display_wavenumbers[i]:.0f}' for i in tick_indices])
                    
                    # è®¾ç½®yè½´æ ‡ç­¾
                    ax.set_yticks(range(n_components))
                    ax.set_yticklabels([f'PC{i+1}' for i in range(n_components)])
                    
                    plt.colorbar(im, ax=ax)
                    plt.tight_layout()
                    st.pyplot(fig)
                
                # 3. æ•°æ®è¡¨æ ¼
                st.write("### ğŸ“‹ PCAç»“æœæ‘˜è¦")
                
                pca_summary = pd.DataFrame({
                    'ä¸»æˆåˆ†': [f'PC{i+1}' for i in range(n_components)],
                    'è§£é‡Šæ–¹å·®æ¯”': [f'{ratio:.4f}' for ratio in pca.explained_variance_ratio_],
                    'ç´¯ç§¯è§£é‡Šæ–¹å·®æ¯”': [f'{cum:.4f}' for cum in cumsum_var],
                    'ç‰¹å¾å€¼': [f'{val:.4f}' for val in pca.explained_variance_]
                })
                
                st.dataframe(pca_summary, use_container_width=True)
                
                # 4. ä¿å­˜ç»“æœåˆ°session state
                st.session_state.pca_results = {
                    'pca': pca,
                    'X_pca': X_pca,
                    'explained_variance_ratio': pca.explained_variance_ratio_,
                    'components': pca.components_
                }
                
                # 5. å¦‚æœæœ‰æ ‡ç­¾æ•°æ®ï¼Œæ˜¾ç¤ºæ ‡ç­¾ä¸ä¸»æˆåˆ†çš„å…³ç³»
                if hasattr(st.session_state, 'y') and st.session_state.y is not None:
                    show_pca_label_relationship(X_pca, pca.explained_variance_ratio_)
                
            except Exception as e:
                st.error(f"PCAåˆ†æå‡ºé”™: {e}")
                st.error(traceback.format_exc())


def show_pca_label_relationship(X_pca, explained_var_ratio):
    """æ˜¾ç¤ºPCAä¸æ ‡ç­¾çš„å…³ç³»"""
    st.write("### ğŸ¯ ä¸»æˆåˆ†ä¸æ ‡ç­¾å…³ç³»")
    
    y = st.session_state.y
    
    if y.ndim == 1:
        # å•æ ‡ç­¾æƒ…å†µ
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
        
        # PC1 vs æ ‡ç­¾
        ax1.scatter(X_pca[:, 0], y, alpha=0.6)
        corr1 = np.corrcoef(X_pca[:, 0], y)[0, 1]
        ax1.set_title(f'PC1 vs æ ‡ç­¾ (r={corr1:.3f})')
        ax1.set_xlabel(f'PC1 ({explained_var_ratio[0]:.1%})')
        ax1.set_ylabel('æ ‡ç­¾å€¼')
        ax1.grid(True, alpha=0.3)
        
        # PC2 vs æ ‡ç­¾
        if X_pca.shape[1] > 1:
            ax2.scatter(X_pca[:, 1], y, alpha=0.6)
            corr2 = np.corrcoef(X_pca[:, 1], y)[0, 1]
            ax2.set_title(f'PC2 vs æ ‡ç­¾ (r={corr2:.3f})')
            ax2.set_xlabel(f'PC2 ({explained_var_ratio[1]:.1%})')
            ax2.set_ylabel('æ ‡ç­¾å€¼')
            ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        st.pyplot(fig)
        
    else:
        # å¤šæ ‡ç­¾æƒ…å†µ
        target_names = st.session_state.selected_cols
        selected_target = st.selectbox(
            "é€‰æ‹©ç›®æ ‡å˜é‡æŸ¥çœ‹ä¸ä¸»æˆåˆ†çš„å…³ç³»", 
            range(len(target_names)), 
            format_func=lambda x: target_names[x]
        )
        
        y_selected = y[:, selected_target]
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
        
        # PC1 vs é€‰æ‹©çš„æ ‡ç­¾
        ax1.scatter(X_pca[:, 0], y_selected, alpha=0.6)
        corr1 = np.corrcoef(X_pca[:, 0], y_selected)[0, 1]
        ax1.set_title(f'PC1 vs {target_names[selected_target]} (r={corr1:.3f})')
        ax1.set_xlabel(f'PC1 ({explained_var_ratio[0]:.1%})')
        ax1.set_ylabel(target_names[selected_target])
        ax1.grid(True, alpha=0.3)
        
        # PC2 vs é€‰æ‹©çš„æ ‡ç­¾
        if X_pca.shape[1] > 1:
            ax2.scatter(X_pca[:, 1], y_selected, alpha=0.6)
            corr2 = np.corrcoef(X_pca[:, 1], y_selected)[0, 1]
            ax2.set_title(f'PC2 vs {target_names[selected_target]} (r={corr2:.3f})')
            ax2.set_xlabel(f'PC2 ({explained_var_ratio[1]:.1%})')
            ax2.set_ylabel(target_names[selected_target])
            ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        st.pyplot(fig)


def show_component_decomposition(X, wavenumbers):
    """æˆåˆ†åˆ†è§£åˆ†æ"""
    st.write("### ğŸ§ª æˆåˆ†åˆ†è§£åˆ†æ")
    
    st.markdown("""
    ä½¿ç”¨çŸ©é˜µåˆ†è§£æŠ€æœ¯åˆ†ç¦»å…‰è°±ä¸­çš„åŒ–å­¦æˆåˆ†ä¿¡æ¯ï¼Œé€‚ç”¨äºï¼š
    - æ··åˆç‰©æˆåˆ†è¯†åˆ«
    - ä¸»å‰¯äº§ç‰©åˆ†ç¦»
    - åŒ–å­¦è¿‡ç¨‹ç›‘æ§
    """)
    
    # åˆ†è§£æ–¹æ³•é€‰æ‹©
    decomp_method = st.selectbox(
        "åˆ†è§£æ–¹æ³•",
        ["NMF", "ICA", "Factor Analysis"],
        format_func=lambda x: {
            "NMF": "éè´ŸçŸ©é˜µåˆ†è§£ (NMF)",
            "ICA": "ç‹¬ç«‹æˆåˆ†åˆ†æ (ICA)",
            "Factor Analysis": "å› å­åˆ†æ"
        }[x],
        key="decomp_method_select"
    )
    
    # å‚æ•°è®¾ç½®
    col1, col2 = st.columns(2)
    
    with col1:
        n_components = st.slider("æˆåˆ†æ•°é‡", 2, min(10, X.shape[0], X.shape[1]), 3, key="decomp_n_components")
    
    with col2:
        if decomp_method == "NMF":
            max_iter = st.slider("æœ€å¤§è¿­ä»£æ¬¡æ•°", 100, 1000, 200, key="decomp_nmf_max_iter")
        elif decomp_method == "ICA":
            max_iter = st.slider("æœ€å¤§è¿­ä»£æ¬¡æ•°", 100, 1000, 200, key="decomp_ica_max_iter")
            tolerance = st.selectbox("æ”¶æ•›å®¹å·®", [1e-3, 1e-4, 1e-5], index=0, key="decomp_ica_tol")
        else:
            max_iter = None
    
    if st.button("æ‰§è¡Œæˆåˆ†åˆ†è§£", key="decomp_execute_button"):
        with st.spinner("æ­£åœ¨è¿›è¡Œæˆåˆ†åˆ†è§£..."):
            try:
                # æ•°æ®é¢„å¤„ç†æ£€æŸ¥
                st.write("### æ•°æ®é¢„å¤„ç†æ£€æŸ¥:")
                
                has_nan = np.isnan(X).any()
                has_inf = np.isinf(X).any()
                st.write(f"- æ˜¯å¦åŒ…å«NaN: {has_nan}")
                st.write(f"- æ˜¯å¦åŒ…å«æ— ç©·å¤§: {has_inf}")
                st.write(f"- æ•°æ®èŒƒå›´: [{X.min():.3f}, {X.max():.3f}]")
                st.write(f"- æ•°æ®å½¢çŠ¶: {X.shape}")
                
                # æ¸…ç†æ•°æ®
                X_clean = X.copy()
                
                if has_nan or has_inf:
                    st.warning("æ£€æµ‹åˆ°å¼‚å¸¸å€¼ï¼Œæ­£åœ¨æ¸…ç†...")
                    X_clean = np.nan_to_num(X_clean, nan=0.0, posinf=X_clean[np.isfinite(X_clean)].max(), 
                                          neginf=X_clean[np.isfinite(X_clean)].min())
                    st.info("å·²æ¸…ç†å¼‚å¸¸å€¼")
                
                # â­ å…³é”®ä¿®å¤ï¼šæ•°æ®é™ç»´å’Œæ­£åˆ™åŒ–å¤„ç† â­
                if decomp_method == "ICA":
                    from sklearn.preprocessing import StandardScaler
                    from sklearn.decomposition import PCA
                    
                    # 1. æ ‡å‡†åŒ–
                    scaler = StandardScaler()
                    X_scaled = scaler.fit_transform(X_clean)
                    st.info("ICA: å·²åº”ç”¨æ ‡å‡†åŒ–")
                    
                    # 2. æ£€æŸ¥æ•°æ®çš„æœ‰æ•ˆç»´åº¦
                    try:
                        # ä½¿ç”¨SVDæ£€æŸ¥æœ‰æ•ˆç§©
                        U, s, Vt = np.linalg.svd(X_scaled, full_matrices=False)
                        
                        # æ‰¾åˆ°éé›¶å¥‡å¼‚å€¼çš„æ•°é‡ï¼ˆæœ‰æ•ˆç§©ï¼‰
                        tolerance_svd = 1e-10
                        effective_rank = np.sum(s > tolerance_svd)
                        
                        st.write(f"- æ•°æ®æœ‰æ•ˆç§©: {effective_rank}")
                        st.write(f"- å¥‡å¼‚å€¼èŒƒå›´: [{s.min():.2e}, {s.max():.2e}]")
                        
                        # å¦‚æœæœ‰æ•ˆç§©å°äºæˆåˆ†æ•°é‡ï¼Œè°ƒæ•´æˆåˆ†æ•°é‡
                        if effective_rank < n_components:
                            n_components = min(effective_rank - 1, n_components)
                            st.warning(f"æ•°æ®æœ‰æ•ˆç§©ä¸è¶³ï¼Œå·²è°ƒæ•´æˆåˆ†æ•°é‡ä¸º: {n_components}")
                        
                        # å¦‚æœæœ‰æ•ˆç§©å¤ªå°ï¼Œä½¿ç”¨PCAé¢„å¤„ç†
                        if effective_rank < min(X_scaled.shape) * 0.8:
                            st.info("ä½¿ç”¨PCAè¿›è¡Œæ•°æ®é¢„å¤„ç†ä»¥æé«˜æ•°å€¼ç¨³å®šæ€§")
                            
                            # ä¿ç•™95%çš„æ–¹å·®
                            pca_components = min(effective_rank, int(min(X_scaled.shape) * 0.95))
                            pca = PCA(n_components=pca_components)
                            X_pca = pca.fit_transform(X_scaled)
                            
                            st.write(f"- PCAé™ç»´: {X_scaled.shape[1]} â†’ {X_pca.shape[1]}")
                            st.write(f"- ä¿ç•™æ–¹å·®æ¯”ä¾‹: {pca.explained_variance_ratio_.sum():.3f}")
                            
                            X_for_ica = X_pca
                        else:
                            X_for_ica = X_scaled
                            
                    except np.linalg.LinAlgError:
                        st.warning("SVDåˆ†æå¤±è´¥ï¼Œä½¿ç”¨ç®€å•é¢„å¤„ç†")
                        X_for_ica = X_scaled
                    
                    # 3. æœ€ç»ˆæ¡ä»¶æ•°æ£€æŸ¥
                    try:
                        cond_num = np.linalg.cond(X_for_ica)
                        st.write(f"- é¢„å¤„ç†åæ¡ä»¶æ•°: {cond_num:.2e}")
                        
                        if cond_num > 1e12:
                            st.warning("æ¡ä»¶æ•°ä»ç„¶è¿‡å¤§ï¼Œå°†ä½¿ç”¨æ›´ä¿å®ˆçš„å‚æ•°")
                            # è¿›ä¸€æ­¥é™ç»´
                            if X_for_ica.shape[1] > n_components * 3:
                                from sklearn.decomposition import TruncatedSVD
                                svd = TruncatedSVD(n_components=n_components * 3, random_state=42)
                                X_for_ica = svd.fit_transform(X_for_ica)
                                st.info(f"è¿›ä¸€æ­¥é™ç»´è‡³: {X_for_ica.shape[1]} ç»´")
                    except:
                        st.warning("æ— æ³•è®¡ç®—é¢„å¤„ç†åçš„æ¡ä»¶æ•°")
                    
                else:
                    X_for_ica = X_clean
                
                # æ‰§è¡Œåˆ†è§£
                if decomp_method == "NMF":
                    from sklearn.decomposition import NMF
                    
                    X_positive = X_clean - X_clean.min() + 1e-6
                    
                    model = NMF(n_components=n_components, max_iter=max_iter, random_state=42)
                    W = model.fit_transform(X_positive)
                    H = model.components_
                    X_for_reconstruction = X_positive
                    
                elif decomp_method == "ICA":
                    from sklearn.decomposition import FastICA
                    
                    # â­ å¤šç§ICAç­–ç•¥å°è¯• â­
                    ica_success = False
                    
                    # ç­–ç•¥1: æ ‡å‡†ICA with ä¿å®ˆå‚æ•°
                    try:
                        st.info("å°è¯•ç­–ç•¥1: æ ‡å‡†ICA")
                        model = FastICA(
                            n_components=n_components, 
                            max_iter=max_iter, 
                            tol=tolerance,
                            random_state=42,
                            whiten='unit-variance',
                            fun='logcosh',
                            algorithm='parallel'
                        )
                        S = model.fit_transform(X_for_ica)
                        A = model.mixing_
                        ica_success = True
                        
                    except Exception as e1:
                        st.warning(f"ç­–ç•¥1å¤±è´¥: {str(e1)[:100]}...")
                        
                        # ç­–ç•¥2: å‡å°‘æˆåˆ†æ•°é‡
                        try:
                            reduced_components = max(2, n_components // 2)
                            st.info(f"å°è¯•ç­–ç•¥2: å‡å°‘æˆåˆ†æ•°é‡è‡³ {reduced_components}")
                            
                            model = FastICA(
                                n_components=reduced_components, 
                                max_iter=max_iter, 
                                tol=1e-3,  # æ”¾å®½å®¹å·®
                                random_state=42,
                                whiten='unit-variance',
                                fun='exp',
                                algorithm='deflation'
                            )
                            S = model.fit_transform(X_for_ica)
                            A = model.mixing_
                            n_components = reduced_components
                            ica_success = True
                            
                        except Exception as e2:
                            st.warning(f"ç­–ç•¥2å¤±è´¥: {str(e2)[:100]}...")
                            
                            # ç­–ç•¥3: ä½¿ç”¨PCAé¢„ç™½åŒ–
                            try:
                                st.info("å°è¯•ç­–ç•¥3: PCAé¢„ç™½åŒ–")
                                from sklearn.decomposition import PCA
                                
                                # ä½¿ç”¨PCAè¿›è¡Œé¢„ç™½åŒ–
                                pca_dim = min(n_components * 2, X_for_ica.shape[1] // 2, X_for_ica.shape[0] // 2)
                                pca = PCA(n_components=pca_dim, whiten=True)
                                X_whitened = pca.fit_transform(X_for_ica)
                                
                                model = FastICA(
                                    n_components=min(n_components, pca_dim),
                                    max_iter=max_iter,
                                    tol=1e-2,  # è¿›ä¸€æ­¥æ”¾å®½å®¹å·®
                                    random_state=42,
                                    whiten=False,  # å·²ç»ç™½åŒ–è¿‡äº†
                                    fun='cube',
                                    algorithm='deflation'
                                )
                                S = model.fit_transform(X_whitened)
                                
                                # å°†ç»“æœè½¬æ¢å›åŸå§‹ç©ºé—´
                                A = pca.components_.T @ model.mixing_
                                n_components = S.shape[1]
                                ica_success = True
                                
                            except Exception as e3:
                                st.error(f"æ‰€æœ‰ICAç­–ç•¥éƒ½å¤±è´¥äº†:")
                                st.error(f"ç­–ç•¥1: {str(e1)[:50]}...")
                                st.error(f"ç­–ç•¥2: {str(e2)[:50]}...")
                                st.error(f"ç­–ç•¥3: {str(e3)[:50]}...")
                                st.error("å»ºè®®å°è¯•å…¶ä»–åˆ†è§£æ–¹æ³•ï¼ˆNMFæˆ–å› å­åˆ†æï¼‰")
                                return
                    
                    if ica_success:
                        W = S
                        H = A.T
                        X_for_reconstruction = X_for_ica
                        st.success(f"ICAæˆåŠŸå®Œæˆï¼Œæœ€ç»ˆæˆåˆ†æ•°é‡: {n_components}")
                    
                else:  # Factor Analysis
                    from sklearn.decomposition import FactorAnalysis
                    
                    model = FactorAnalysis(n_components=n_components, random_state=42)
                    W = model.fit_transform(X_clean)
                    H = model.components_
                    X_for_reconstruction = X_clean
                
                # åç»­çš„å¯è§†åŒ–ä»£ç ä¿æŒä¸å˜...
                st.write("### åˆ†è§£ç»“æœ:")
                st.write(f"- æœ€ç»ˆæˆåˆ†æ•°é‡: {n_components}")
                st.write(f"- åˆ†è§£ç»“æœ W å½¢çŠ¶: {W.shape}")
                st.write(f"- åˆ†è§£ç»“æœ H å½¢çŠ¶: {H.shape}")
                
                # ç¡®ä¿æ³¢æ•°å’Œæˆåˆ†å…‰è°±ç»´åº¦åŒ¹é…
                if H.shape[1] != len(wavenumbers):
                    st.warning(f"æ£€æµ‹åˆ°ç»´åº¦ä¸åŒ¹é…: Hç»´åº¦={H.shape[1]}, æ³¢æ•°é•¿åº¦={len(wavenumbers)}")
                    
                    min_length = min(H.shape[1], len(wavenumbers))
                    wavenumbers_plot = wavenumbers[:min_length]
                    H_plot = H[:, :min_length]
                    
                    st.info(f"å·²è°ƒæ•´ç»˜å›¾æ•°æ®é•¿åº¦ä¸º: {min_length}")
                else:
                    wavenumbers_plot = wavenumbers
                    H_plot = H
                
                st.success(f"âœ… {decomp_method} åˆ†è§£å®Œæˆï¼")
                
                # å¯è§†åŒ–ç»“æœ
                fig, axes = plt.subplots(2, 2, figsize=(15, 12))
                
                # 1. æˆåˆ†å…‰è°±
                ax1 = axes[0, 0]
                for i in range(n_components):
                    ax1.plot(wavenumbers_plot, H_plot[i], label=f'æˆåˆ† {i+1}', alpha=0.8)
                ax1.set_title('åˆ†è§£å¾—åˆ°çš„æˆåˆ†å…‰è°±')
                ax1.set_xlabel('æ³¢æ•° (cmâ»Â¹)')
                ax1.set_ylabel('å¼ºåº¦')
                ax1.legend()
                ax1.grid(True, alpha=0.3)
                
                # 2. æ ·æœ¬ç³»æ•°
                ax2 = axes[0, 1]
                for i in range(n_components):
                    ax2.plot(W[:, i], label=f'æˆåˆ† {i+1}', marker='o', alpha=0.7)
                ax2.set_title('æ ·æœ¬ä¸­å„æˆåˆ†çš„ç³»æ•°')
                ax2.set_xlabel('æ ·æœ¬ç´¢å¼•')
                ax2.set_ylabel('ç³»æ•°')
                ax2.legend()
                ax2.grid(True, alpha=0.3)
                
                # 3. æˆåˆ†è´¡çŒ®åº¦
                ax3 = axes[1, 0]
                contributions = np.mean(np.abs(W), axis=0)
                contributions = contributions / contributions.sum() * 100
                ax3.bar(range(1, n_components+1), contributions, alpha=0.7)
                ax3.set_title('å„æˆåˆ†å¹³å‡è´¡çŒ®åº¦')
                ax3.set_xlabel('æˆåˆ†')
                ax3.set_ylabel('è´¡çŒ®åº¦ (%)')
                ax3.grid(True, alpha=0.3)
                
                # 4. é‡æ„è¯¯å·®
                ax4 = axes[1, 1]
                try:
                    X_reconstructed = W @ H
                    if X_reconstructed.shape == X_for_reconstruction.shape:
                        reconstruction_error = np.mean((X_for_reconstruction - X_reconstructed)**2, axis=1)
                        ax4.plot(reconstruction_error, 'o-', alpha=0.7)
                        ax4.set_title('æ ·æœ¬é‡æ„è¯¯å·®')
                        ax4.set_xlabel('æ ·æœ¬ç´¢å¼•')
                        ax4.set_ylabel('é‡æ„è¯¯å·®')
                    else:
                        ax4.text(0.5, 0.5, f'é‡æ„ç»´åº¦ä¸åŒ¹é…\nåŸå§‹: {X_for_reconstruction.shape}\né‡æ„: {X_reconstructed.shape}', 
                                ha='center', va='center', transform=ax4.transAxes)
                        ax4.set_title('é‡æ„è¯¯å·® (ç»´åº¦ä¸åŒ¹é…)')
                        
                except Exception as e:
                    ax4.text(0.5, 0.5, f'é‡æ„è¯¯å·®è®¡ç®—å¤±è´¥:\n{str(e)}', 
                            ha='center', va='center', transform=ax4.transAxes)
                    ax4.set_title('é‡æ„è¯¯å·® (è®¡ç®—å¤±è´¥)')
                
                ax4.grid(True, alpha=0.3)
                
                plt.tight_layout()
                st.pyplot(fig)
                
                # æˆåˆ†åˆ†æç»“æœè¡¨æ ¼
                st.write("### ğŸ“‹ æˆåˆ†åˆ†æç»“æœ")
                
                component_stats = []
                for i in range(n_components):
                    peak_idx = np.argmax(np.abs(H_plot[i]))
                    peak_wavenumber = wavenumbers_plot[peak_idx]
                    
                    stats = {
                        'æˆåˆ†': f'æˆåˆ†{i+1}',
                        'å¹³å‡æµ“åº¦': f"{np.mean(W[:, i]):.3f}",
                        'æµ“åº¦æ ‡å‡†å·®': f"{np.std(W[:, i]):.3f}",
                        'æœ€å¤§æµ“åº¦æ ·æœ¬': f"æ ·æœ¬{np.argmax(np.abs(W[:, i]))+1}",
                        'æœ€å¤§æµ“åº¦å€¼': f"{np.max(np.abs(W[:, i])):.3f}",
                        'å…‰è°±å³°å€¼æ³¢æ•°': f"{peak_wavenumber:.1f} cmâ»Â¹"
                    }
                    component_stats.append(stats)
                
                st.dataframe(pd.DataFrame(component_stats), use_container_width=True)
                
                # ä¿å­˜ç»“æœ
                st.session_state.decomposition_results = {
                    'method': decomp_method,
                    'model': model,
                    'components': H,
                    'coefficients': W,
                    'n_components': n_components
                }
                
                # å¦‚æœæœ‰æ ‡ç­¾ï¼Œåˆ†ææˆåˆ†ä¸æ ‡ç­¾çš„å…³ç³»
                if hasattr(st.session_state, 'y') and st.session_state.y is not None:
                    show_component_label_relationship(W, H, decomp_method)
                
            except Exception as e:
                st.error(f"æˆåˆ†åˆ†è§£å‡ºé”™: {e}")
                st.error(traceback.format_exc())


def show_component_label_relationship(W, H, method):
    """æ˜¾ç¤ºæˆåˆ†ä¸æ ‡ç­¾çš„å…³ç³»"""
    st.write(f"### ğŸ¯ {method} æˆåˆ†ä¸æ ‡ç­¾å…³ç³»")
    
    y = st.session_state.y
    
    if y.ndim == 1:
        # å•æ ‡ç­¾æƒ…å†µ
        fig, axes = plt.subplots(1, min(3, W.shape[1]), figsize=(15, 5))
        if W.shape[1] == 1:
            axes = [axes]
        
        for i in range(min(3, W.shape[1])):
            if i < len(axes):
                corr = np.corrcoef(W[:, i], y)[0, 1]
                axes[i].scatter(W[:, i], y, alpha=0.6)
                axes[i].set_title(f'æˆåˆ† {i+1} vs æ ‡ç­¾ (r={corr:.3f})')
                axes[i].set_xlabel(f'æˆåˆ† {i+1} ç³»æ•°')
                axes[i].set_ylabel('æ ‡ç­¾å€¼')
                axes[i].grid(True, alpha=0.3)
        
        plt.tight_layout()
        st.pyplot(fig)
        
    else:
        # å¤šæ ‡ç­¾æƒ…å†µ
        target_names = st.session_state.selected_cols
        selected_target = st.selectbox(
            "é€‰æ‹©ç›®æ ‡å˜é‡æŸ¥çœ‹ä¸æˆåˆ†çš„å…³ç³»", 
            range(len(target_names)), 
            format_func=lambda x: target_names[x],
            key="component_target_select"
        )
        
        y_selected = y[:, selected_target]
        
        fig, axes = plt.subplots(1, min(3, W.shape[1]), figsize=(15, 5))
        if W.shape[1] == 1:
            axes = [axes]
        
        for i in range(min(3, W.shape[1])):
            if i < len(axes):
                corr = np.corrcoef(W[:, i], y_selected)[0, 1]
                axes[i].scatter(W[:, i], y_selected, alpha=0.6)
                axes[i].set_title(f'æˆåˆ† {i+1} vs {target_names[selected_target]} (r={corr:.3f})')
                axes[i].set_xlabel(f'æˆåˆ† {i+1} ç³»æ•°')
                axes[i].set_ylabel(target_names[selected_target])
                axes[i].grid(True, alpha=0.3)
        
        plt.tight_layout()
        st.pyplot(fig)


def show_time_trend_analysis(X, wavenumbers):
    """æ—¶é—´è¶‹åŠ¿åˆ†æ"""
    st.write("### ğŸ“ˆ æ—¶é—´è¶‹åŠ¿åˆ†æ")
    
    st.markdown("""
    åˆ†æå…‰è°±æ•°æ®éšæ—¶é—´çš„å˜åŒ–è¶‹åŠ¿ï¼Œé€‚ç”¨äºï¼š
    - ååº”è¿‡ç¨‹ç›‘æ§
    - å·¥è‰ºè¿‡ç¨‹åˆ†æ
    - æ ·å“ç¨³å®šæ€§ç ”ç©¶
    """)
    
    # æ—¶é—´åºåˆ—è®¾ç½®
    col1, col2 = st.columns(2)
    
    with col1:
        time_mode = st.selectbox(
            "æ—¶é—´æ¨¡å¼",
            ["æ ·æœ¬ç´¢å¼•ä½œä¸ºæ—¶é—´", "ç­‰é—´éš”æ—¶é—´åºåˆ—", "è‡ªå®šä¹‰æ—¶é—´"],
            help="é€‰æ‹©å¦‚ä½•å®šä¹‰æ—¶é—´è½´"
        )
    
    with col2:
        if time_mode == "ç­‰é—´éš”æ—¶é—´åºåˆ—":
            time_interval = st.number_input("æ—¶é—´é—´éš”", min_value=0.1, value=1.0, step=0.1)
            time_unit = st.selectbox("æ—¶é—´å•ä½", ["ç§’", "åˆ†é’Ÿ", "å°æ—¶", "å¤©"])
        elif time_mode == "è‡ªå®šä¹‰æ—¶é—´":
            st.info("è¯·åœ¨ä¸‹æ–¹è¾“å…¥æ—¶é—´åºåˆ—")
    
    # ç”Ÿæˆæ—¶é—´è½´
    if time_mode == "æ ·æœ¬ç´¢å¼•ä½œä¸ºæ—¶é—´":
        time_axis = np.arange(1, X.shape[0] + 1)
        time_label = "æ ·æœ¬ç´¢å¼•"
    
    elif time_mode == "ç­‰é—´éš”æ—¶é—´åºåˆ—":
        time_axis = np.arange(0, X.shape[0] * time_interval, time_interval)
        time_label = f"æ—¶é—´ ({time_unit})"
    
    else:  # è‡ªå®šä¹‰æ—¶é—´
        time_input = st.text_area(
            "è¾“å…¥æ—¶é—´åºåˆ—ï¼ˆæ¯è¡Œä¸€ä¸ªå€¼ï¼‰",
            value="\n".join(str(i) for i in range(1, min(11, X.shape[0] + 1))),
            height=100
        )
        
        try:
            time_values = [float(line.strip()) for line in time_input.split('\n') if line.strip()]
            if len(time_values) != X.shape[0]:
                st.warning(f"æ—¶é—´ç‚¹æ•°é‡ ({len(time_values)}) ä¸æ ·æœ¬æ•°é‡ ({X.shape[0]}) ä¸åŒ¹é…")
                time_axis = np.arange(1, X.shape[0] + 1)
                time_label = "æ ·æœ¬ç´¢å¼•"
            else:
                time_axis = np.array(time_values)
                time_label = "æ—¶é—´"
        except ValueError:
            st.error("æ—¶é—´åºåˆ—æ ¼å¼é”™è¯¯ï¼Œè¯·è¾“å…¥æ•°å€¼")
            time_axis = np.arange(1, X.shape[0] + 1)
            time_label = "æ ·æœ¬ç´¢å¼•"
    
    # åˆ†æé€‰é¡¹
    analysis_options = st.multiselect(
        "é€‰æ‹©åˆ†æå†…å®¹",
        ["æ•´ä½“è¶‹åŠ¿", "ç‰¹å®šæ³¢æ•°è¶‹åŠ¿", "å…‰è°±æ¼”åŒ–", "å˜åŒ–ç‡åˆ†æ", "ç›¸å…³æ€§è¶‹åŠ¿"],
        default=["æ•´ä½“è¶‹åŠ¿", "å…‰è°±æ¼”åŒ–"]
    )
    
    # åˆå§‹åŒ–selected_wn_indicesï¼Œå¦‚æœä¸å­˜åœ¨
    if "selected_wn_indices" not in st.session_state:
        n_wavenumbers = min(5, len(wavenumbers))
        st.session_state.selected_wn_indices = np.linspace(0, len(wavenumbers)-1, n_wavenumbers, dtype=int).tolist()
    
    # ç‰¹å®šæ³¢æ•°è¶‹åŠ¿éƒ¨åˆ†çš„é€‰æ‹©æ§ä»¶
    if "ç‰¹å®šæ³¢æ•°è¶‹åŠ¿" in analysis_options:
        st.write("### ğŸ“ ç‰¹å®šæ³¢æ•°ç‚¹è¶‹åŠ¿")
        
        # å®šä¹‰ä¸€ä¸ªå›è°ƒå‡½æ•°æ¥æ›´æ–°é€‰æ‹©çš„æ³¢æ•°ç‚¹
        def update_selected_wn():
            pass  # ä»…ç”¨äºè§¦å‘å›è°ƒï¼Œå®é™…æ›´æ–°å·²ç»åœ¨multiselectæ§ä»¶ä¸­å®Œæˆ
        
        # ä½¿ç”¨on_changeå›è°ƒå‡½æ•°æ¥é¿å…ä¸å¿…è¦çš„åˆ·æ–°
        selected_wn_indices = st.multiselect(
            "é€‰æ‹©å…³æ³¨çš„æ³¢æ•°ç‚¹",
            range(len(wavenumbers)),
            default=st.session_state.selected_wn_indices,
            format_func=lambda x: f"{wavenumbers[x]:.1f} cmâ»Â¹",
            key="wn_multiselect",
            on_change=update_selected_wn
        )
        
        # æ›´æ–°session_state
        st.session_state.selected_wn_indices = selected_wn_indices
    
    if st.button("æ‰§è¡Œè¶‹åŠ¿åˆ†æ"):
        with st.spinner("æ­£åœ¨è¿›è¡Œè¶‹åŠ¿åˆ†æ..."):
            try:
                results = {}
                
                if "æ•´ä½“è¶‹åŠ¿" in analysis_options:
                    # è®¡ç®—æ•´ä½“å…‰è°±æŒ‡æ ‡çš„æ—¶é—´è¶‹åŠ¿
                    total_intensity = np.sum(X, axis=1)
                    mean_intensity = np.mean(X, axis=1)
                    max_intensity = np.max(X, axis=1)
                    std_intensity = np.std(X, axis=1)
                    
                    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))
                    
                    ax1.plot(time_axis, total_intensity, 'o-', alpha=0.7)
                    ax1.set_title('æ€»å¼ºåº¦éšæ—¶é—´å˜åŒ–')
                    ax1.set_xlabel(time_label)
                    ax1.set_ylabel('æ€»å¼ºåº¦')
                    ax1.grid(True, alpha=0.3)
                    
                    ax2.plot(time_axis, mean_intensity, 'o-', color='orange', alpha=0.7)
                    ax2.set_title('å¹³å‡å¼ºåº¦éšæ—¶é—´å˜åŒ–')
                    ax2.set_xlabel(time_label)
                    ax2.set_ylabel('å¹³å‡å¼ºåº¦')
                    ax2.grid(True, alpha=0.3)
                    
                    ax3.plot(time_axis, max_intensity, 'o-', color='green', alpha=0.7)
                    ax3.set_title('æœ€å¤§å¼ºåº¦éšæ—¶é—´å˜åŒ–')
                    ax3.set_xlabel(time_label)
                    ax3.set_ylabel('æœ€å¤§å¼ºåº¦')
                    ax3.grid(True, alpha=0.3)
                    
                    ax4.plot(time_axis, std_intensity, 'o-', color='red', alpha=0.7)
                    ax4.set_title('å¼ºåº¦æ ‡å‡†å·®éšæ—¶é—´å˜åŒ–')
                    ax4.set_xlabel(time_label)
                    ax4.set_ylabel('å¼ºåº¦æ ‡å‡†å·®')
                    ax4.grid(True, alpha=0.3)
                    
                    plt.tight_layout()
                    st.pyplot(fig)
                    
                    results['overall_trends'] = {
                        'total_intensity': total_intensity,
                        'mean_intensity': mean_intensity,
                        'max_intensity': max_intensity,
                        'std_intensity': std_intensity
                    }
                
                if "ç‰¹å®šæ³¢æ•°è¶‹åŠ¿" in analysis_options and st.session_state.selected_wn_indices:
                    # ä½¿ç”¨session_stateä¸­å­˜å‚¨çš„é€‰æ‹©
                    selected_wn_indices = st.session_state.selected_wn_indices
                    
                    if selected_wn_indices:
                        fig, ax = plt.subplots(figsize=(12, 6))
                        
                        for idx in selected_wn_indices:
                            ax.plot(time_axis, X[:, idx], 'o-', alpha=0.7, 
                                label=f'{wavenumbers[idx]:.1f} cmâ»Â¹')
                        
                        ax.set_title('ç‰¹å®šæ³¢æ•°ç‚¹å¼ºåº¦éšæ—¶é—´å˜åŒ–')
                        ax.set_xlabel(time_label)
                        ax.set_ylabel('å¼ºåº¦')
                        ax.legend()
                        ax.grid(True, alpha=0.3)
                        
                        plt.tight_layout()
                        st.pyplot(fig)
                
                # å…¶ä»–åˆ†æé€‰é¡¹çš„ä»£ç ä¿æŒä¸å˜
                if "å…‰è°±æ¼”åŒ–" in analysis_options:
                    st.write("### ğŸŒˆ å…‰è°±æ¼”åŒ–")
                    
                    # 3Då…‰è°±æ¼”åŒ–å›¾
                    fig = plt.figure(figsize=(12, 8))
                    ax = fig.add_subplot(111, projection='3d')
                    
                    # é€‰æ‹©éƒ¨åˆ†æ—¶é—´ç‚¹å’Œæ³¢æ•°ç‚¹ä»¥é¿å…å›¾å½¢è¿‡äºå¤æ‚
                    n_time_points = min(10, X.shape[0])
                    n_wavenumber_points = min(100, len(wavenumbers))
                    
                    time_indices = np.linspace(0, X.shape[0]-1, n_time_points, dtype=int)
                    wn_indices = np.linspace(0, len(wavenumbers)-1, n_wavenumber_points, dtype=int)
                    
                    T, W = np.meshgrid(time_axis[time_indices], wavenumbers[wn_indices])
                    Z = X[time_indices][:, wn_indices].T
                    
                    ax.plot_surface(T, W, Z, cmap='viridis', alpha=0.8)
                    ax.set_xlabel(time_label)
                    ax.set_ylabel('æ³¢æ•° (cmâ»Â¹)')
                    ax.set_zlabel('å¼ºåº¦')
                    ax.set_title('å…‰è°±éšæ—¶é—´æ¼”åŒ– (3Dè§†å›¾)')
                    
                    plt.tight_layout()
                    st.pyplot(fig)
                    
                    # 2Dçƒ­å›¾
                    fig, ax = plt.subplots(figsize=(12, 6))
                    
                    im = ax.imshow(X.T, aspect='auto', cmap='viridis', interpolation='nearest')
                    ax.set_title('å…‰è°±æ¼”åŒ–çƒ­å›¾')
                    ax.set_xlabel('æ—¶é—´ç‚¹')
                    ax.set_ylabel('æ³¢æ•° (cmâ»Â¹)')
                    
                    # è®¾ç½®åæ ‡è½´æ ‡ç­¾
                    n_time_ticks = min(10, len(time_axis))
                    time_tick_indices = np.linspace(0, len(time_axis)-1, n_time_ticks, dtype=int)
                    ax.set_xticks(time_tick_indices)
                    ax.set_xticklabels([f'{time_axis[i]:.1f}' for i in time_tick_indices])
                    
                    n_wn_ticks = min(10, len(wavenumbers))
                    wn_tick_indices = np.linspace(0, len(wavenumbers)-1, n_wn_ticks, dtype=int)
                    ax.set_yticks(wn_tick_indices)
                    ax.set_yticklabels([f'{wavenumbers[i]:.0f}' for i in wn_tick_indices])
                    
                    plt.colorbar(im, ax=ax, label='å¼ºåº¦')
                    plt.tight_layout()
                    st.pyplot(fig)
                
                if "å˜åŒ–ç‡åˆ†æ" in analysis_options:
                    st.write("### ğŸ“Š å˜åŒ–ç‡åˆ†æ")
                    
                    if len(time_axis) > 1:
                        # è®¡ç®—å„æ—¶é—´ç‚¹çš„å˜åŒ–ç‡
                        diff_X = np.diff(X, axis=0)
                        diff_time = np.diff(time_axis)
                        
                        # å˜åŒ–ç‡ = å…‰è°±å·®å€¼ / æ—¶é—´å·®å€¼
                        rate_X = diff_X / diff_time[:, np.newaxis]
                        
                        # æ€»å˜åŒ–ç‡
                        total_rate = np.sum(np.abs(rate_X), axis=1)
                        
                        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))
                        
                        # æ€»å˜åŒ–ç‡éšæ—¶é—´å˜åŒ–
                        ax1.plot(time_axis[1:], total_rate, 'o-', alpha=0.7, color='red')
                        ax1.set_title('æ€»å˜åŒ–ç‡éšæ—¶é—´å˜åŒ–')
                        ax1.set_xlabel(time_label)
                        ax1.set_ylabel('æ€»å˜åŒ–ç‡')
                        ax1.grid(True, alpha=0.3)
                        
                        # å˜åŒ–ç‡çƒ­å›¾
                        im = ax2.imshow(rate_X.T, aspect='auto', cmap='RdBu_r', interpolation='nearest')
                        ax2.set_title('å˜åŒ–ç‡çƒ­å›¾')
                        ax2.set_xlabel('æ—¶é—´é—´éš”')
                        ax2.set_ylabel('æ³¢æ•° (cmâ»Â¹)')
                        
                        # è®¾ç½®åæ ‡è½´
                        n_ticks = min(10, len(time_axis)-1)
                        tick_indices = np.linspace(0, len(time_axis)-2, n_ticks, dtype=int)
                        ax2.set_xticks(tick_indices)
                        ax2.set_xticklabels([f'{time_axis[i+1]:.1f}' for i in tick_indices])
                        
                        n_wn_ticks = min(10, len(wavenumbers))
                        wn_tick_indices = np.linspace(0, len(wavenumbers)-1, n_wn_ticks, dtype=int)
                        ax2.set_yticks(wn_tick_indices)
                        ax2.set_yticklabels([f'{wavenumbers[i]:.0f}' for i in wn_tick_indices])
                        
                        plt.colorbar(im, ax=ax2, label='å˜åŒ–ç‡')
                        plt.tight_layout()
                        st.pyplot(fig)
                        
                        results['change_rates'] = {
                            'rate_matrix': rate_X,
                            'total_rates': total_rate
                        }
                
                if "ç›¸å…³æ€§è¶‹åŠ¿" in analysis_options and hasattr(st.session_state, 'y') and st.session_state.y is not None:
                    st.write("### ğŸ”— ç›¸å…³æ€§è¶‹åŠ¿åˆ†æ")
                    
                    y = st.session_state.y
                    
                    if y.ndim == 1:
                        # è®¡ç®—æ¯ä¸ªæ—¶é—´ç‚¹ä¸æ ‡ç­¾çš„ç›¸å…³æ€§
                        correlations = []
                        for i in range(X.shape[0]):
                            corr = np.corrcoef(X[i], wavenumbers)[0, 1] if len(wavenumbers) == len(X[i]) else 0
                            correlations.append(corr)
                        
                        fig, ax = plt.subplots(figsize=(12, 6))
                        ax.plot(time_axis, correlations, 'o-', alpha=0.7)
                        ax.set_title('å…‰è°±ä¸æ³¢æ•°ç›¸å…³æ€§éšæ—¶é—´å˜åŒ–')
                        ax.set_xlabel(time_label)
                        ax.set_ylabel('ç›¸å…³ç³»æ•°')
                        ax.grid(True, alpha=0.3)
                        
                        plt.tight_layout()
                        st.pyplot(fig)
                
                # ä¿å­˜ç»“æœ
                st.session_state.trend_analysis_results = {
                    'time_axis': time_axis,
                    'time_label': time_label,
                    'analysis_results': results
                }
                
                st.success("âœ… è¶‹åŠ¿åˆ†æå®Œæˆï¼")
                
            except Exception as e:
                st.error(f"è¶‹åŠ¿åˆ†æå‡ºé”™: {e}")
                st.error(traceback.format_exc())


def show_clustering_analysis(X, wavenumbers):
    """èšç±»åˆ†æ"""
    st.write("### ğŸ¯ èšç±»åˆ†æ")
    
    st.markdown("""
    å¯¹å…‰è°±æ ·æœ¬è¿›è¡Œèšç±»åˆ†æï¼Œå‘ç°æ•°æ®ä¸­çš„æ½œåœ¨åˆ†ç»„ç»“æ„ï¼š
    - æ ·æœ¬ç›¸ä¼¼æ€§åˆ†æ
    - å¼‚å¸¸æ ·æœ¬è¯†åˆ«
    - æ•°æ®ç»“æ„æ¢ç´¢
    """)
    
    from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
    from sklearn.preprocessing import StandardScaler
    
    # èšç±»æ–¹æ³•é€‰æ‹©
    clustering_method = st.selectbox(
        "èšç±»æ–¹æ³•",
        ["K-Means", "å±‚æ¬¡èšç±»", "DBSCAN"],
        help="ä¸åŒèšç±»æ–¹æ³•é€‚ç”¨äºä¸åŒçš„æ•°æ®ç»“æ„",
        key="clustering_method_selectbox"  # æ·»åŠ å”¯ä¸€key
    )
    
    # å‚æ•°è®¾ç½®
    col1, col2 = st.columns(2)
    
    with col1:
        standardize = st.checkbox("æ•°æ®æ ‡å‡†åŒ–", value=True, key="clustering_standardize_checkbox")  # æ·»åŠ å”¯ä¸€key
        
        if clustering_method in ["K-Means", "å±‚æ¬¡èšç±»"]:
            n_clusters = st.slider("èšç±»æ•°é‡", 2, min(10, X.shape[0]//2), 3, key="clustering_n_clusters_slider")  # æ·»åŠ å”¯ä¸€key
    
    with col2:
        if clustering_method == "DBSCAN":
            eps = st.slider("é‚»åŸŸåŠå¾„ (eps)", 0.1, 2.0, 0.5, 0.1, key="clustering_eps_slider")  # æ·»åŠ å”¯ä¸€key
            min_samples = st.slider("æœ€å°æ ·æœ¬æ•°", 2, 10, 3, key="clustering_min_samples_slider")  # æ·»åŠ å”¯ä¸€key
        
        # é™ç»´å¯è§†åŒ–
        use_pca_viz = st.checkbox("ä½¿ç”¨PCAé™ç»´å¯è§†åŒ–", value=True, key="clustering_pca_viz_checkbox")  # æ·»åŠ å”¯ä¸€key
    
    if st.button("æ‰§è¡Œèšç±»åˆ†æ", key="clustering_execute_button"):  # æ·»åŠ å”¯ä¸€key
        with st.spinner("æ­£åœ¨è¿›è¡Œèšç±»åˆ†æ..."):
            try:
                # æ•°æ®é¢„å¤„ç†
                if standardize:
                    scaler = StandardScaler()
                    X_scaled = scaler.fit_transform(X)
                else:
                    X_scaled = X
                
                # æ‰§è¡Œèšç±»
                if clustering_method == "K-Means":
                    model = KMeans(n_clusters=n_clusters, random_state=42)
                    labels = model.fit_predict(X_scaled)
                    
                elif clustering_method == "å±‚æ¬¡èšç±»":
                    model = AgglomerativeClustering(n_clusters=n_clusters)
                    labels = model.fit_predict(X_scaled)
                    
                elif clustering_method == "DBSCAN":
                    model = DBSCAN(eps=eps, min_samples=min_samples)
                    labels = model.fit_predict(X_scaled)
                    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
                    n_noise = list(labels).count(-1)
                    
                    st.info(f"å‘ç° {n_clusters} ä¸ªèšç±»ï¼Œ{n_noise} ä¸ªå™ªå£°ç‚¹")
                
                # èšç±»ç»“æœåˆ†æ
                unique_labels = set(labels)
                n_clusters_found = len(unique_labels) - (1 if -1 in unique_labels else 0)
                
                st.success(f"âœ… èšç±»å®Œæˆï¼å‘ç° {n_clusters_found} ä¸ªèšç±»")
                
                # å¯è§†åŒ–ç»“æœ
                if use_pca_viz and X.shape[1] > 2:
                    # PCAé™ç»´å¯è§†åŒ–
                    from sklearn.decomposition import PCA
                    
                    pca = PCA(n_components=2)
                    X_pca = pca.fit_transform(X_scaled)
                    
                    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))
                    
                    # PCAèšç±»ç»“æœ
                    colors = plt.cm.Set3(np.linspace(0, 1, len(unique_labels)))
                    for k, col in zip(unique_labels, colors):
                        if k == -1:
                            # å™ªå£°ç‚¹ç”¨é»‘è‰²
                            col = 'black'
                        
                        class_member_mask = (labels == k)
                        xy = X_pca[class_member_mask]
                        
                        label_name = f'èšç±» {k}' if k != -1 else 'å™ªå£°'
                        ax1.scatter(xy[:, 0], xy[:, 1], c=[col], alpha=0.7, s=50, label=label_name)
                    
                    ax1.set_title(f'{clustering_method} èšç±»ç»“æœ (PCAå¯è§†åŒ–)')
                    ax1.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})')
                    ax1.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})')
                    ax1.legend()
                    ax1.grid(True, alpha=0.3)
                    
                else:
                    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))
                
                # èšç±»ä¸­å¿ƒå…‰è°±ï¼ˆå¦‚æœé€‚ç”¨ï¼‰
                if clustering_method != "DBSCAN":
                    cluster_centers = []
                    for k in unique_labels:
                        if k != -1:
                            mask = (labels == k)
                            center = np.mean(X[mask], axis=0)
                            cluster_centers.append(center)
                            ax2.plot(wavenumbers, center, label=f'èšç±» {k} ä¸­å¿ƒ', alpha=0.8)
                    
                    ax2.set_title('å„èšç±»ä¸­å¿ƒå…‰è°±')
                    ax2.set_xlabel('æ³¢æ•° (cmâ»Â¹)')
                    ax2.set_ylabel('å¼ºåº¦')
                    ax2.legend()
                    ax2.grid(True, alpha=0.3)
                
                else:
                    # DBSCANæ˜¾ç¤ºå„èšç±»çš„ä»£è¡¨å…‰è°±
                    for k in unique_labels:
                        if k != -1:
                            mask = (labels == k)
                            if np.sum(mask) > 0:
                                center = np.mean(X[mask], axis=0)
                                ax2.plot(wavenumbers, center, label=f'èšç±» {k}', alpha=0.8)
                    
                    ax2.set_title('å„èšç±»ä»£è¡¨å…‰è°±')
                    ax2.set_xlabel('æ³¢æ•° (cmâ»Â¹)')
                    ax2.set_ylabel('å¼ºåº¦')
                    ax2.legend()
                    ax2.grid(True, alpha=0.3)
                
                # èšç±»å¤§å°åˆ†å¸ƒ
                cluster_sizes = []
                cluster_names = []
                for k in unique_labels:
                    size = np.sum(labels == k)
                    cluster_sizes.append(size)
                    cluster_names.append(f'èšç±» {k}' if k != -1 else 'å™ªå£°')
                
                ax3.bar(cluster_names, cluster_sizes, alpha=0.7)
                ax3.set_title('èšç±»å¤§å°åˆ†å¸ƒ')
                ax3.set_xlabel('èšç±»')
                ax3.set_ylabel('æ ·æœ¬æ•°é‡')
                ax3.tick_params(axis='x', rotation=45)
                ax3.grid(True, alpha=0.3)
                
                # èšç±»å†…è·ç¦»åˆ†å¸ƒ
                if clustering_method == "K-Means":
                    distances_to_center = []
                    for i, label in enumerate(labels):
                        if label != -1:
                            center = model.cluster_centers_[label]
                            dist = np.linalg.norm(X_scaled[i] - center)
                            distances_to_center.append(dist)
                    
                    ax4.hist(distances_to_center, bins=20, alpha=0.7)
                    ax4.set_title('æ ·æœ¬åˆ°èšç±»ä¸­å¿ƒè·ç¦»åˆ†å¸ƒ')
                    ax4.set_xlabel('è·ç¦»')
                    ax4.set_ylabel('é¢‘æ•°')
                    ax4.grid(True, alpha=0.3)
                
                plt.tight_layout()
                st.pyplot(fig)
                
                # èšç±»ç»“æœè¡¨æ ¼
                st.write("### ğŸ“‹ èšç±»ç»“æœæ‘˜è¦")
                
                cluster_summary = []
                for k in unique_labels:
                    mask = (labels == k)
                    size = np.sum(mask)
                    
                    if size > 0:
                        cluster_data = X[mask]
                        
                        cluster_summary.append({
                            'èšç±»': f'èšç±» {k}' if k != -1 else 'å™ªå£°',
                            'æ ·æœ¬æ•°': size,
                            'å æ¯”': f'{size/len(labels)*100:.1f}%',
                            'å¹³å‡å¼ºåº¦': f'{np.mean(cluster_data):.4f}',
                            'å¼ºåº¦æ ‡å‡†å·®': f'{np.std(cluster_data):.4f}',
                            'æ ·æœ¬ç¼–å·': ', '.join([str(i+1) for i in np.where(mask)[0][:5]]) + 
                                     ('...' if size > 5 else '')
                        })
                
                cluster_df = pd.DataFrame(cluster_summary)
                st.dataframe(cluster_df, use_container_width=True)
                
                # ä¿å­˜ç»“æœ
                st.session_state.clustering_results = {
                    'method': clustering_method,
                    'model': model,
                    'labels': labels,
                    'n_clusters': n_clusters_found,
                    'cluster_centers': cluster_centers if clustering_method != "DBSCAN" else None
                }
                
                # å¦‚æœæœ‰æ ‡ç­¾ï¼Œåˆ†æèšç±»ä¸æ ‡ç­¾çš„å…³ç³»
                if hasattr(st.session_state, 'y') and st.session_state.y is not None:
                    show_clustering_label_relationship(labels, unique_labels)
                
            except Exception as e:
                st.error(f"èšç±»åˆ†æå‡ºé”™: {e}")
                st.error(traceback.format_exc())


def show_clustering_label_relationship(labels, unique_labels):
    """æ˜¾ç¤ºèšç±»ä¸æ ‡ç­¾çš„å…³ç³»"""
    st.write("### ğŸ¯ èšç±»ä¸æ ‡ç­¾å…³ç³»")
    
    y = st.session_state.y
    
    if y.ndim == 1:
        # å•æ ‡ç­¾æƒ…å†µ
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
        
        # å„èšç±»çš„æ ‡ç­¾åˆ†å¸ƒç®±çº¿å›¾
        cluster_data = []
        cluster_names = []
        
        for k in unique_labels:
            if k != -1:
                mask = (labels == k)
                if np.sum(mask) > 0:
                    cluster_data.append(y[mask])
                    cluster_names.append(f'èšç±» {k}')
        
        if cluster_data:
            ax1.boxplot(cluster_data, labels=cluster_names)
            ax1.set_title('å„èšç±»æ ‡ç­¾å€¼åˆ†å¸ƒ')
            ax1.set_xlabel('èšç±»')
            ax1.set_ylabel('æ ‡ç­¾å€¼')
            ax1.grid(True, alpha=0.3)
        
        # èšç±»æ ‡ç­¾å‡å€¼å¯¹æ¯”
        cluster_means = []
        cluster_stds = []
        valid_clusters = []
        
        for k in unique_labels:
            if k != -1:
                mask = (labels == k)
                if np.sum(mask) > 0:
                    cluster_means.append(np.mean(y[mask]))
                    cluster_stds.append(np.std(y[mask]))
                    valid_clusters.append(k)
        
        if cluster_means:
            ax2.bar(range(len(valid_clusters)), cluster_means, 
                   yerr=cluster_stds, alpha=0.7, capsize=5)
            ax2.set_xticks(range(len(valid_clusters)))
            ax2.set_xticklabels([f'èšç±» {k}' for k in valid_clusters])
            ax2.set_title('å„èšç±»æ ‡ç­¾å‡å€¼ Â± æ ‡å‡†å·®')
            ax2.set_xlabel('èšç±»')
            ax2.set_ylabel('æ ‡ç­¾å‡å€¼')
            ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        st.pyplot(fig)
        
        # æ˜¾ç¤ºç»Ÿè®¡è¡¨æ ¼
        if cluster_means:
            cluster_stats = pd.DataFrame({
                'èšç±»': [f'èšç±» {k}' for k in valid_clusters],
                'æ ·æœ¬æ•°': [np.sum(labels == k) for k in valid_clusters],
                'æ ‡ç­¾å‡å€¼': [f'{mean:.4f}' for mean in cluster_means],
                'æ ‡ç­¾æ ‡å‡†å·®': [f'{std:.4f}' for std in cluster_stds]
            })
            st.dataframe(cluster_stats, use_container_width=True)
    
    else:
        # å¤šæ ‡ç­¾æƒ…å†µ
        target_names = st.session_state.selected_cols
        selected_target = st.selectbox(
            "é€‰æ‹©ç›®æ ‡å˜é‡æŸ¥çœ‹ä¸èšç±»çš„å…³ç³»", 
            range(len(target_names)), 
            format_func=lambda x: target_names[x],
            key="clustering_target_select"
        )
        
        y_selected = y[:, selected_target]
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
        
        # å„èšç±»çš„æ ‡ç­¾åˆ†å¸ƒ
        cluster_data = []
        cluster_names = []
        
        for k in unique_labels:
            if k != -1:
                mask = (labels == k)
                if np.sum(mask) > 0:
                    cluster_data.append(y_selected[mask])
                    cluster_names.append(f'èšç±» {k}')
        
        if cluster_data:
            ax1.boxplot(cluster_data, labels=cluster_names)
            ax1.set_title(f'å„èšç±» {target_names[selected_target]} åˆ†å¸ƒ')
            ax1.set_xlabel('èšç±»')
            ax1.set_ylabel(target_names[selected_target])
            ax1.grid(True, alpha=0.3)
            
            # èšç±»å‡å€¼å¯¹æ¯”
            cluster_means = [np.mean(data) for data in cluster_data]
            cluster_stds = [np.std(data) for data in cluster_data]
            
            ax2.bar(range(len(cluster_names)), cluster_means, 
                   yerr=cluster_stds, alpha=0.7, capsize=5)
            ax2.set_xticks(range(len(cluster_names)))
            ax2.set_xticklabels(cluster_names)
            ax2.set_title(f'å„èšç±» {target_names[selected_target]} å‡å€¼')
            ax2.set_xlabel('èšç±»')
            ax2.set_ylabel(f'{target_names[selected_target]} å‡å€¼')
            ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        st.pyplot(fig)


def show_anomaly_detection(X, wavenumbers):
    """å¼‚å¸¸æ£€æµ‹"""
    func_key = "anomaly_detection"
    st.write("### ğŸš¨ å¼‚å¸¸æ£€æµ‹")
    
    st.markdown("""
    æ£€æµ‹å…‰è°±æ•°æ®ä¸­çš„å¼‚å¸¸æ ·æœ¬ï¼š
    - è¯†åˆ«æµ‹é‡å¼‚å¸¸
    - å‘ç°æ ·æœ¬æ±¡æŸ“
    - è´¨é‡æ§åˆ¶åˆ†æ
    """)
    
    from sklearn.ensemble import IsolationForest
    from sklearn.svm import OneClassSVM
    from sklearn.preprocessing import StandardScaler
    
    # å¼‚å¸¸æ£€æµ‹æ–¹æ³•é€‰æ‹©
    detection_method = st.selectbox(
        "å¼‚å¸¸æ£€æµ‹æ–¹æ³•",
        ["Isolation Forest", "One-Class SVM", "ç»Ÿè®¡æ–¹æ³•"],
        help="ä¸åŒæ–¹æ³•é€‚ç”¨äºä¸åŒç±»å‹çš„å¼‚å¸¸"
    )
    
    # å‚æ•°è®¾ç½®
    col1, col2 = st.columns(2)
    
    with col1:
        contamination = st.slider(
            "å¼‚å¸¸æ¯”ä¾‹ä¼°è®¡", 
            0.01, 0.5, 0.1, 0.01,
            help="é¢„æœŸå¼‚å¸¸æ ·æœ¬çš„æ¯”ä¾‹"
        )
        
        standardize = st.checkbox("æ•°æ®æ ‡å‡†åŒ–", value=True, key=f"{func_key}_standardize")
    
    with col2:
        if detection_method == "One-Class SVM":
            nu = st.slider("Nuå‚æ•°", 0.01, 0.5, 0.1, 0.01)
            kernel = st.selectbox("æ ¸å‡½æ•°", ["rbf", "linear", "poly"])
        
        elif detection_method == "ç»Ÿè®¡æ–¹æ³•":
            method = st.selectbox("ç»Ÿè®¡æ–¹æ³•", ["Z-score", "IQR", "Mahalanobis"])
            threshold = st.slider("é˜ˆå€¼", 1.0, 5.0, 2.0, 0.1)
    
    if st.button("æ‰§è¡Œå¼‚å¸¸æ£€æµ‹"):
        with st.spinner("æ­£åœ¨è¿›è¡Œå¼‚å¸¸æ£€æµ‹..."):
            try:
                # æ•°æ®é¢„å¤„ç†
                if standardize:
                    scaler = StandardScaler()
                    X_scaled = scaler.fit_transform(X)
                else:
                    X_scaled = X
                
                # æ‰§è¡Œå¼‚å¸¸æ£€æµ‹
                if detection_method == "Isolation Forest":
                    model = IsolationForest(contamination=contamination, random_state=42)
                    anomaly_labels = model.fit_predict(X_scaled)
                    anomaly_scores = model.decision_function(X_scaled)
                    
                elif detection_method == "One-Class SVM":
                    model = OneClassSVM(nu=nu, kernel=kernel)
                    anomaly_labels = model.fit_predict(X_scaled)
                    anomaly_scores = model.decision_function(X_scaled)
                    
                elif detection_method == "ç»Ÿè®¡æ–¹æ³•":
                    if method == "Z-score":
                        # åŸºäºZ-scoreçš„å¼‚å¸¸æ£€æµ‹
                        z_scores = np.abs((X_scaled - np.mean(X_scaled, axis=0)) / np.std(X_scaled, axis=0))
                        max_z_scores = np.max(z_scores, axis=1)
                        anomaly_labels = np.where(max_z_scores > threshold, -1, 1)
                        anomaly_scores = -max_z_scores
                        
                    elif method == "IQR":
                        # åŸºäºå››åˆ†ä½è·çš„å¼‚å¸¸æ£€æµ‹
                        Q1 = np.percentile(X_scaled, 25, axis=0)
                        Q3 = np.percentile(X_scaled, 75, axis=0)
                        IQR = Q3 - Q1
                        
                        lower_bound = Q1 - threshold * IQR
                        upper_bound = Q3 + threshold * IQR
                        
                        outlier_mask = (X_scaled < lower_bound) | (X_scaled > upper_bound)
                        outlier_count = np.sum(outlier_mask, axis=1)
                        anomaly_labels = np.where(outlier_count > 0, -1, 1)
                        anomaly_scores = -outlier_count
                        
                    else:  # Mahalanobis
                        # åŸºäºé©¬æ°è·ç¦»çš„å¼‚å¸¸æ£€æµ‹
                        mean = np.mean(X_scaled, axis=0)
                        cov = np.cov(X_scaled.T)
                        
                        # è®¡ç®—é©¬æ°è·ç¦»
                        inv_cov = np.linalg.pinv(cov)
                        mahal_dist = []
                        for i in range(X_scaled.shape[0]):
                            diff = X_scaled[i] - mean
                            mahal_dist.append(np.sqrt(diff.T @ inv_cov @ diff))
                        
                        mahal_dist = np.array(mahal_dist)
                        anomaly_labels = np.where(mahal_dist > threshold, -1, 1)
                        anomaly_scores = -mahal_dist
                
                # åˆ†æç»“æœ
                n_anomalies = np.sum(anomaly_labels == -1)
                anomaly_indices = np.where(anomaly_labels == -1)[0]
                
                st.success(f"âœ… å¼‚å¸¸æ£€æµ‹å®Œæˆï¼å‘ç° {n_anomalies} ä¸ªå¼‚å¸¸æ ·æœ¬")
                
                if n_anomalies > 0:
                    st.warning(f"å¼‚å¸¸æ ·æœ¬ç¼–å·: {', '.join([str(i+1) for i in anomaly_indices])}")
                
                # å¯è§†åŒ–ç»“æœ
                fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))
                
                # 1. å¼‚å¸¸åˆ†æ•°åˆ†å¸ƒ
                ax1.hist(anomaly_scores, bins=30, alpha=0.7, color='blue', edgecolor='black')
                if n_anomalies > 0:
                    ax1.axvline(np.max(anomaly_scores[anomaly_labels == -1]), 
                               color='red', linestyle='--', label='å¼‚å¸¸é˜ˆå€¼')
                ax1.set_title('å¼‚å¸¸åˆ†æ•°åˆ†å¸ƒ')
                ax1.set_xlabel('å¼‚å¸¸åˆ†æ•°')
                ax1.set_ylabel('é¢‘æ•°')
                ax1.legend()
                ax1.grid(True, alpha=0.3)
                
                # 2. æ ·æœ¬å¼‚å¸¸åˆ†æ•°
                colors = ['red' if label == -1 else 'blue' for label in anomaly_labels]
                ax2.scatter(range(len(anomaly_scores)), anomaly_scores, c=colors, alpha=0.7)
                ax2.set_title('å„æ ·æœ¬å¼‚å¸¸åˆ†æ•°')
                ax2.set_xlabel('æ ·æœ¬ç´¢å¼•')
                ax2.set_ylabel('å¼‚å¸¸åˆ†æ•°')
                ax2.grid(True, alpha=0.3)
                
                # æ·»åŠ å›¾ä¾‹
                from matplotlib.patches import Patch
                legend_elements = [Patch(facecolor='blue', label='æ­£å¸¸'),
                                 Patch(facecolor='red', label='å¼‚å¸¸')]
                ax2.legend(handles=legend_elements)
                
                # 3. æ­£å¸¸vså¼‚å¸¸å…‰è°±å¯¹æ¯”
                normal_indices = np.where(anomaly_labels == 1)[0]
                
                if len(normal_indices) > 0:
                    normal_mean = np.mean(X[normal_indices], axis=0)
                    normal_std = np.std(X[normal_indices], axis=0)
                    
                    ax3.plot(wavenumbers, normal_mean, 'b-', label='æ­£å¸¸æ ·æœ¬å‡å€¼', linewidth=2)
                    ax3.fill_between(wavenumbers, 
                                    normal_mean - normal_std, 
                                    normal_mean + normal_std,
                                    alpha=0.3, color='blue', label='æ­£å¸¸æ ·æœ¬Â±1Ïƒ')
                    
                    # æ˜¾ç¤ºå¼‚å¸¸æ ·æœ¬
                    if n_anomalies > 0:
                        for i, idx in enumerate(anomaly_indices[:3]):  # æœ€å¤šæ˜¾ç¤º3ä¸ªå¼‚å¸¸æ ·æœ¬
                            ax3.plot(wavenumbers, X[idx], 'r--', alpha=0.7, 
                                    label=f'å¼‚å¸¸æ ·æœ¬ {idx+1}' if i == 0 else "")
                    
                    ax3.set_title('æ­£å¸¸ vs å¼‚å¸¸å…‰è°±å¯¹æ¯”')
                    ax3.set_xlabel('æ³¢æ•° (cmâ»Â¹)')
                    ax3.set_ylabel('å¼ºåº¦')
                    ax3.legend()
                    ax3.grid(True, alpha=0.3)
                
                # 4. PCAå¯è§†åŒ–å¼‚å¸¸æ£€æµ‹ç»“æœ
                if X.shape[1] > 2:
                    from sklearn.decomposition import PCA
                    
                    pca = PCA(n_components=2)
                    X_pca = pca.fit_transform(X_scaled)
                    
                    colors = ['red' if label == -1 else 'blue' for label in anomaly_labels]
                    ax4.scatter(X_pca[:, 0], X_pca[:, 1], c=colors, alpha=0.7)
                    
                    ax4.set_title('å¼‚å¸¸æ£€æµ‹ç»“æœ (PCAå¯è§†åŒ–)')
                    ax4.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})')
                    ax4.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})')
                    ax4.grid(True, alpha=0.3)
                    
                    # æ·»åŠ å›¾ä¾‹
                    ax4.legend(handles=legend_elements)
                
                plt.tight_layout()
                st.pyplot(fig)
                
                # å¼‚å¸¸æ ·æœ¬è¯¦ç»†ä¿¡æ¯
                if n_anomalies > 0:
                    st.write("### ğŸ“‹ å¼‚å¸¸æ ·æœ¬è¯¦ç»†ä¿¡æ¯")
                    
                    anomaly_details = []
                    for idx in anomaly_indices:
                        anomaly_details.append({
                            'æ ·æœ¬ç¼–å·': idx + 1,
                            'å¼‚å¸¸åˆ†æ•°': f'{anomaly_scores[idx]:.4f}',
                            'å¹³å‡å¼ºåº¦': f'{np.mean(X[idx]):.4f}',
                            'å¼ºåº¦æ ‡å‡†å·®': f'{np.std(X[idx]):.4f}',
                            'æœ€å¤§å¼ºåº¦': f'{np.max(X[idx]):.4f}',
                            'æœ€å°å¼ºåº¦': f'{np.min(X[idx]):.4f}'
                        })
                    
                    anomaly_df = pd.DataFrame(anomaly_details)
                    st.dataframe(anomaly_df, use_container_width=True)
                
                # ä¿å­˜ç»“æœ
                st.session_state.anomaly_detection_results = {
                    'method': detection_method,
                    'anomaly_labels': anomaly_labels,
                    'anomaly_scores': anomaly_scores,
                    'anomaly_indices': anomaly_indices,
                    'n_anomalies': n_anomalies
                }
                
            except Exception as e:
                st.error(f"å¼‚å¸¸æ£€æµ‹å‡ºé”™: {e}")
                st.error(traceback.format_exc())


def show_comprehensive_report(X, wavenumbers):
    """ç»¼åˆæŠ¥å‘Š"""
    st.write("### ğŸ“„ ç»¼åˆåˆ†ææŠ¥å‘Š")
    
    st.markdown("""
    ç”Ÿæˆå…‰è°±æ•°æ®çš„ç»¼åˆåˆ†ææŠ¥å‘Šï¼Œæ•´åˆæ‰€æœ‰åˆ†æç»“æœã€‚
    """)
    
    if st.button("ç”Ÿæˆç»¼åˆæŠ¥å‘Š"):
        with st.spinner("æ­£åœ¨ç”Ÿæˆç»¼åˆæŠ¥å‘Š..."):
            try:
                # æŠ¥å‘Šæ ‡é¢˜
                st.markdown("## ğŸ”¬ å…‰è°±æ•°æ®åˆ†æç»¼åˆæŠ¥å‘Š")
                st.markdown("---")
                
                # 1. æ•°æ®æ¦‚è§ˆ
                st.markdown("### ğŸ“Š æ•°æ®æ¦‚è§ˆ")
                
                data_overview = {
                    "æ•°æ®é¡¹": ["æ ·æœ¬æ•°é‡", "ç‰¹å¾æ•°é‡", "æ³¢æ•°èŒƒå›´", "æ•°æ®ç±»å‹"],
                    "æ•°å€¼": [
                        f"{X.shape[0]}",
                        f"{X.shape[1]}",
                        f"{wavenumbers.min():.1f} - {wavenumbers.max():.1f} cmâ»Â¹",
                        "æœ‰æ ‡ç­¾æ•°æ®" if hasattr(st.session_state, 'y') and st.session_state.y is not None else "æ— æ ‡ç­¾æ•°æ®"
                    ]
                }
                
                overview_df = pd.DataFrame(data_overview)
                st.dataframe(overview_df, use_container_width=True)
                
                # 2. æ•°æ®è´¨é‡è¯„ä¼°
                st.markdown("### ğŸ” æ•°æ®è´¨é‡è¯„ä¼°")
                
                # åŸºæœ¬ç»Ÿè®¡
                missing_ratio = np.sum(np.isnan(X)) / X.size * 100
                zero_ratio = np.sum(X == 0) / X.size * 100
                
                quality_metrics = {
                    "è´¨é‡æŒ‡æ ‡": ["ç¼ºå¤±å€¼æ¯”ä¾‹", "é›¶å€¼æ¯”ä¾‹", "æ•°æ®èŒƒå›´", "å¹³å‡ä¿¡å™ªæ¯”ä¼°è®¡"],
                    "æ•°å€¼": [
                        f"{missing_ratio:.2f}%",
                        f"{zero_ratio:.2f}%",
                        f"{X.min():.4f} - {X.max():.4f}",
                        f"{np.mean(X) / np.std(X):.2f}"
                    ]
                }
                
                quality_df = pd.DataFrame(quality_metrics)
                st.dataframe(quality_df, use_container_width=True)
                
                # 3. åˆ†æç»“æœæ±‡æ€»
                st.markdown("### ğŸ“ˆ åˆ†æç»“æœæ±‡æ€»")
                
                analysis_summary = []
                
                # PCAç»“æœ
                if hasattr(st.session_state, 'pca_results'):
                    pca_results = st.session_state.pca_results
                    total_var = np.sum(pca_results['explained_variance_ratio'])
                    analysis_summary.append({
                        "åˆ†ææ–¹æ³•": "ä¸»æˆåˆ†åˆ†æ (PCA)",
                        "ä¸»è¦å‘ç°": f"å‰{len(pca_results['explained_variance_ratio'])}ä¸ªä¸»æˆåˆ†è§£é‡Š{total_var:.1%}çš„æ–¹å·®",
                        "å»ºè®®": "æ•°æ®é™ç»´æ•ˆæœè‰¯å¥½" if total_var > 0.8 else "è€ƒè™‘å¢åŠ ä¸»æˆåˆ†æ•°é‡"
                    })
                
                # èšç±»ç»“æœ
                if hasattr(st.session_state, 'clustering_results'):
                    clustering_results = st.session_state.clustering_results
                    analysis_summary.append({
                        "åˆ†ææ–¹æ³•": f"èšç±»åˆ†æ ({clustering_results['method']})",
                        "ä¸»è¦å‘ç°": f"å‘ç°{clustering_results['n_clusters']}ä¸ªèšç±»",
                        "å»ºè®®": "æ ·æœ¬å­˜åœ¨æ˜æ˜¾åˆ†ç»„ç»“æ„" if clustering_results['n_clusters'] > 1 else "æ ·æœ¬ç›¸å¯¹å‡åŒ€åˆ†å¸ƒ"
                    })
                
                # å¼‚å¸¸æ£€æµ‹ç»“æœ
                if hasattr(st.session_state, 'anomaly_detection_results'):
                    anomaly_results = st.session_state.anomaly_detection_results
                    anomaly_ratio = anomaly_results['n_anomalies'] / X.shape[0] * 100
                    analysis_summary.append({
                        "åˆ†ææ–¹æ³•": f"å¼‚å¸¸æ£€æµ‹ ({anomaly_results['method']})",
                        "ä¸»è¦å‘ç°": f"æ£€æµ‹åˆ°{anomaly_results['n_anomalies']}ä¸ªå¼‚å¸¸æ ·æœ¬({anomaly_ratio:.1f}%)",
                        "å»ºè®®": "æ•°æ®è´¨é‡è¾ƒå¥½" if anomaly_ratio < 5 else "å»ºè®®æ£€æŸ¥å¼‚å¸¸æ ·æœ¬"
                    })
                
                # æˆåˆ†åˆ†è§£ç»“æœ
                if hasattr(st.session_state, 'decomposition_results'):
                    decomp_results = st.session_state.decomposition_results
                    analysis_summary.append({
                        "åˆ†ææ–¹æ³•": f"æˆåˆ†åˆ†è§£ ({decomp_results['method']})",
                        "ä¸»è¦å‘ç°": f"åˆ†è§£å‡º{decomp_results['n_components']}ä¸ªåŒ–å­¦æˆåˆ†",
                        "å»ºè®®": "å¯ç”¨äºæˆåˆ†è¯†åˆ«å’Œå®šé‡åˆ†æ"
                    })
                
                if analysis_summary:
                    summary_df = pd.DataFrame(analysis_summary)
                    st.dataframe(summary_df, use_container_width=True)
                else:
                    st.info("è¯·å…ˆæ‰§è¡Œç›¸å…³åˆ†æä»¥ç”Ÿæˆå®Œæ•´æŠ¥å‘Š")
                
                # 4. æ ‡ç­¾ç›¸å…³åˆ†æï¼ˆå¦‚æœæœ‰æ ‡ç­¾ï¼‰
                if hasattr(st.session_state, 'y') and st.session_state.y is not None:
                    st.markdown("### ğŸ¯ æ ‡ç­¾ç›¸å…³åˆ†æ")
                    
                    y = st.session_state.y
                    
                    if y.ndim == 1:
                        label_stats = {
                            "æ ‡ç­¾ç»Ÿè®¡": ["æ ‡ç­¾æ•°é‡", "æ ‡ç­¾èŒƒå›´", "æ ‡ç­¾å‡å€¼", "æ ‡ç­¾æ ‡å‡†å·®"],
                            "æ•°å€¼": [
                                f"{len(y)}",
                                f"{y.min():.4f} - {y.max():.4f}",
                                f"{np.mean(y):.4f}",
                                f"{np.std(y):.4f}"
                            ]
                        }
                    else:
                        target_names = st.session_state.selected_cols
                        label_stats = {
                            "æ ‡ç­¾ç»Ÿè®¡": ["ç›®æ ‡å˜é‡æ•°é‡", "ç›®æ ‡å˜é‡åç§°", "æ ·æœ¬æ•°é‡"],
                            "æ•°å€¼": [
                                f"{y.shape[1]}",
                                ", ".join(target_names),
                                f"{y.shape[0]}"
                            ]
                        }
                    
                    label_df = pd.DataFrame(label_stats)
                    st.dataframe(label_df, use_container_width=True)
                
                # 5. å»ºè®®å’Œç»“è®º
                st.markdown("### ğŸ’¡ åˆ†æå»ºè®®å’Œç»“è®º")
                
                recommendations = []
                
                # åŸºäºæ•°æ®è´¨é‡çš„å»ºè®®
                if missing_ratio > 5:
                    recommendations.append("â€¢ æ•°æ®å­˜åœ¨è¾ƒå¤šç¼ºå¤±å€¼ï¼Œå»ºè®®è¿›è¡Œæ•°æ®æ¸…æ´—")
                
                if zero_ratio > 10:
                    recommendations.append("â€¢ æ•°æ®ä¸­é›¶å€¼è¾ƒå¤šï¼Œå¯èƒ½éœ€è¦æ£€æŸ¥æ•°æ®é‡‡é›†è¿‡ç¨‹")
                
                # åŸºäºåˆ†æç»“æœçš„å»ºè®®
                if hasattr(st.session_state, 'pca_results'):
                    pca_results = st.session_state.pca_results
                    if np.sum(pca_results['explained_variance_ratio'][:2]) > 0.8:
                        recommendations.append("â€¢ å‰ä¸¤ä¸ªä¸»æˆåˆ†è§£é‡Šäº†å¤§éƒ¨åˆ†æ–¹å·®ï¼Œé€‚åˆè¿›è¡Œé™ç»´å¯è§†åŒ–")
                
                if hasattr(st.session_state, 'clustering_results'):
                    clustering_results = st.session_state.clustering_results
                    if clustering_results['n_clusters'] > 3:
                        recommendations.append("â€¢ æ ·æœ¬å­˜åœ¨å¤šä¸ªåˆ†ç»„ï¼Œå»ºè®®è¿›ä¸€æ­¥åˆ†æå„ç»„ç‰¹å¾")
                
                if hasattr(st.session_state, 'anomaly_detection_results'):
                    anomaly_results = st.session_state.anomaly_detection_results
                    if anomaly_results['n_anomalies'] > 0:
                        recommendations.append(f"â€¢ å‘ç°{anomaly_results['n_anomalies']}ä¸ªå¼‚å¸¸æ ·æœ¬ï¼Œå»ºè®®è¿›è¡Œäººå·¥å®¡æŸ¥")
                
                # é€šç”¨å»ºè®®
                recommendations.extend([
                    "â€¢ å»ºè®®ç»“åˆé¢†åŸŸçŸ¥è¯†è§£é‡Šåˆ†æç»“æœ",
                    "â€¢ å¯è€ƒè™‘è¿›è¡Œäº¤å‰éªŒè¯ä»¥è¯„ä¼°æ¨¡å‹ç¨³å®šæ€§",
                    "â€¢ å¦‚éœ€é¢„æµ‹å»ºæ¨¡ï¼Œå»ºè®®è¿›è¡Œç‰¹å¾é€‰æ‹©å’Œæ¨¡å‹ä¼˜åŒ–"
                ])
                
                for rec in recommendations:
                    st.markdown(rec)
                
                # 6. ç”ŸæˆæŠ¥å‘Šæ—¶é—´
                from datetime import datetime
                current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                st.markdown(f"**æŠ¥å‘Šç”Ÿæˆæ—¶é—´**: {current_time}")
                
                st.success("âœ… ç»¼åˆæŠ¥å‘Šç”Ÿæˆå®Œæˆï¼")
                
            except Exception as e:
                st.error(f"ç”ŸæˆæŠ¥å‘Šæ—¶å‡ºé”™: {e}")
                st.error(traceback.format_exc())


def show_data_split_page():
    """æ•°æ®é›†åˆ’åˆ†é¡µé¢"""
    st.markdown("<h1 class='section-header'>æ•°æ®é›†åˆ’åˆ†</h1>", unsafe_allow_html=True)
    
    # æ£€æŸ¥å‰ç½®æ¡ä»¶
    if not check_data_prerequisites(need_labels=True, need_preprocessing=False):
        return
    
    st.markdown("""
    <div class="info-box">
    å°†æ•°æ®åˆ’åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼Œæˆ–è®¾ç½®äº¤å‰éªŒè¯æ–¹æ¡ˆã€‚
    </div>
    """, unsafe_allow_html=True)
    
    # è·å–å½“å‰æ•°æ®
    X, wavenumbers, data_info = get_current_data()
    show_status_message(data_info, "info")
    
    y = st.session_state.y
    
    st.info(f"ğŸ“Š æ•°æ®å½¢çŠ¶: {X.shape}, æ ‡ç­¾å½¢çŠ¶: {y.shape}")
    
    # åˆ’åˆ†æ–¹æ³•é€‰æ‹©
    st.subheader("ğŸ“Š æ•°æ®åˆ’åˆ†æ–¹æ¡ˆ")
    
    split_method = st.selectbox(
        "é€‰æ‹©æ•°æ®åˆ’åˆ†æ–¹æ³•",
        ["éšæœºåˆ’åˆ†", "KFoldäº¤å‰éªŒè¯", "ç•™ä¸€æ³•(LOOCV)"],
        help="ä¸åŒåˆ’åˆ†æ–¹æ³•é€‚ç”¨äºä¸åŒçš„æ•°æ®è§„æ¨¡å’ŒéªŒè¯éœ€æ±‚"
    )
    
    # å‚æ•°è®¾ç½®
    if split_method == "éšæœºåˆ’åˆ†":
        col1, col2 = st.columns(2)
        
        with col1:
            test_size = st.slider("æµ‹è¯•é›†æ¯”ä¾‹", 0.1, 0.5, 0.2, 0.05)
            random_state = st.number_input("éšæœºç§å­", value=42, min_value=0)
        
        with col2:
            stratify = st.checkbox("åˆ†å±‚æŠ½æ ·", value=False, 
                                 help="ä¿æŒè®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸­æ ‡ç­¾åˆ†å¸ƒä¸€è‡´")
    
    elif split_method == "KFoldäº¤å‰éªŒè¯":
        col1, col2 = st.columns(2)
        
        with col1:
            n_splits = st.slider("äº¤å‰éªŒè¯æŠ˜æ•°", 3, 10, 5)
            random_state = st.number_input("éšæœºç§å­", value=42, min_value=0)
        
        with col2:
            shuffle = st.checkbox("æ‰“ä¹±æ•°æ®", value=True)
    
    else:  # LOOCV
        st.info("ç•™ä¸€æ³•äº¤å‰éªŒè¯å°†ä½¿ç”¨ n-1 ä¸ªæ ·æœ¬è®­ç»ƒï¼Œ1ä¸ªæ ·æœ¬æµ‹è¯•ï¼Œé‡å¤ n æ¬¡")
        if X.shape[0] > 100:
            st.warning("âš ï¸ æ ·æœ¬æ•°é‡è¾ƒå¤šï¼Œç•™ä¸€æ³•å¯èƒ½è®¡ç®—æ—¶é—´è¾ƒé•¿")
    
    # æ‰§è¡Œæ•°æ®åˆ’åˆ†
    if st.button("ğŸš€ æ‰§è¡Œæ•°æ®åˆ’åˆ†"):
        try:
            from sklearn.model_selection import train_test_split
            
            if split_method == "éšæœºåˆ’åˆ†":
                # å¤„ç†åˆ†å±‚æŠ½æ ·
                stratify_y = None
                if stratify:
                    if y.ndim == 1:
                        # å•è¾“å‡ºï¼šæ£€æŸ¥æ˜¯å¦é€‚åˆåˆ†å±‚
                        unique_values = np.unique(y)
                        if len(unique_values) < X.shape[0] * 0.5:  # ç¦»æ•£å€¼è¾ƒå°‘
                            stratify_y = y
                        else:
                            # è¿ç»­å€¼ï¼šè½¬æ¢ä¸ºåˆ†ç®±
                            n_bins = min(5, len(unique_values))
                            stratify_y = pd.cut(y, bins=n_bins, labels=False)
                    else:
                        # å¤šè¾“å‡ºï¼šä½¿ç”¨ç¬¬ä¸€ä¸ªç›®æ ‡å˜é‡
                        unique_values = np.unique(y[:, 0])
                        if len(unique_values) < X.shape[0] * 0.5:
                            stratify_y = y[:, 0]
                        else:
                            n_bins = min(5, len(unique_values))
                            stratify_y = pd.cut(y[:, 0], bins=n_bins, labels=False)
                
                # æ‰§è¡Œåˆ’åˆ†
                X_train, X_test, y_train, y_test = train_test_split(
                    X, y, 
                    test_size=test_size, 
                    random_state=random_state,
                    stratify=stratify_y
                )
                
                # ä¿å­˜ç»“æœ
                st.session_state.X_train = X_train
                st.session_state.X_test = X_test
                st.session_state.y_train = y_train
                st.session_state.y_test = y_test
                st.session_state.split_method = split_method
                
                st.success("âœ… æ•°æ®åˆ’åˆ†å®Œæˆï¼")
                st.info(f"ğŸ“Š è®­ç»ƒé›†: {X_train.shape}, æµ‹è¯•é›†: {X_test.shape}")
                
                # æ˜¾ç¤ºåˆ’åˆ†ç»“æœ
                show_split_visualization(X_train, X_test, y_train, y_test, wavenumbers)
                
            elif split_method == "KFoldäº¤å‰éªŒè¯":
                # ä¿å­˜äº¤å‰éªŒè¯å‚æ•°
                st.session_state.X_train = X
                st.session_state.y_train = y
                st.session_state.X_test = None
                st.session_state.y_test = None
                st.session_state.split_method = split_method
                st.session_state.cv_splits = n_splits
                st.session_state.cv_shuffle = shuffle
                st.session_state.cv_random_state = random_state
                
                st.success("âœ… äº¤å‰éªŒè¯è®¾ç½®å®Œæˆï¼")
                st.info(f"ğŸ“Š å°†ä½¿ç”¨ {n_splits} æŠ˜äº¤å‰éªŒè¯")
                
            else:  # LOOCV
                st.session_state.X_train = X
                st.session_state.y_train = y
                st.session_state.X_test = None
                st.session_state.y_test = None
                st.session_state.split_method = split_method
                
                st.success("âœ… ç•™ä¸€æ³•äº¤å‰éªŒè¯è®¾ç½®å®Œæˆï¼")
                st.info(f"ğŸ“Š å°†è¿›è¡Œ {X.shape[0]} æ¬¡ç•™ä¸€æ³•éªŒè¯")
            
        except Exception as e:
            st.error(f"æ•°æ®åˆ’åˆ†å‡ºé”™: {e}")
            st.error(traceback.format_exc())


def show_split_visualization(X_train, X_test, y_train, y_test, wavenumbers):
    """æ˜¾ç¤ºæ•°æ®åˆ’åˆ†å¯è§†åŒ–"""
    st.subheader("ğŸ“ˆ æ•°æ®åˆ’åˆ†å¯è§†åŒ–")
    
    # æ ‡ç­¾åˆ†å¸ƒå¯¹æ¯”
    if y_train.ndim == 1:
        # å•è¾“å‡ºæƒ…å†µ
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))
        
        # è®­ç»ƒé›†æ ‡ç­¾åˆ†å¸ƒ
        ax1.hist(y_train, bins=20, alpha=0.7, color='blue', label='è®­ç»ƒé›†')
        ax1.set_title('è®­ç»ƒé›†æ ‡ç­¾åˆ†å¸ƒ')
        ax1.set_xlabel('æ ‡ç­¾å€¼')
        ax1.set_ylabel('é¢‘æ•°')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # æµ‹è¯•é›†æ ‡ç­¾åˆ†å¸ƒ
        ax2.hist(y_test, bins=20, alpha=0.7, color='orange', label='æµ‹è¯•é›†')
        ax2.set_title('æµ‹è¯•é›†æ ‡ç­¾åˆ†å¸ƒ')
        ax2.set_xlabel('æ ‡ç­¾å€¼')
        ax2.set_ylabel('é¢‘æ•°')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        # è®­ç»ƒé›†vsæµ‹è¯•é›†æ ‡ç­¾å¯¹æ¯”
        ax3.hist(y_train, bins=20, alpha=0.5, color='blue', label='è®­ç»ƒé›†')
        ax3.hist(y_test, bins=20, alpha=0.5, color='orange', label='æµ‹è¯•é›†')
        ax3.set_title('è®­ç»ƒé›† vs æµ‹è¯•é›†æ ‡ç­¾åˆ†å¸ƒå¯¹æ¯”')
        ax3.set_xlabel('æ ‡ç­¾å€¼')
        ax3.set_ylabel('é¢‘æ•°')
        ax3.legend()
        ax3.grid(True, alpha=0.3)
        
        # è®­ç»ƒé›†å’Œæµ‹è¯•é›†å…‰è°±å¯¹æ¯”
        n_samples = min(5, X_train.shape[0], X_test.shape[0])
        
        for i in range(n_samples):
            ax4.plot(wavenumbers, X_train[i], alpha=0.5, color='blue')
            ax4.plot(wavenumbers, X_test[i], alpha=0.5, color='orange')
        
        ax4.set_title('è®­ç»ƒé›† vs æµ‹è¯•é›†å…‰è°±å¯¹æ¯”')
        ax4.set_xlabel('æ³¢æ•° (cmâ»Â¹)')
        ax4.set_ylabel('å¼ºåº¦')
        
        # æ·»åŠ å›¾ä¾‹
        from matplotlib.lines import Line2D
        legend_elements = [Line2D([0], [0], color='blue', alpha=0.7, label='è®­ç»ƒé›†'),
                          Line2D([0], [0], color='orange', alpha=0.7, label='æµ‹è¯•é›†')]
        ax4.legend(handles=legend_elements)
        ax4.grid(True, alpha=0.3)
        
        plt.tight_layout()
        st.pyplot(fig)
        
    else:
        # å¤šè¾“å‡ºæƒ…å†µ
        target_names = st.session_state.selected_cols
        selected_target = st.selectbox(
            "é€‰æ‹©ç›®æ ‡å˜é‡æŸ¥çœ‹åˆ†å¸ƒ", 
            range(len(target_names)), 
            format_func=lambda x: target_names[x]
        )
        
        y_train_selected = y_train[:, selected_target]
        y_test_selected = y_test[:, selected_target]
        
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))
        
        # è®­ç»ƒé›†æ ‡ç­¾åˆ†å¸ƒ
        ax1.hist(y_train_selected, bins=20, alpha=0.7, color='blue')
        ax1.set_title(f'è®­ç»ƒé›† {target_names[selected_target]} åˆ†å¸ƒ')
        ax1.set_xlabel('æ ‡ç­¾å€¼')
        ax1.set_ylabel('é¢‘æ•°')
        ax1.grid(True, alpha=0.3)
        
        # æµ‹è¯•é›†æ ‡ç­¾åˆ†å¸ƒ
        ax2.hist(y_test_selected, bins=20, alpha=0.7, color='orange')
        ax2.set_title(f'æµ‹è¯•é›† {target_names[selected_target]} åˆ†å¸ƒ')
        ax2.set_xlabel('æ ‡ç­¾å€¼')
        ax2.set_ylabel('é¢‘æ•°')
        ax2.grid(True, alpha=0.3)
        
        # å¯¹æ¯”åˆ†å¸ƒ
        ax3.hist(y_train_selected, bins=20, alpha=0.5, color='blue', label='è®­ç»ƒé›†')
        ax3.hist(y_test_selected, bins=20, alpha=0.5, color='orange', label='æµ‹è¯•é›†')
        ax3.set_title(f'{target_names[selected_target]} åˆ†å¸ƒå¯¹æ¯”')
        ax3.set_xlabel('æ ‡ç­¾å€¼')
        ax3.set_ylabel('é¢‘æ•°')
        ax3.legend()
        ax3.grid(True, alpha=0.3)
        
        # å…‰è°±å¯¹æ¯”
        n_samples = min(5, X_train.shape[0], X_test.shape[0])
        
        for i in range(n_samples):
            ax4.plot(wavenumbers, X_train[i], alpha=0.5, color='blue')
            ax4.plot(wavenumbers, X_test[i], alpha=0.5, color='orange')
        
        ax4.set_title('è®­ç»ƒé›† vs æµ‹è¯•é›†å…‰è°±å¯¹æ¯”')
        ax4.set_xlabel('æ³¢æ•° (cmâ»Â¹)')
        ax4.set_ylabel('å¼ºåº¦')
        ax4.legend(handles=legend_elements)
        ax4.grid(True, alpha=0.3)
        
        plt.tight_layout()
        st.pyplot(fig)
    
    # ç»Ÿè®¡ä¿¡æ¯è¡¨æ ¼
    st.write("### ğŸ“‹ åˆ’åˆ†ç»Ÿè®¡ä¿¡æ¯")
    
    if y_train.ndim == 1:
        stats_data = {
            'æ•°æ®é›†': ['è®­ç»ƒé›†', 'æµ‹è¯•é›†'],
            'æ ·æœ¬æ•°é‡': [len(X_train), len(X_test)],
            'æ ‡ç­¾å‡å€¼': [f'{np.mean(y_train):.4f}', f'{np.mean(y_test):.4f}'],
            'æ ‡ç­¾æ ‡å‡†å·®': [f'{np.std(y_train):.4f}', f'{np.std(y_test):.4f}'],
            'æ ‡ç­¾èŒƒå›´': [f'{y_train.min():.4f} - {y_train.max():.4f}', 
                        f'{y_test.min():.4f} - {y_test.max():.4f}']
        }
    else:
        stats_data = {
            'æ•°æ®é›†': ['è®­ç»ƒé›†', 'æµ‹è¯•é›†'],
            'æ ·æœ¬æ•°é‡': [len(X_train), len(X_test)],
            'ç›®æ ‡å˜é‡æ•°': [y_train.shape[1], y_test.shape[1]],
            'ç‰¹å¾æ•°é‡': [X_train.shape[1], X_test.shape[1]]
        }
    
    stats_df = pd.DataFrame(stats_data)
    st.dataframe(stats_df, use_container_width=True)


# æ¨¡å‹ç›¸å…³å‡½æ•°
MODEL_NAMES = {
    'linear': 'çº¿æ€§å›å½’',
    'ridge': 'å²­å›å½’',
    'lasso': 'Lassoå›å½’',
    'svr': 'æ”¯æŒå‘é‡å›å½’',
    'rf': 'éšæœºæ£®æ—',
    'gbr': 'æ¢¯åº¦æå‡å›å½’',
    'mlp': 'å¤šå±‚æ„ŸçŸ¥æœº',
    'pls': 'åæœ€å°äºŒä¹˜å›å½’',
    'xgb': 'XGBoost'
}


def setup_model_parameters_ui(model_name, index):
    """è®¾ç½®æ¨¡å‹å‚æ•°UI"""
    st.write(f"**{MODEL_NAMES[model_name]} å‚æ•°è®¾ç½®**")
    
    if model_name == 'linear':
        return setup_linear_params(index)
    elif model_name == 'ridge':
        return setup_ridge_params(index)
    elif model_name == 'lasso':
        return setup_lasso_params(index)
    elif model_name == 'svr':
        return setup_svr_params(index)
    elif model_name == 'rf':
        return setup_rf_params(index)
    elif model_name == 'gbr':
        return setup_gbr_params(index)
    elif model_name == 'mlp':
        return setup_mlp_params(index)
    elif model_name == 'pls':
        return setup_pls_params(index)
    elif model_name == 'xgb':
        return setup_xgb_params(index)


def setup_linear_params(index):
    """çº¿æ€§å›å½’å‚æ•°è®¾ç½®"""
    st.write("çº¿æ€§å›å½’æ— éœ€è°ƒå‚")
    return {}


def setup_ridge_params(index):
    """å²­å›å½’å‚æ•°è®¾ç½®"""
    alpha = st.selectbox("æ­£åˆ™åŒ–å¼ºåº¦ (alpha)", [0.1, 1.0, 10.0, 100.0], 
                        index=1, key=f"ridge_alpha_{index}")
    return {'alpha': alpha}


def setup_lasso_params(index):
    """Lassoå›å½’å‚æ•°è®¾ç½®"""
    alpha = st.selectbox("æ­£åˆ™åŒ–å¼ºåº¦ (alpha)", [0.01, 0.1, 1.0, 10.0], 
                        index=1, key=f"lasso_alpha_{index}")
    return {'alpha': alpha}


def setup_svr_params(index):
    """æ”¯æŒå‘é‡å›å½’å‚æ•°è®¾ç½®"""
    col1, col2 = st.columns(2)
    with col1:
        kernel = st.selectbox("æ ¸å‡½æ•°", ['rbf', 'linear', 'poly'], 
                             index=0, key=f"svr_kernel_{index}")
        C = st.selectbox("æ­£åˆ™åŒ–å‚æ•° (C)", [0.1, 1.0, 10.0, 100.0], 
                        index=1, key=f"svr_C_{index}")
    with col2:
        gamma = st.selectbox("Gamma", ['scale', 'auto', 0.001, 0.01, 0.1], 
                           index=0, key=f"svr_gamma_{index}")
        epsilon = st.selectbox("Epsilon", [0.01, 0.1, 0.2], 
                             index=1, key=f"svr_epsilon_{index}")
    
    return {
        'kernel': kernel,
        'C': C,
        'gamma': gamma,
        'epsilon': epsilon
    }


def setup_rf_params(index):
    """éšæœºæ£®æ—å‚æ•°è®¾ç½®"""
    col1, col2 = st.columns(2)
    with col1:
        n_estimators = st.slider("æ ‘çš„æ•°é‡", 50, 500, 100, key=f"rf_trees_{index}")
        max_depth = st.selectbox("æœ€å¤§æ·±åº¦", [None, 5, 10, 15, 20], 
                               index=0, key=f"rf_depth_{index}")
    with col2:
        min_samples_split = st.slider("åˆ†è£‚æœ€å°æ ·æœ¬æ•°", 2, 10, 2, key=f"rf_split_{index}")
        min_samples_leaf = st.slider("å¶èŠ‚ç‚¹æœ€å°æ ·æœ¬æ•°", 1, 5, 1, key=f"rf_leaf_{index}")
    
    random_state = st.number_input("éšæœºç§å­", value=42, key=f"rf_seed_{index}")
    
    return {
        'n_estimators': n_estimators,
        'max_depth': max_depth,
        'min_samples_split': min_samples_split,
        'min_samples_leaf': min_samples_leaf,
        'random_state': random_state
    }


def setup_gbr_params(index):
    """æ¢¯åº¦æå‡å‚æ•°è®¾ç½®"""
    col1, col2 = st.columns(2)
    with col1:
        n_estimators = st.slider("æå‡é˜¶æ®µæ•°", 50, 500, 100, key=f"gbr_stages_{index}")
        learning_rate = st.selectbox("å­¦ä¹ ç‡", [0.01, 0.05, 0.1, 0.2], 
                                   index=2, key=f"gbr_lr_{index}")
    with col2:
        max_depth = st.slider("æœ€å¤§æ·±åº¦", 2, 10, 3, key=f"gbr_depth_{index}")
        subsample = st.slider("å­é‡‡æ ·æ¯”ä¾‹", 0.5, 1.0, 1.0, step=0.1, key=f"gbr_subsample_{index}")
    
    random_state = st.number_input("éšæœºç§å­", value=42, key=f"gbr_seed_{index}")
    
    return {
        'n_estimators': n_estimators,
        'learning_rate': learning_rate,
        'max_depth': max_depth,
        'subsample': subsample,
        'random_state': random_state
    }


def setup_pls_params(index):
    """PLSå‚æ•°è®¾ç½®"""
    n_components = st.slider("ä¸»æˆåˆ†æ•°é‡", 1, min(20, st.session_state.X_train.shape[1]), 
                           5, key=f"pls_components_{index}")
    scale = st.checkbox("æ ‡å‡†åŒ–", value=True, key=f"pls_scale_{index}")
    
    return {
        'n_components': n_components,
        'scale': scale
    }


def setup_mlp_params(index):
    """MLPå‚æ•°è®¾ç½®"""
    col1, col2 = st.columns(2)
    with col1:
        layer_option = st.selectbox("éšè—å±‚ç»“æ„", ["ä¸€å±‚", "ä¸¤å±‚", "ä¸‰å±‚"], 
                                  index=1, key=f"mlp_layers_{index}")
        
        if layer_option == "ä¸€å±‚":
            layer1_size = st.slider("éšè—å±‚ç¥ç»å…ƒæ•°", 10, 200, 50, key=f"mlp_l1_{index}")
            hidden_layer_sizes = (layer1_size,)
        elif layer_option == "ä¸¤å±‚":
            layer1_size = st.slider("ç¬¬ä¸€å±‚ç¥ç»å…ƒæ•°", 10, 200, 100, key=f"mlp_l1_{index}")
            layer2_size = st.slider("ç¬¬äºŒå±‚ç¥ç»å…ƒæ•°", 10, 100, 50, key=f"mlp_l2_{index}")
            hidden_layer_sizes = (layer1_size, layer2_size)
        else:  # ä¸‰å±‚
            layer1_size = st.slider("ç¬¬ä¸€å±‚ç¥ç»å…ƒæ•°", 10, 200, 100, key=f"mlp_l1_{index}")
            layer2_size = st.slider("ç¬¬äºŒå±‚ç¥ç»å…ƒæ•°", 10, 100, 50, key=f"mlp_l2_{index}")
            layer3_size = st.slider("ç¬¬ä¸‰å±‚ç¥ç»å…ƒæ•°", 10, 50, 25, key=f"mlp_l3_{index}")
            hidden_layer_sizes = (layer1_size, layer2_size, layer3_size)
        
        activation = st.selectbox("æ¿€æ´»å‡½æ•°", ['relu', 'tanh', 'logistic'], 
                                index=0, key=f"mlp_activation_{index}")
    
    with col2:
        solver = st.selectbox("ä¼˜åŒ–ç®—æ³•", ['adam', 'lbfgs', 'sgd'], 
                            index=0, key=f"mlp_solver_{index}")
        learning_rate_init = st.selectbox("åˆå§‹å­¦ä¹ ç‡", [0.0001, 0.001, 0.01], 
                                        index=1, key=f"mlp_lr_{index}")
        max_iter = st.slider("æœ€å¤§è¿­ä»£æ¬¡æ•°", 100, 1000, 500, key=f"mlp_iter_{index}")
        alpha = st.selectbox("L2æ­£åˆ™åŒ–å‚æ•°", [0.0001, 0.001, 0.01], 
                           index=0, key=f"mlp_alpha_{index}")
    
    random_state = st.number_input("éšæœºç§å­", value=42, key=f"mlp_seed_{index}")
    
    return {
        'hidden_layer_sizes': hidden_layer_sizes,
        'activation': activation,
        'solver': solver,
        'learning_rate_init': learning_rate_init,
        'max_iter': max_iter,
        'alpha': alpha,
        'random_state': random_state
    }


def setup_xgb_params(index):
    """XGBoostå‚æ•°è®¾ç½®"""
    col1, col2 = st.columns(2)
    with col1:
        n_estimators = st.slider("æå‡è½®æ•°", 50, 500, 100, key=f"xgb_trees_{index}")
        learning_rate = st.selectbox("å­¦ä¹ ç‡", [0.01, 0.05, 0.1, 0.2], 
                                   index=2, key=f"xgb_lr_{index}")
        max_depth = st.slider("æœ€å¤§æ·±åº¦", 2, 10, 6, key=f"xgb_depth_{index}")
    with col2:
        subsample = st.slider("å­é‡‡æ ·æ¯”ä¾‹", 0.5, 1.0, 1.0, step=0.1, key=f"xgb_subsample_{index}")
        colsample_bytree = st.slider("ç‰¹å¾é‡‡æ ·æ¯”ä¾‹", 0.5, 1.0, 1.0, step=0.1, key=f"xgb_colsample_{index}")
        reg_alpha = st.selectbox("L1æ­£åˆ™åŒ–", [0, 0.01, 0.1], index=0, key=f"xgb_alpha_{index}")
    
    random_state = st.number_input("éšæœºç§å­", value=42, key=f"xgb_seed_{index}")
    
    return {
        'n_estimators': n_estimators,
        'learning_rate': learning_rate,
        'max_depth': max_depth,
        'subsample': subsample,
        'colsample_bytree': colsample_bytree,
        'reg_alpha': reg_alpha,
        'random_state': random_state
    }


def create_model_instance(model_name, params, is_multioutput):
    """åˆ›å»ºæ¨¡å‹å®ä¾‹"""
    from sklearn.linear_model import LinearRegression, Ridge, Lasso
    from sklearn.svm import SVR
    from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
    from sklearn.neural_network import MLPRegressor
    from sklearn.cross_decomposition import PLSRegression
    from sklearn.preprocessing import StandardScaler
    from sklearn.multioutput import MultiOutputRegressor
    
    use_scaler = False
    
    if model_name == 'linear':
        model = LinearRegression()
        use_scaler = True
        
    elif model_name == 'ridge':
        model = Ridge(**params)
        use_scaler = True
        
    elif model_name == 'lasso':
        model = Lasso(**params)
        use_scaler = True
        
    elif model_name == 'svr':
        model = SVR(**params)
        if is_multioutput:
            model = MultiOutputRegressor(model)
        use_scaler = True
        
    elif model_name == 'rf':
        model = RandomForestRegressor(**params)
        
    elif model_name == 'gbr':
        if is_multioutput:
            model = MultiOutputRegressor(GradientBoostingRegressor(**params))
        else:
            model = GradientBoostingRegressor(**params)
            
    elif model_name == 'mlp':
        model = MLPRegressor(**params)
        if is_multioutput:
            model = MultiOutputRegressor(model)
        use_scaler = True
        
    elif model_name == 'pls':
        model = PLSRegression(**params)
        
    elif model_name == 'xgb':
        try:
            import xgboost as xgb
            model = xgb.XGBRegressor(**params)
            if is_multioutput:
                model = MultiOutputRegressor(model)
        except ImportError:
            raise ImportError("XGBoostæœªå®‰è£…ï¼Œè¯·å®‰è£…åä½¿ç”¨")
    
    return model, use_scaler


# ====================================
# é‡æ„åçš„æ¨¡å‹è®­ç»ƒä¸»å‡½æ•°
# ====================================

def show_model_training_page():
    """æ¨¡å‹è®­ç»ƒä¸è¯„ä¼°é¡µé¢ - é‡æ„ç‰ˆæœ¬"""
    st.markdown("<h1 class='section-header'>æ¨¡å‹è®­ç»ƒä¸è¯„ä¼°</h1>", unsafe_allow_html=True)
    
    # ä½¿ç”¨æ–°çš„å‰ç½®æ¡ä»¶æ£€æŸ¥å‡½æ•°
    if not check_data_prerequisites(need_labels=True, need_preprocessing=False):
        return
    
    if not hasattr(st.session_state, 'split_method'):
        show_status_message("è¯·å…ˆå®Œæˆæ•°æ®é›†åˆ’åˆ†", "warning")
        return
    
    st.markdown("""
    <div class="info-box">
    é€‰æ‹©æœºå™¨å­¦ä¹ ç®—æ³•è¿›è¡Œæ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°ï¼Œæ”¯æŒå¤šç§å›å½’ç®—æ³•å’Œå‚æ•°è°ƒä¼˜ã€‚
    </div>
    """, unsafe_allow_html=True)
    
    # æ˜¾ç¤ºå½“å‰æ•°æ®ä¿¡æ¯ - ä½¿ç”¨æ–°çš„å·¥å…·å‡½æ•°
    X, wavenumbers, data_info = get_current_data()
    show_status_message(data_info, "info")
    
    # æ¨¡å‹é€‰æ‹©
    st.subheader("ğŸ¤– æ¨¡å‹é€‰æ‹©ä¸å‚æ•°è®¾ç½®")
    
    # æ£€æŸ¥XGBoostå¯ç”¨æ€§
    available_models = MODEL_NAMES.copy()
    try:
        import xgboost as xgb
    except ImportError:
        available_models.pop('xgb', None)
    
    selected_models = st.multiselect(
        "é€‰æ‹©è¦è®­ç»ƒçš„æ¨¡å‹", 
        list(available_models.keys()), 
        format_func=lambda x: available_models[x]
    )
    
    if not selected_models:
        show_status_message("è¯·è‡³å°‘é€‰æ‹©ä¸€ä¸ªæ¨¡å‹", "warning")
        return
    
    # æ£€æŸ¥æ˜¯å¦ä¸ºå¤šè¾“å‡ºé—®é¢˜
    is_multioutput = len(st.session_state.selected_cols) > 1
    
    # å‚æ•°è®¾ç½®
    model_params = {}
    for i, model_name in enumerate(selected_models):
        model_params[model_name] = setup_model_parameters_ui(model_name, i)
    
    # æ˜¾ç¤ºäº¤å‰éªŒè¯ä¿¡æ¯
    display_cv_info()
    
    # å¼€å§‹è®­ç»ƒ
    if st.button("ğŸš€ å¼€å§‹è®­ç»ƒæ¨¡å‹", type="primary"):
        train_all_models(selected_models, model_params, is_multioutput)


def display_cv_info():
    """æ˜¾ç¤ºäº¤å‰éªŒè¯ä¿¡æ¯"""
    if st.session_state.split_method in ["KFoldäº¤å‰éªŒè¯", "ç•™ä¸€æ³•(LOOCV)"]:
        if st.session_state.split_method == "KFoldäº¤å‰éªŒè¯":
            cv_folds = getattr(st.session_state, 'cv_splits', 5)
            show_status_message(f"å°†ä½¿ç”¨ {cv_folds} æŠ˜äº¤å‰éªŒè¯", "info")
        else:
            show_status_message("å°†ä½¿ç”¨ç•™ä¸€æ³•äº¤å‰éªŒè¯", "info")
    else:
        show_status_message("å°†ä½¿ç”¨è®­ç»ƒé›†/æµ‹è¯•é›†åˆ’åˆ†", "info")


def train_all_models(selected_models, model_params, is_multioutput):
    """è®­ç»ƒæ‰€æœ‰é€‰å®šçš„æ¨¡å‹"""
    X_train = st.session_state.X_train
    y_train = st.session_state.y_train
    
    if st.session_state.split_method == "éšæœºåˆ’åˆ†":
        X_test = st.session_state.X_test
        y_test = st.session_state.y_test
    else:
        X_test = X_train
        y_test = y_train
    
    results = []
    trained_models = {}
    detailed_results = {}
    
    progress_bar = st.progress(0)
    progress_text = st.empty()
    
    for i, model_name in enumerate(selected_models):
        progress_text.text(f"æ­£åœ¨è®­ç»ƒ {MODEL_NAMES[model_name]} ({i+1}/{len(selected_models)})...")
        
        result = safe_execute(
            lambda: train_single_model(
                model_name, model_params[model_name], 
                X_train, y_train, X_test, y_test, is_multioutput
            ),
            f"è®­ç»ƒæ¨¡å‹ {MODEL_NAMES[model_name]} æ—¶å‡ºé”™"
        )
        
        if result:
            model, train_pred, test_pred, scaler, train_time, cv_results = result
            
            # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
            metrics = calculate_model_metrics(
                y_train, y_test, train_pred, test_pred, is_multioutput
            )
            
            # ä¿å­˜ç»“æœ
            result_entry = {
                'Model': MODEL_NAMES[model_name],
                'Training Time (s)': train_time,
                **metrics
            }
            
            if cv_results:
                result_entry.update(cv_results)
            
            results.append(result_entry)
            trained_models[model_name] = model
            detailed_results[model_name] = {
                'train_pred': train_pred,
                'test_pred': test_pred,
                'params': model_params[model_name],
                'scaler': scaler
            }
            
            # å¦‚æœæ˜¯å¤šè¾“å‡ºï¼Œä¿å­˜æ¯ä¸ªç›®æ ‡çš„è¯¦ç»†ç»“æœ
            if is_multioutput:
                detailed_results[model_name].update(
                    calculate_multioutput_details(y_train, y_test, train_pred, test_pred)
                )
        
        progress_bar.progress((i + 1) / len(selected_models))
    
    progress_text.text("æ‰€æœ‰æ¨¡å‹è®­ç»ƒå®Œæˆï¼")
    
    if results:
        display_training_results(results, trained_models, detailed_results, is_multioutput)
    else:
        show_status_message("æ²¡æœ‰æˆåŠŸè®­ç»ƒä»»ä½•æ¨¡å‹ï¼Œè¯·æ£€æŸ¥æ•°æ®å’Œå‚æ•°è®¾ç½®", "error")


def train_single_model(model_name, params, X_train, y_train, X_test, y_test, is_multioutput):
    """è®­ç»ƒå•ä¸ªæ¨¡å‹"""
    import time
    
    # åˆ›å»ºæ¨¡å‹
    model, use_scaler = create_model_instance(model_name, params.copy(), is_multioutput)
    
    # å¤„ç†æ ‡å‡†åŒ–
    scaler = None
    X_train_scaled = X_train
    X_test_scaled = X_test
    
    if use_scaler and model_name in ['ridge', 'lasso', 'linear']:
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)
    
    # è®­ç»ƒæ¨¡å‹
    start_time = time.time()
    
    # åˆ¤æ–­æ˜¯å¦ä½¿ç”¨äº¤å‰éªŒè¯
    use_cv = st.session_state.split_method in ["KFoldäº¤å‰éªŒè¯", "ç•™ä¸€æ³•(LOOCV)"]
    cv_results = None
    
    if use_cv:
        train_pred, cv_results = train_with_cv(
            model, X_train_scaled, y_train, st.session_state.split_method
        )
        test_pred = model.predict(X_test_scaled)
    else:
        model.fit(X_train_scaled, y_train)
        train_pred = model.predict(X_train_scaled)
        test_pred = model.predict(X_test_scaled)
    
    train_time = time.time() - start_time
    
    return model, train_pred, test_pred, scaler, train_time, cv_results


def train_with_cv(model, X_train, y_train, cv_method):
    """ä½¿ç”¨äº¤å‰éªŒè¯è®­ç»ƒæ¨¡å‹"""
    from sklearn.model_selection import KFold, LeaveOneOut
    from sklearn.metrics import r2_score
    
    if cv_method == "KFoldäº¤å‰éªŒè¯":
        cv_folds = getattr(st.session_state, 'cv_splits', 5)
        cv = KFold(n_splits=cv_folds, shuffle=True, random_state=42)
    else:  # LOOCV
        cv = LeaveOneOut()
    
    # æ‰§è¡Œäº¤å‰éªŒè¯
    cv_predictions = np.zeros_like(y_train)
    cv_scores = []
    
    for fold, (train_idx, val_idx) in enumerate(cv.split(X_train)):
        X_fold_train = X_train[train_idx]
        X_fold_val = X_train[val_idx]
        y_fold_train = y_train[train_idx]
        y_fold_val = y_train[val_idx]
        
        # è®­ç»ƒ
        model.fit(X_fold_train, y_fold_train)
        
        # é¢„æµ‹
        fold_pred = model.predict(X_fold_val)
        cv_predictions[val_idx] = fold_pred
        
        # è®¡ç®—foldå¾—åˆ†
        if y_fold_val.ndim > 1 and y_fold_val.shape[1] > 1:
            fold_score = np.mean([r2_score(y_fold_val[:, j], fold_pred[:, j]) 
                                for j in range(y_fold_val.shape[1])])
        else:
            fold_score = r2_score(y_fold_val, fold_pred)
        cv_scores.append(fold_score)
    
    # ç”¨å…¨éƒ¨æ•°æ®é‡æ–°è®­ç»ƒ
    model.fit(X_train, y_train)
    
    cv_mean = np.mean(cv_scores)
    cv_std = np.std(cv_scores)
    
    return cv_predictions, {'CV RÂ² Mean': cv_mean, 'CV RÂ² Std': cv_std}


def show_model_performance_visualization(results_df, detailed_results, is_multioutput):
    """æ˜¾ç¤ºæ¨¡å‹æ€§èƒ½å¯è§†åŒ–"""
    st.subheader("ğŸ“ˆ æ¨¡å‹æ€§èƒ½å¯è§†åŒ–")
    
    # åˆ›å»ºæ ‡ç­¾é¡µ
    if is_multioutput:
        tab1, tab2, tab3 = st.tabs(["æ€§èƒ½å¯¹æ¯”", "é¢„æµ‹æ•ˆæœ", "ç›®æ ‡å˜é‡è¯¦æƒ…"])
    else:
        tab1, tab2 = st.tabs(["æ€§èƒ½å¯¹æ¯”", "é¢„æµ‹æ•ˆæœ"])
    
    with tab1:
        # æ€§èƒ½å¯¹æ¯”å›¾
        col1, col2 = st.columns(2)
        
        with col1:
            # RÂ²å¯¹æ¯”
            fig, ax = plt.subplots(figsize=(10, 6))
            models = results_df['Model']
            test_r2 = results_df['Test RÂ²'].astype(float)
            train_r2 = results_df['Train RÂ²'].astype(float)
            
            y_pos = np.arange(len(models))
            width = 0.35
            
            ax.barh(y_pos - width/2, train_r2, width, label='è®­ç»ƒ RÂ²', alpha=0.7, color='skyblue')
            ax.barh(y_pos + width/2, test_r2, width, label='æµ‹è¯• RÂ²', alpha=0.7, color='lightcoral')
            
            ax.set_yticks(y_pos)
            ax.set_yticklabels(models)
            ax.set_xlabel('RÂ² åˆ†æ•°')
            ax.set_title('æ¨¡å‹ RÂ² æ€§èƒ½å¯¹æ¯”')
            ax.legend()
            ax.grid(True, linestyle='--', alpha=0.7)
            
            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for i, (train, test) in enumerate(zip(train_r2, test_r2)):
                ax.text(train + 0.01, i - width/2, f'{train:.3f}', va='center', fontsize=8)
                ax.text(test + 0.01, i + width/2, f'{test:.3f}', va='center', fontsize=8)
            
            st.pyplot(fig)
        
        with col2:
            # RMSEå¯¹æ¯”
            fig, ax = plt.subplots(figsize=(10, 6))
            test_rmse = results_df['Test RMSE'].astype(float)
            train_rmse = results_df['Train RMSE'].astype(float)
            
            ax.barh(y_pos - width/2, train_rmse, width, label='è®­ç»ƒ RMSE', alpha=0.7, color='lightgreen')
            ax.barh(y_pos + width/2, test_rmse, width, label='æµ‹è¯• RMSE', alpha=0.7, color='orange')
            
            ax.set_yticks(y_pos)
            ax.set_yticklabels(models)
            ax.set_xlabel('RMSE')
            ax.set_title('æ¨¡å‹ RMSE æ€§èƒ½å¯¹æ¯”')
            ax.legend()
            ax.grid(True, linestyle='--', alpha=0.7)
            
            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for i, (train, test) in enumerate(zip(train_rmse, test_rmse)):
                ax.text(train + max(train_rmse) * 0.01, i - width/2, f'{train:.3f}', va='center', fontsize=8)
                ax.text(test + max(test_rmse) * 0.01, i + width/2, f'{test:.3f}', va='center', fontsize=8)
            
            st.pyplot(fig)
    
    with tab2:
        # é¢„æµ‹æ•ˆæœæ•£ç‚¹å›¾
        st.write("### ğŸ¯ é¢„æµ‹ vs å®é™…å€¼å¯¹æ¯”")
        
        # é€‰æ‹©æ¨¡å‹
        model_names = list(detailed_results.keys())
        available_models = {
            'linear': 'çº¿æ€§å›å½’', 'ridge': 'å²­å›å½’', 'lasso': 'Lassoå›å½’',
            'svr': 'æ”¯æŒå‘é‡å›å½’', 'rf': 'éšæœºæ£®æ—', 'gbr': 'æ¢¯åº¦æå‡å›å½’',
            'mlp': 'å¤šå±‚æ„ŸçŸ¥æœº', 'pls': 'åæœ€å°äºŒä¹˜å›å½’', 'xgb': 'XGBoost'
        }
        
        selected_model = st.selectbox(
            "é€‰æ‹©æ¨¡å‹æŸ¥çœ‹é¢„æµ‹æ•ˆæœ", 
            model_names, 
            format_func=lambda x: available_models.get(x, x)
        )
        
        if selected_model in detailed_results:
            model_results = detailed_results[selected_model]
            
            # è·å–å®é™…å€¼
            y_train = st.session_state.y_train
            y_test = st.session_state.y_test if st.session_state.y_test is not None else y_train
            
            if is_multioutput:
                # å¤šè¾“å‡ºæƒ…å†µ
                target_names = st.session_state.selected_cols
                target_idx = st.selectbox(
                    "é€‰æ‹©ç›®æ ‡å˜é‡", 
                    range(len(target_names)), 
                    format_func=lambda x: target_names[x]
                )
                
                fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
                
                # è®­ç»ƒé›†é¢„æµ‹
                y_train_actual = y_train[:, target_idx]
                y_train_pred = model_results['train_pred'][:, target_idx]
                
                ax1.scatter(y_train_actual, y_train_pred, alpha=0.6, color='blue')
                ax1.plot([y_train_actual.min(), y_train_actual.max()], 
                        [y_train_actual.min(), y_train_actual.max()], 'r--', lw=2)
                ax1.set_xlabel('å®é™…å€¼')
                ax1.set_ylabel('é¢„æµ‹å€¼')
                ax1.set_title(f'è®­ç»ƒé›† - {target_names[target_idx]}')
                ax1.grid(True, alpha=0.3)
                
                # æ·»åŠ RÂ²å’ŒRMSE
                r2 = r2_score(y_train_actual, y_train_pred)
                rmse = np.sqrt(mean_squared_error(y_train_actual, y_train_pred))
                ax1.text(0.05, 0.95, f'RÂ² = {r2:.3f}\nRMSE = {rmse:.3f}', 
                        transform=ax1.transAxes, verticalalignment='top',
                        bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))
                
                # æµ‹è¯•é›†é¢„æµ‹
                y_test_actual = y_test[:, target_idx]
                y_test_pred = model_results['test_pred'][:, target_idx]
                
                ax2.scatter(y_test_actual, y_test_pred, alpha=0.6, color='green')
                ax2.plot([y_test_actual.min(), y_test_actual.max()], 
                        [y_test_actual.min(), y_test_actual.max()], 'r--', lw=2)
                ax2.set_xlabel('å®é™…å€¼')
                ax2.set_ylabel('é¢„æµ‹å€¼')
                ax2.set_title(f'æµ‹è¯•é›† - {target_names[target_idx]}')
                ax2.grid(True, alpha=0.3)
                
                # æ·»åŠ RÂ²å’ŒRMSE
                r2 = r2_score(y_test_actual, y_test_pred)
                rmse = np.sqrt(mean_squared_error(y_test_actual, y_test_pred))
                ax2.text(0.05, 0.95, f'RÂ² = {r2:.3f}\nRMSE = {rmse:.3f}', 
                        transform=ax2.transAxes, verticalalignment='top',
                        bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))
                
                plt.tight_layout()
                st.pyplot(fig)
                
            else:
                # å•è¾“å‡ºæƒ…å†µ
                fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
                
                # è®­ç»ƒé›†
                ax1.scatter(y_train, model_results['train_pred'], alpha=0.6, color='blue')
                ax1.plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], 'r--', lw=2)
                ax1.set_xlabel('å®é™…å€¼')
                ax1.set_ylabel('é¢„æµ‹å€¼')
                ax1.set_title('è®­ç»ƒé›†é¢„æµ‹æ•ˆæœ')
                ax1.grid(True, alpha=0.3)
                
                r2 = r2_score(y_train, model_results['train_pred'])
                rmse = np.sqrt(mean_squared_error(y_train, model_results['train_pred']))
                ax1.text(0.05, 0.95, f'RÂ² = {r2:.3f}\nRMSE = {rmse:.3f}', 
                        transform=ax1.transAxes, verticalalignment='top',
                        bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))
                
                # æµ‹è¯•é›†
                ax2.scatter(y_test, model_results['test_pred'], alpha=0.6, color='green')
                ax2.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
                ax2.set_xlabel('å®é™…å€¼')
                ax2.set_ylabel('é¢„æµ‹å€¼')
                ax2.set_title('æµ‹è¯•é›†é¢„æµ‹æ•ˆæœ')
                ax2.grid(True, alpha=0.3)
                
                r2 = r2_score(y_test, model_results['test_pred'])
                rmse = np.sqrt(mean_squared_error(y_test, model_results['test_pred']))
                ax2.text(0.05, 0.95, f'RÂ² = {r2:.3f}\nRMSE = {rmse:.3f}', 
                        transform=ax2.transAxes, verticalalignment='top',
                        bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))
                
                plt.tight_layout()
                st.pyplot(fig)
    
    if is_multioutput:
        with tab3:
            # å„ç›®æ ‡å˜é‡çš„è¯¦ç»†æ€§èƒ½
            st.write("### ğŸ“Š å„ç›®æ ‡å˜é‡æ€§èƒ½è¯¦æƒ…")
            
            # é€‰æ‹©æ¨¡å‹
            selected_model = st.selectbox(
                "é€‰æ‹©æ¨¡å‹", 
                model_names, 
                format_func=lambda x: available_models.get(x, x),
                key="detail_model_select"
            )
            
            if selected_model in detailed_results:
                model_results = detailed_results[selected_model]
                target_names = st.session_state.selected_cols
                
                # åˆ›å»ºè¯¦ç»†ç»“æœè¡¨æ ¼
                target_results = []
                for i, target_name in enumerate(target_names):
                    target_results.append({
                        'ç›®æ ‡å˜é‡': target_name,
                        'è®­ç»ƒ RÂ²': f"{model_results['train_r2_per_target'][i]:.4f}",
                        'æµ‹è¯• RÂ²': f"{model_results['test_r2_per_target'][i]:.4f}",
                        'è®­ç»ƒ RMSE': f"{model_results['train_rmse_per_target'][i]:.4f}",
                        'æµ‹è¯• RMSE': f"{model_results['test_rmse_per_target'][i]:.4f}",
                        'è®­ç»ƒ MAE': f"{model_results['train_mae_per_target'][i]:.4f}",
                        'æµ‹è¯• MAE': f"{model_results['test_mae_per_target'][i]:.4f}"
                    })
                
                target_df = pd.DataFrame(target_results)
                st.dataframe(target_df, use_container_width=True)
                
                # å¯è§†åŒ–å„ç›®æ ‡å˜é‡æ€§èƒ½
                fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))
                
                # RÂ²å¯¹æ¯”
                ax1.bar(target_names, model_results['train_r2_per_target'], 
                       alpha=0.7, label='è®­ç»ƒ RÂ²', color='skyblue')
                ax1.bar(target_names, model_results['test_r2_per_target'], 
                       alpha=0.7, label='æµ‹è¯• RÂ²', color='lightcoral')
                ax1.set_ylabel('RÂ²')
                ax1.set_title('å„ç›®æ ‡å˜é‡ RÂ² å¯¹æ¯”')
                ax1.legend()
                ax1.tick_params(axis='x', rotation=45)
                ax1.grid(True, alpha=0.3)
                
                # RMSEå¯¹æ¯”
                ax2.bar(target_names, model_results['train_rmse_per_target'], 
                       alpha=0.7, label='è®­ç»ƒ RMSE', color='lightgreen')
                ax2.bar(target_names, model_results['test_rmse_per_target'], 
                       alpha=0.7, label='æµ‹è¯• RMSE', color='orange')
                ax2.set_ylabel('RMSE')
                ax2.set_title('å„ç›®æ ‡å˜é‡ RMSE å¯¹æ¯”')
                ax2.legend()
                ax2.tick_params(axis='x', rotation=45)
                ax2.grid(True, alpha=0.3)
                
                # MAEå¯¹æ¯”
                ax3.bar(target_names, model_results['train_mae_per_target'], 
                       alpha=0.7, label='è®­ç»ƒ MAE', color='plum')
                ax3.bar(target_names, model_results['test_mae_per_target'], 
                       alpha=0.7, label='æµ‹è¯• MAE', color='gold')
                ax3.set_ylabel('MAE')
                ax3.set_title('å„ç›®æ ‡å˜é‡ MAE å¯¹æ¯”')
                ax3.legend()
                ax3.tick_params(axis='x', rotation=45)
                ax3.grid(True, alpha=0.3)
                
                # ç»¼åˆæ€§èƒ½é›·è¾¾å›¾
                from math import pi
                
                # å½’ä¸€åŒ–æ€§èƒ½æŒ‡æ ‡
                r2_norm = np.array(model_results['test_r2_per_target'])
                rmse_norm = 1 - np.array(model_results['test_rmse_per_target']) / max(model_results['test_rmse_per_target'])
                mae_norm = 1 - np.array(model_results['test_mae_per_target']) / max(model_results['test_mae_per_target'])
                
                # é›·è¾¾å›¾
                angles = [n / len(target_names) * 2 * pi for n in range(len(target_names))]
                angles += angles[:1]
                
                ax4 = plt.subplot(224, projection='polar')
                
                r2_values = list(r2_norm) + [r2_norm[0]]
                rmse_values = list(rmse_norm) + [rmse_norm[0]]
                mae_values = list(mae_norm) + [mae_norm[0]]
                
                ax4.plot(angles, r2_values, 'o-', linewidth=2, label='RÂ² (æ ‡å‡†åŒ–)', color='blue')
                ax4.fill(angles, r2_values, alpha=0.25, color='blue')
                
                ax4.plot(angles, rmse_values, 'o-', linewidth=2, label='RMSE (æ ‡å‡†åŒ–)', color='red')
                ax4.fill(angles, rmse_values, alpha=0.25, color='red')
                
                ax4.plot(angles, mae_values, 'o-', linewidth=2, label='MAE (æ ‡å‡†åŒ–)', color='green')
                ax4.fill(angles, mae_values, alpha=0.25, color='green')
                
                ax4.set_xticks(angles[:-1])
                ax4.set_xticklabels(target_names)
                ax4.set_ylim(0, 1)
                ax4.set_title('ç»¼åˆæ€§èƒ½é›·è¾¾å›¾', pad=20)
                ax4.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))
                
                plt.tight_layout()
                st.pyplot(fig)


def show_blind_prediction_page():
    """ç›²æ ·é¢„æµ‹é¡µé¢"""
    st.markdown("<h1 class='section-header'>ç›²æ ·é¢„æµ‹</h1>", unsafe_allow_html=True)
    
    if not hasattr(st.session_state, 'trained_models') or not st.session_state.trained_models:
        st.warning("è¯·å…ˆè®­ç»ƒæ¨¡å‹")
        return
    
    if st.session_state.X_preprocessed is None:
        st.warning("è¯·å…ˆå®Œæˆæ•°æ®é¢„å¤„ç†")
        return
    
    st.markdown("""
    <div class="info-box">
    ä¸Šä¼ ç›²æ ·æ•°æ®æ–‡ä»¶è¿›è¡Œé¢„æµ‹ã€‚ç›²æ ·æ•°æ®å°†ä½¿ç”¨ä¸è®­ç»ƒæ•°æ®ç›¸åŒçš„é¢„å¤„ç†æµç¨‹ã€‚
    </div>
    """, unsafe_allow_html=True)
    
    # æ˜¾ç¤ºé¢„å¤„ç†å‚æ•°ä¿¡æ¯
    st.subheader("å½“å‰é¢„å¤„ç†å‚æ•°")
    params = st.session_state.preprocessing_params
    
    with st.expander("æŸ¥çœ‹è¯¦ç»†å‚æ•°"):
        st.json(params)
    
    # æ˜¾ç¤ºç‰¹å¾é€‰æ‹©ä¿¡æ¯
    if hasattr(st.session_state, 'feature_selected') and st.session_state.feature_selected:
        if st.session_state.feature_selection_method != "ä¸è¿›è¡Œç‰¹å¾é€‰æ‹©":
            st.info(f"âœ… å·²è¿›è¡Œç‰¹å¾é€‰æ‹©ï¼š{st.session_state.feature_selection_method}")
            st.info(f"ç‰¹å¾æ•°é‡ï¼š{st.session_state.X_preprocessed.shape[1]} â†’ {st.session_state.X_final.shape[1]}")
        else:
            st.info("â„¹ï¸ æœªè¿›è¡Œç‰¹å¾é€‰æ‹©ï¼Œä½¿ç”¨å…¨éƒ¨é¢„å¤„ç†åçš„ç‰¹å¾")
    
    # ä¸Šä¼ ç›²æ ·æ•°æ®æ–‡ä»¶
    blind_file = st.file_uploader("ä¸Šä¼ ç›²æ ·æ•°æ®æ–‡ä»¶", type=["csv", "xlsx", "xls"])
    
    if blind_file is not None:
        try:
            # è¯»å–ç›²æ ·æ•°æ®
            if blind_file.name.endswith('.csv'):
                blind_df = pd.read_csv(blind_file)
            else:
                blind_df = pd.read_excel(blind_file)
            
            st.success(f"ç›²æ ·æ•°æ®ä¸Šä¼ æˆåŠŸï¼å…±{blind_df.shape[0]}è¡Œï¼Œ{blind_df.shape[1]}åˆ—")
            st.dataframe(blind_df.head())
            
            # æ£€æŸ¥æ•°æ®æ ¼å¼
            st.subheader("æ•°æ®æ ¼å¼æ£€æŸ¥")

            # è®©ç”¨æˆ·é€‰æ‹©å…‰è°±æ•°æ®çš„èµ·å§‹åˆ—
            st.write("**é€‰æ‹©å…‰è°±æ•°æ®èµ·å§‹åˆ—ï¼š**")
            start_col_options = list(range(1, min(blind_df.shape[1], 6)))  # æœ€å¤šæ˜¾ç¤ºå‰5åˆ—ä½œä¸ºé€‰é¡¹
            start_col_labels = [f"ç¬¬{i}åˆ— ({blind_df.columns[i-1]})" for i in start_col_options]

            selected_start_col = st.selectbox(
                "å…‰è°±æ³¢æ•°æ•°æ®ä»ç¬¬å‡ åˆ—å¼€å§‹ï¼Ÿ",
                start_col_options,
                index=2 if len(start_col_options) > 2 else 0,  # é»˜è®¤é€‰æ‹©ç¬¬3åˆ—
                format_func=lambda x: f"ç¬¬{x}åˆ— ({blind_df.columns[x-1]})"
            )

            st.info(f"å·²é€‰æ‹©ä»ç¬¬{selected_start_col}åˆ—å¼€å§‹æå–å…‰è°±æ•°æ®")

            # è¯†åˆ«æ³¢æ•°åˆ—
            try:
                # ä½¿ç”¨ç”¨æˆ·é€‰æ‹©çš„èµ·å§‹åˆ—
                potential_wavenumbers = blind_df.columns[selected_start_col-1:]  # è½¬æ¢ä¸º0åŸºç´¢å¼•
                numeric_columns = []
                
                for col in potential_wavenumbers:
                    try:
                        float(col)
                        numeric_columns.append(col)
                    except ValueError:
                        continue
                
                if len(numeric_columns) < 10:
                    st.error("æ£€æµ‹åˆ°çš„æ³¢æ•°åˆ—æ•°é‡ä¸è¶³ï¼Œè¯·æ£€æŸ¥æ•°æ®æ ¼å¼æˆ–é‡æ–°é€‰æ‹©èµ·å§‹åˆ—")
                    return
                
                blind_wavenumbers = pd.Series(numeric_columns).astype(float)
                st.info(f"ç›²æ ·æ•°æ®æ³¢æ•°èŒƒå›´: {blind_wavenumbers.min():.1f} ~ {blind_wavenumbers.max():.1f} cmâ»Â¹")
                st.info(f"æ£€æµ‹åˆ° {len(numeric_columns)} ä¸ªæ³¢æ•°åˆ—")
                
                # ä¸è®­ç»ƒæ•°æ®çš„æ³¢æ•°èŒƒå›´å¯¹æ¯”
                train_wavenumbers = st.session_state.wavenumbers
                st.info(f"è®­ç»ƒæ•°æ®æ³¢æ•°èŒƒå›´: {train_wavenumbers.min():.1f} ~ {train_wavenumbers.max():.1f} cmâ»Â¹")
                
                # æ£€æŸ¥æ³¢æ•°èŒƒå›´å…¼å®¹æ€§
                if (blind_wavenumbers.min() > train_wavenumbers.min() or 
                    blind_wavenumbers.max() < train_wavenumbers.max()):
                    st.warning("âš ï¸ ç›²æ ·æ•°æ®çš„æ³¢æ•°èŒƒå›´ä¸è®­ç»ƒæ•°æ®ä¸å®Œå…¨åŒ¹é…ï¼Œå¯èƒ½å½±å“é¢„æµ‹ç²¾åº¦")
                
                # æå–ç›²æ ·å…‰è°±æ•°æ®
                blind_spectra = blind_df[numeric_columns].values.astype(float)
                st.info(f"åŸå§‹ç›²æ ·å…‰è°±å½¢çŠ¶: {blind_spectra.shape}")
                
                # é€‰æ‹©è¦ä½¿ç”¨çš„æ¨¡å‹
                st.subheader("æ¨¡å‹é€‰æ‹©")
                model_names = list(st.session_state.trained_models.keys())
                available_models = {
                    'linear': 'çº¿æ€§å›å½’', 'ridge': 'å²­å›å½’', 'lasso': 'Lassoå›å½’',
                    'svr': 'æ”¯æŒå‘é‡å›å½’', 'rf': 'éšæœºæ£®æ—', 'gbr': 'æ¢¯åº¦æå‡å›å½’',
                    'mlp': 'å¤šå±‚æ„ŸçŸ¥æœº', 'pls': 'åæœ€å°äºŒä¹˜å›å½’', 'xgb': 'XGBoost'
                }
                
                selected_model_key = st.selectbox(
                    "é€‰æ‹©é¢„æµ‹æ¨¡å‹", 
                    model_names,
                    format_func=lambda x: available_models.get(x, x)
                )
                
                if st.button("è¿›è¡Œé¢„æµ‹"):
                    with st.spinner("æ­£åœ¨åº”ç”¨é¢„å¤„ç†æµç¨‹å¹¶è¿›è¡Œé¢„æµ‹..."):
                        try:
                            # åº”ç”¨é¢„å¤„ç†æµç¨‹
                            st.write("**é¢„å¤„ç†æ­¥éª¤:**")
                            
                           # â­ 1. é¦–å…ˆæˆªå–æ³¢æ•°èŒƒå›´ - æŒ‰ç…§é¢„å¤„ç†å‚æ•° â­
                            start_wn = params['start_wavenumber']
                            end_wn = params['end_wavenumber']

                            st.write(f"**é¢„å¤„ç†å‚æ•°ä¸­çš„æ³¢æ•°èŒƒå›´**: {start_wn} ~ {end_wn} cmâ»Â¹")

                            # åœ¨ç›²æ ·æ•°æ®ä¸­æ‰¾åˆ°å¯¹åº”çš„æ³¢æ•°èŒƒå›´
                            start_idx = np.argmin(np.abs(blind_wavenumbers - start_wn))
                            end_idx = np.argmin(np.abs(blind_wavenumbers - end_wn))

                            # ç¡®ä¿ç´¢å¼•é¡ºåºæ­£ç¡®ï¼ˆå¤„ç†é€’å‡æ³¢æ•°çš„æƒ…å†µï¼‰
                            if start_idx > end_idx:
                                start_idx, end_idx = end_idx, start_idx

                            # æˆªå–æ³¢æ•°å’Œå…‰è°±æ•°æ®
                            blind_wavenumbers_crop = blind_wavenumbers[start_idx:end_idx+1]
                            blind_X_crop = blind_spectra[:, start_idx:end_idx+1]

                            # è·å–è®­ç»ƒæ—¶çš„é¢„å¤„ç†åç‰¹å¾æ•°é‡
                            expected_feature_count = st.session_state.X_preprocessed.shape[1]
                            actual_feature_count = blind_X_crop.shape[1]

                            st.write(f"âœ“ æ³¢æ•°æˆªå–: {blind_wavenumbers_crop.min():.1f} ~ {blind_wavenumbers_crop.max():.1f} cmâ»Â¹")
                            st.write(f"âœ“ æˆªå–åå½¢çŠ¶: {blind_X_crop.shape}")
                            st.write(f"æˆªå–çš„ç‰¹å¾æ•°é‡: {actual_feature_count}ï¼ŒæœŸæœ›: {expected_feature_count}")

                            # å¦‚æœç‰¹å¾æ•°é‡ä¸åŒ¹é…ï¼Œè¿›è¡Œæ’å€¼è°ƒæ•´
                            if actual_feature_count != expected_feature_count:
                                st.warning(f"ç‰¹å¾æ•°é‡ä¸åŒ¹é…ï¼Œä½¿ç”¨æ’å€¼è°ƒæ•´: {actual_feature_count} â†’ {expected_feature_count}")
                                
                                from scipy.interpolate import interp1d
                                
                                # è·å–è®­ç»ƒæ—¶çš„æ³¢æ•°ç½‘æ ¼
                                if hasattr(st.session_state, 'wavenumbers_preprocessed'):
                                    train_wavenumbers_crop = st.session_state.wavenumbers_preprocessed
                                else:
                                    # é‡æ–°è®¡ç®—è®­ç»ƒæ—¶çš„æ³¢æ•°æˆªå–
                                    train_wavenumbers = st.session_state.wavenumbers
                                    train_start_idx = np.argmin(np.abs(train_wavenumbers - start_wn))
                                    train_end_idx = np.argmin(np.abs(train_wavenumbers - end_wn))
                                    if train_start_idx > train_end_idx:
                                        train_start_idx, train_end_idx = train_end_idx, train_start_idx
                                    train_wavenumbers_crop = train_wavenumbers[train_start_idx:train_end_idx+1]
                                
                                interpolated_spectra = np.zeros((blind_X_crop.shape[0], expected_feature_count))
                                
                                for i in range(blind_X_crop.shape[0]):
                                    # åˆ›å»ºæ’å€¼å‡½æ•°
                                    f = interp1d(blind_wavenumbers_crop, blind_X_crop[i], 
                                                kind='linear', bounds_error=False, fill_value='extrapolate')
                                    # æ’å€¼åˆ°è®­ç»ƒæ•°æ®çš„æ³¢æ•°ç½‘æ ¼
                                    interpolated_spectra[i] = f(train_wavenumbers_crop)
                                
                                blind_wavenumbers_crop = train_wavenumbers_crop
                                blind_X_crop = interpolated_spectra
                                
                                st.write(f"âœ“ æ’å€¼è°ƒæ•´å®Œæˆï¼Œæœ€ç»ˆå½¢çŠ¶: {blind_X_crop.shape}")

                            elif actual_feature_count > expected_feature_count:
                                st.warning(f"ç›²æ ·ç‰¹å¾æ•°é‡è¿‡å¤š({actual_feature_count} > {expected_feature_count})ï¼Œè¿›è¡Œé‡é‡‡æ ·")
                                
                                # ä½¿ç”¨æ’å€¼é‡é‡‡æ ·åˆ°è®­ç»ƒæ•°æ®çš„æ³¢æ•°ç½‘æ ¼
                                from scipy.interpolate import interp1d
                                
                                resampled_spectra = np.zeros((blind_X_crop.shape[0], expected_feature_count))
                                
                                for i in range(blind_X_crop.shape[0]):
                                    f = interp1d(blind_wavenumbers_crop, blind_X_crop[i], 
                                                kind='linear', bounds_error=False, fill_value='extrapolate')
                                    resampled_spectra[i] = f(train_wavenumbers_crop)
                                
                                blind_wavenumbers_crop = train_wavenumbers_crop
                                blind_X_crop = resampled_spectra
                                
                                st.write(f"âœ“ é‡é‡‡æ ·å®Œæˆï¼Œæœ€ç»ˆå½¢çŠ¶: {blind_X_crop.shape}")
                            
                            # 2. å¹³æ»‘å¤„ç†
                            if params.get('apply_smooth', True):
                                blind_X_smooth = np.zeros_like(blind_X_crop)
                                for i in range(blind_X_crop.shape[0]):
                                    blind_X_smooth[i] = savgol_filter(
                                        blind_X_crop[i], 
                                        params['smooth_window'], 
                                        params['smooth_poly']
                                    )
                                st.write(f"âœ“ Savitzky-Golayå¹³æ»‘: çª—å£={params['smooth_window']}, å¤šé¡¹å¼é˜¶æ•°={params['smooth_poly']}")
                            else:
                                blind_X_smooth = blind_X_crop.copy()
                                st.write("â—‹ è·³è¿‡å¹³æ»‘å¤„ç†")
                            
                            # 3. åŸºçº¿æ ¡æ­£
                            if params.get('apply_baseline', True):
                                corrector = SpectrumBaselineCorrector()
                                blind_X_corr = np.zeros_like(blind_X_smooth)
                                
                                baseline_method = params['baseline_method']
                                baseline_params = params['baseline_params']
                                
                                for i in range(blind_X_smooth.shape[0]):
                                    try:
                                        baseline, corrected = corrector.correct_baseline(
                                            blind_X_smooth[i], 
                                            baseline_method, 
                                            **baseline_params
                                        )
                                        blind_X_corr[i] = corrected
                                    except Exception as e:
                                        st.warning(f"æ ·æœ¬ {i+1} åŸºçº¿æ ¡æ­£å¤±è´¥ï¼Œä½¿ç”¨å¹³æ»‘åçš„æ•°æ®: {e}")
                                        blind_X_corr[i] = blind_X_smooth[i]
                                
                                st.write(f"âœ“ åŸºçº¿æ ¡æ­£: {baseline_method.upper()}ç®—æ³•")
                            else:
                                blind_X_corr = blind_X_smooth.copy()
                                st.write("â—‹ è·³è¿‡åŸºçº¿æ ¡æ­£")
                            
                            # 4. å½’ä¸€åŒ–
                            if params.get('apply_normalize', True):
                                blind_X_norm = np.zeros_like(blind_X_corr)
                                normalize_method = params['normalize_method']
                                
                                for i in range(blind_X_corr.shape[0]):
                                    spectrum = blind_X_corr[i]
                                    
                                    if normalize_method == 'area':
                                        # ä¿®æ­£ï¼šç›´æ¥ä½¿ç”¨åŸå§‹å…‰è°±è®¡ç®—é¢ç§¯ï¼Œä¸å–ç»å¯¹å€¼
                                        total_area = np.trapz(spectrum, blind_wavenumbers_crop)
                                        if abs(total_area) < 1e-12:  # é¿å…é™¤é›¶
                                            blind_X_norm[i] = spectrum
                                        else:
                                            blind_X_norm[i] = spectrum / total_area
                                            
                                    elif normalize_method == 'max':
                                        # ä¿®æ­£ï¼šæ‰¾åˆ°æœ€å¤§ç»å¯¹å€¼ï¼Œä½†ä¿æŒåŸå§‹ç¬¦å·
                                        max_abs_val = np.max(np.abs(spectrum))
                                        if max_abs_val < 1e-12:  # é¿å…é™¤é›¶
                                            blind_X_norm[i] = spectrum
                                        else:
                                            blind_X_norm[i] = spectrum / max_abs_val
                                            
                                    elif normalize_method == 'vector':
                                        # å‘é‡å½’ä¸€åŒ–ï¼ˆL2èŒƒæ•°ï¼‰
                                        norm_val = np.linalg.norm(spectrum)
                                        if norm_val < 1e-12:  # é¿å…é™¤é›¶
                                            blind_X_norm[i] = spectrum
                                        else:
                                            blind_X_norm[i] = spectrum / norm_val
                                            
                                    elif normalize_method == 'minmax':
                                        # æœ€å°-æœ€å¤§å½’ä¸€åŒ–åˆ°[0,1]
                                        min_val = np.min(spectrum)
                                        max_val = np.max(spectrum)
                                        if abs(max_val - min_val) < 1e-12:  # é¿å…é™¤é›¶
                                            blind_X_norm[i] = spectrum
                                        else:
                                            blind_X_norm[i] = (spectrum - min_val) / (max_val - min_val)
                                            
                                    elif normalize_method == 'std':
                                        # æ ‡å‡†åŒ–ï¼ˆé›¶å‡å€¼ï¼Œå•ä½æ–¹å·®ï¼‰
                                        mean_val = np.mean(spectrum)
                                        std_val = np.std(spectrum)
                                        if std_val < 1e-12:  # é¿å…é™¤é›¶
                                            blind_X_norm[i] = spectrum - mean_val
                                        else:
                                            blind_X_norm[i] = (spectrum - mean_val) / std_val
                                            
                                    else:
                                        # æœªçŸ¥æ–¹æ³•ï¼Œç›´æ¥å¤åˆ¶
                                        blind_X_norm[i] = spectrum
                                        st.warning(f"æœªçŸ¥å½’ä¸€åŒ–æ–¹æ³•: {normalize_method}")
                                
                                st.write(f"âœ“ å½’ä¸€åŒ–: {normalize_method}æ–¹æ³•")
                            else:
                                blind_X_norm = blind_X_corr.copy()
                                st.write("â—‹ è·³è¿‡å½’ä¸€åŒ–")
                            
                            # 5. SNVå¤„ç†
                            if params.get('apply_snv', False):
                                blind_X_preprocessed = np.zeros_like(blind_X_norm)
                                for i, spectrum in enumerate(blind_X_norm):
                                    mean_val = np.mean(spectrum)
                                    std_val = np.std(spectrum)
                                    if std_val == 0:
                                        blind_X_preprocessed[i] = spectrum
                                    else:
                                        blind_X_preprocessed[i] = (spectrum - mean_val) / std_val
                                st.write("âœ“ æ ‡å‡†æ­£æ€å˜é‡å˜æ¢(SNV): å·²åº”ç”¨")
                            else:
                                blind_X_preprocessed = blind_X_norm
                                st.write("â—‹ æ ‡å‡†æ­£æ€å˜é‡å˜æ¢(SNV): æœªåº”ç”¨")
                            
                            # â­ å…³é”®ä¿®å¤ï¼šåº”ç”¨ç‰¹å¾é€‰æ‹© â­
                            if hasattr(st.session_state, 'feature_selected') and st.session_state.feature_selected:
                                if st.session_state.feature_selection_method != "ä¸è¿›è¡Œç‰¹å¾é€‰æ‹©":
                                    # åº”ç”¨è®­ç»ƒæ—¶çš„ç‰¹å¾é€‰æ‹©
                                    selected_indices = st.session_state.selected_features
                                    blind_X_final = blind_X_preprocessed[:, selected_indices]
                                    st.write(f"âœ“ åº”ç”¨ç‰¹å¾é€‰æ‹©: {st.session_state.feature_selection_method}")
                                    st.write(f"ç‰¹å¾æ•°é‡: {blind_X_preprocessed.shape[1]} â†’ {blind_X_final.shape[1]}")
                                else:
                                    blind_X_final = blind_X_preprocessed
                                    st.write("â—‹ æœªè¿›è¡Œç‰¹å¾é€‰æ‹©")
                            else:
                                blind_X_final = blind_X_preprocessed
                                st.write("â—‹ æœªè¿›è¡Œç‰¹å¾é€‰æ‹©")
                            
                            # ç¡®å®šæœŸæœ›çš„ç‰¹å¾æ•°é‡
                            if hasattr(st.session_state, 'feature_selected') and st.session_state.feature_selected:
                                expected_features = st.session_state.X_final.shape[1]
                                data_source = "ç‰¹å¾é€‰æ‹©å"
                            else:
                                expected_features = st.session_state.X_preprocessed.shape[1]
                                data_source = "é¢„å¤„ç†å"
                            
                            st.write(f"**{data_source}ç‰¹å¾æ•°é‡**: {blind_X_final.shape[1]}")
                            st.write(f"**æ¨¡å‹æœŸæœ›ç‰¹å¾æ•°é‡**: {expected_features}")
                            
                            if blind_X_final.shape[1] != expected_features:
                                st.error(f"ç‰¹å¾æ•°é‡ä¸åŒ¹é…ï¼ç›²æ ·: {blind_X_final.shape[1]}, æœŸæœ›: {expected_features}")
                                
                                if blind_X_final.shape[1] > expected_features:
                                    st.warning("ç›²æ ·ç‰¹å¾æ•°é‡è¿‡å¤šï¼Œå°†æˆªå–å‰é¢çš„ç‰¹å¾")
                                    blind_X_final = blind_X_final[:, :expected_features]
                                    st.info(f"å·²è°ƒæ•´ä¸º: {blind_X_final.shape[1]} ä¸ªç‰¹å¾")
                                else:
                                    st.error("ç›²æ ·ç‰¹å¾æ•°é‡ä¸è¶³ï¼Œæ— æ³•è‡ªåŠ¨è°ƒæ•´")
                                    st.error("å¯èƒ½çš„åŸå› ï¼š")
                                    st.error("1. ç›²æ ·æ•°æ®çš„æ³¢æ•°èŒƒå›´ä¸è®­ç»ƒæ•°æ®ä¸åŒ¹é…")
                                    st.error("2. é¢„å¤„ç†å‚æ•°è®¾ç½®ä¸åŒ")
                                    st.error("3. ç‰¹å¾é€‰æ‹©è¿‡ç¨‹æœ‰å·®å¼‚")
                                    return
                            
                            # åº”ç”¨æ ‡å‡†åŒ–ï¼ˆå¦‚æœéœ€è¦ï¼‰
                            if selected_model_key in st.session_state.detailed_results:
                                scaler = st.session_state.detailed_results[selected_model_key].get('scaler')
                                if scaler is not None:
                                    blind_X_final = scaler.transform(blind_X_final)
                                    st.write("âœ“ åº”ç”¨è®­ç»ƒæ—¶çš„æ ‡å‡†åŒ–å˜æ¢")
                            
                            # ä½¿ç”¨é€‰å®šçš„æ¨¡å‹è¿›è¡Œé¢„æµ‹
                            model = st.session_state.trained_models[selected_model_key]
                            predictions = model.predict(blind_X_final)
                            
                            # æ˜¾ç¤ºé¢„æµ‹ç»“æœ
                            st.subheader("é¢„æµ‹ç»“æœ")

                            # 1. æ„å»ºç»“æœDataFrameï¼Œå¹¶ç¡®ä¿åˆ—çš„é¡ºåºæ­£ç¡®
                            result_df = pd.DataFrame()

                            # 2. ä»åŸå§‹ç›²æ ·æ•°æ®ä¸­æå–æ ‡è¯†åˆ— (æ ¹æ®å…‰è°±èµ·å§‹åˆ—åŠ¨æ€è°ƒæ•´)
                            for i in range(selected_start_col - 1):  # æå–å…‰è°±æ•°æ®ä¹‹å‰çš„æ‰€æœ‰åˆ—ä½œä¸ºæ ‡è¯†åˆ—
                                if i < blind_df.shape[1]:
                                    result_df[blind_df.columns[i]] = blind_df.iloc[:, i]

                            # 3. æ·»åŠ é¢„æµ‹ç»“æœ
                            if predictions.ndim == 1:
                                # å•ç›®æ ‡é¢„æµ‹
                                pred_col_name = st.session_state.selected_cols[0] if len(st.session_state.selected_cols) == 1 else 'é¢„æµ‹å€¼'
                                result_df[f'{pred_col_name}_é¢„æµ‹å€¼'] = predictions
                            else:
                                # å¤šç›®æ ‡é¢„æµ‹
                                for i, col in enumerate(st.session_state.selected_cols):
                                    result_df[f'{col}_é¢„æµ‹å€¼'] = predictions[:, i]

                            # 4. åœ¨æœ€å‰é¢æ’å…¥ä¸€ä¸ªä»1å¼€å§‹çš„æ ·æœ¬ç´¢å¼•ï¼Œæ–¹ä¾¿æŸ¥çœ‹
                            result_df.insert(0, 'æ ·æœ¬ç´¢å¼•', np.arange(1, len(predictions) + 1))

                            # 5. æ˜¾ç¤ºæœ€ç»ˆçš„DataFrame
                            st.dataframe(result_df, use_container_width=True)
                            
                            # å¯è§†åŒ–é¢„æµ‹ç»“æœ
                            if predictions.ndim == 1:
                                fig, ax = plt.subplots(figsize=(10, 6))
                                ax.plot(result_df['æ ·æœ¬ç´¢å¼•'], predictions, 'o-', color='blue', markersize=6)
                                ax.set_title('ç›²æ ·é¢„æµ‹ç»“æœ')
                                ax.set_xlabel('æ ·æœ¬ç´¢å¼•')
                                ax.set_ylabel('é¢„æµ‹å€¼')
                                ax.grid(True, linestyle='--', alpha=0.7)
                                plt.tight_layout()
                                st.pyplot(fig)
                            
                            
                            # æä¾›ä¸‹è½½é“¾æ¥
                            csv = result_df.to_csv(index=False, encoding='utf-8-sig')
                            b64 = base64.b64encode(csv.encode('utf-8')).decode()
                            href = f'<a href="data:file/csv;base64,{b64}" download="ç›²æ ·é¢„æµ‹ç»“æœ.csv">ğŸ“¥ ä¸‹è½½é¢„æµ‹ç»“æœ</a>'
                            st.markdown(href, unsafe_allow_html=True)

                            # â­ ä¿å­˜é¢„æµ‹ç»“æœåˆ° session_state â­
                            st.session_state.blind_prediction_results = {
                                'predictions': predictions,
                                'blind_X_final': blind_X_final,
                                'result_df': result_df,
                                'prediction_completed': True
                            }

                            # ç¡®å®šæ˜¾ç¤ºç”¨çš„æ³¢æ•°
                            if hasattr(st.session_state, 'feature_selected') and st.session_state.feature_selected:
                                if st.session_state.feature_selection_method != "ä¸è¿›è¡Œç‰¹å¾é€‰æ‹©":
                                    display_wavenumbers = st.session_state.wavenumbers_final
                                else:
                                    display_wavenumbers = blind_wavenumbers_crop
                            else:
                                display_wavenumbers = blind_wavenumbers_crop

                            st.session_state.blind_prediction_results['display_wavenumbers'] = display_wavenumbers

                            st.success("é¢„æµ‹å®Œæˆï¼")

                        except Exception as e:
                            st.error(f"é¢„æµ‹è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
                            st.error(traceback.format_exc())
                
                # â­ å…‰è°±æ˜¾ç¤ºä»£ç æ”¾åœ¨è¿™é‡Œï¼ˆä¸ if st.button("è¿›è¡Œé¢„æµ‹"): åŒçº§ï¼‰â­
                if (hasattr(st.session_state, 'blind_prediction_results') and 
                    st.session_state.blind_prediction_results.get('prediction_completed', False)):
                    
                    # ä» session_state è·å–æ•°æ®
                    blind_results = st.session_state.blind_prediction_results
                    blind_X_final = blind_results['blind_X_final']
                    display_wavenumbers = blind_results['display_wavenumbers']
                    
                    # æ˜¾ç¤ºé¢„å¤„ç†åçš„å…‰è°±
                    if st.checkbox("æŸ¥çœ‹é¢„å¤„ç†åçš„ç›²æ ·å…‰è°±"):
                        fig, ax = plt.subplots(figsize=(12, 6))
                        n_samples = min(10, blind_X_final.shape[0])
                        
                        for i in range(n_samples):
                            ax.plot(display_wavenumbers, blind_X_final[i], alpha=0.7)
                        ax.set_title(f'é¢„å¤„ç†åçš„ç›²æ ·å…‰è°± (æ˜¾ç¤ºå‰{n_samples}ä¸ªæ ·æœ¬)')
                        ax.set_xlabel('æ³¢æ•° (cmâ»Â¹)')
                        ax.set_ylabel('å¤„ç†åå¼ºåº¦')
                        ax.grid(True, linestyle='--', alpha=0.7)
                        plt.tight_layout()
                        st.pyplot(fig)
                        
            except Exception as e:
                st.error(f"å¤„ç†ç›²æ ·æ•°æ®æ—¶å‡ºé”™: {e}")
                st.error(traceback.format_exc())
                
        except Exception as e:
            st.error(f"è¯»å–ç›²æ ·æ•°æ®å‡ºé”™: {e}")
            st.error(traceback.format_exc())
    else:
        st.info("è¯·ä¸Šä¼ ç›²æ ·æ•°æ®æ–‡ä»¶è¿›è¡Œé¢„æµ‹")
        
        # æ˜¾ç¤ºä½¿ç”¨è¯´æ˜
        st.subheader("ä½¿ç”¨è¯´æ˜")
        st.markdown("""
        **ç›²æ ·æ•°æ®æ ¼å¼è¦æ±‚ï¼š**
        1. æ”¯æŒCSVã€Excelæ ¼å¼
        2. æ•°æ®æ ¼å¼åº”ä¸è®­ç»ƒæ•°æ®ä¿æŒä¸€è‡´
        3. å¯è‡ªå®šä¹‰å…‰è°±æ•°æ®èµ·å§‹åˆ—
        4. å…‰è°±èµ·å§‹åˆ—ä¹‹å‰çš„åˆ—ä¸ºæ ·æœ¬æ ‡è¯†ä¿¡æ¯
        5. æ³¢æ•°èŒƒå›´åº”åŒ…å«è®­ç»ƒæ•°æ®çš„æ³¢æ•°èŒƒå›´
        
        **é¢„å¤„ç†æµç¨‹ï¼š**
        - ç³»ç»Ÿä¼šè‡ªåŠ¨åº”ç”¨ä¸è®­ç»ƒæ•°æ®ç›¸åŒçš„é¢„å¤„ç†æ­¥éª¤
        - åŒ…æ‹¬æ³¢æ•°æˆªå–ã€å¹³æ»‘ã€åŸºçº¿æ ¡æ­£ã€å½’ä¸€åŒ–ç­‰
        - **ç‰¹å¾é€‰æ‹©**ï¼šå¦‚æœè®­ç»ƒæ—¶è¿›è¡Œäº†ç‰¹å¾é€‰æ‹©ï¼Œä¼šè‡ªåŠ¨åº”ç”¨ç›¸åŒçš„ç‰¹å¾é€‰æ‹©
        - ç¡®ä¿ç‰¹å¾æ•°é‡ä¸è®­ç»ƒæ¨¡å‹åŒ¹é…
        
        **æ³¨æ„äº‹é¡¹ï¼š**
        - ç›²æ ·æ•°æ®çš„æ³¢æ•°èŒƒå›´åº”è¦†ç›–è®­ç»ƒæ•°æ®çš„æ³¢æ•°èŒƒå›´
        - å…‰è°±æ•°æ®çš„ä»ªå™¨æ¡ä»¶åº”ä¸è®­ç»ƒæ•°æ®ä¸€è‡´
        - é¢„æµ‹ç»“æœçš„å¯é æ€§å–å†³äºç›²æ ·ä¸è®­ç»ƒæ•°æ®çš„ç›¸ä¼¼æ€§
        - å¦‚æœè®­ç»ƒæ—¶è¿›è¡Œäº†ç‰¹å¾é€‰æ‹©ï¼Œç›²æ ·é¢„æµ‹ä¼šè‡ªåŠ¨åº”ç”¨ç›¸åŒçš„ç‰¹å¾é€‰æ‹©æ­¥éª¤
        """)


# ====================================
# ç¼ºå¤±å‡½æ•°å®šä¹‰è¡¥å……
# ====================================


def calculate_model_metrics(y_train, y_test, train_pred, test_pred, is_multioutput):
    """
    è®¡ç®—æ¨¡å‹è¯„ä¼°æŒ‡æ ‡
    
    Args:
        y_train: è®­ç»ƒé›†çœŸå®å€¼
        y_test: æµ‹è¯•é›†çœŸå®å€¼
        train_pred: è®­ç»ƒé›†é¢„æµ‹å€¼
        test_pred: æµ‹è¯•é›†é¢„æµ‹å€¼
        is_multioutput: æ˜¯å¦ä¸ºå¤šè¾“å‡ºé—®é¢˜
    
    Returns:
        dict: åŒ…å«å„ç§è¯„ä¼°æŒ‡æ ‡çš„å­—å…¸
    """
    metrics = {}
    
    if is_multioutput:
        # å¤šè¾“å‡ºæƒ…å†µï¼šè®¡ç®—å¹³å‡æŒ‡æ ‡
        train_r2_scores = []
        test_r2_scores = []
        train_rmse_scores = []
        test_rmse_scores = []
        train_mae_scores = []
        test_mae_scores = []
        
        for i in range(y_train.shape[1]):
            # è®­ç»ƒé›†æŒ‡æ ‡
            train_r2 = r2_score(y_train[:, i], train_pred[:, i])
            train_rmse = np.sqrt(mean_squared_error(y_train[:, i], train_pred[:, i]))
            train_mae = mean_absolute_error(y_train[:, i], train_pred[:, i])
            
            # æµ‹è¯•é›†æŒ‡æ ‡
            test_r2 = r2_score(y_test[:, i], test_pred[:, i])
            test_rmse = np.sqrt(mean_squared_error(y_test[:, i], test_pred[:, i]))
            test_mae = mean_absolute_error(y_test[:, i], test_pred[:, i])
            
            train_r2_scores.append(train_r2)
            test_r2_scores.append(test_r2)
            train_rmse_scores.append(train_rmse)
            test_rmse_scores.append(test_rmse)
            train_mae_scores.append(train_mae)
            test_mae_scores.append(test_mae)
        
        # è®¡ç®—å¹³å‡æŒ‡æ ‡
        metrics['Train RÂ²'] = np.mean(train_r2_scores)
        metrics['Test RÂ²'] = np.mean(test_r2_scores)
        metrics['Train RMSE'] = np.mean(train_rmse_scores)
        metrics['Test RMSE'] = np.mean(test_rmse_scores)
        metrics['Train MAE'] = np.mean(train_mae_scores)
        metrics['Test MAE'] = np.mean(test_mae_scores)
        
    else:
        # å•è¾“å‡ºæƒ…å†µ
        metrics['Train RÂ²'] = r2_score(y_train, train_pred)
        metrics['Test RÂ²'] = r2_score(y_test, test_pred)
        metrics['Train RMSE'] = np.sqrt(mean_squared_error(y_train, train_pred))
        metrics['Test RMSE'] = np.sqrt(mean_squared_error(y_test, test_pred))
        metrics['Train MAE'] = mean_absolute_error(y_train, train_pred)
        metrics['Test MAE'] = mean_absolute_error(y_test, test_pred)
    
    return metrics


def calculate_multioutput_details(y_train, y_test, train_pred, test_pred):
    """
    è®¡ç®—å¤šè¾“å‡ºæ¨¡å‹çš„è¯¦ç»†æŒ‡æ ‡
    
    Args:
        y_train: è®­ç»ƒé›†çœŸå®å€¼
        y_test: æµ‹è¯•é›†çœŸå®å€¼
        train_pred: è®­ç»ƒé›†é¢„æµ‹å€¼
        test_pred: æµ‹è¯•é›†é¢„æµ‹å€¼
    
    Returns:
        dict: åŒ…å«æ¯ä¸ªç›®æ ‡å˜é‡è¯¦ç»†æŒ‡æ ‡çš„å­—å…¸
    """
    details = {}
    
    train_r2_per_target = []
    test_r2_per_target = []
    train_rmse_per_target = []
    test_rmse_per_target = []
    train_mae_per_target = []
    test_mae_per_target = []
    
    for i in range(y_train.shape[1]):
        # æ¯ä¸ªç›®æ ‡å˜é‡çš„æŒ‡æ ‡
        train_r2 = r2_score(y_train[:, i], train_pred[:, i])
        test_r2 = r2_score(y_test[:, i], test_pred[:, i])
        train_rmse = np.sqrt(mean_squared_error(y_train[:, i], train_pred[:, i]))
        test_rmse = np.sqrt(mean_squared_error(y_test[:, i], test_pred[:, i]))
        train_mae = mean_absolute_error(y_train[:, i], train_pred[:, i])
        test_mae = mean_absolute_error(y_test[:, i], test_pred[:, i])
        
        train_r2_per_target.append(train_r2)
        test_r2_per_target.append(test_r2)
        train_rmse_per_target.append(train_rmse)
        test_rmse_per_target.append(test_rmse)
        train_mae_per_target.append(train_mae)
        test_mae_per_target.append(test_mae)
    
    details['train_r2_per_target'] = train_r2_per_target
    details['test_r2_per_target'] = test_r2_per_target
    details['train_rmse_per_target'] = train_rmse_per_target
    details['test_rmse_per_target'] = test_rmse_per_target
    details['train_mae_per_target'] = train_mae_per_target
    details['test_mae_per_target'] = test_mae_per_target
    
    return details


def display_training_results(results, trained_models, detailed_results, is_multioutput):
    """
    æ˜¾ç¤ºè®­ç»ƒç»“æœ
    
    Args:
        results: ç»“æœåˆ—è¡¨
        trained_models: è®­ç»ƒå¥½çš„æ¨¡å‹å­—å…¸
        detailed_results: è¯¦ç»†ç»“æœå­—å…¸
        is_multioutput: æ˜¯å¦ä¸ºå¤šè¾“å‡ºé—®é¢˜
    """
    st.session_state.trained_models = trained_models
    st.session_state.detailed_results = detailed_results
    
    # è½¬æ¢ä¸ºDataFrameå¹¶æ˜¾ç¤º
    results_df = pd.DataFrame(results)
    
    st.success("ğŸ‰ æ¨¡å‹è®­ç»ƒå®Œæˆï¼")
    
    # æ˜¾ç¤ºç»“æœè¡¨æ ¼
    st.subheader("ğŸ“Š æ¨¡å‹æ€§èƒ½å¯¹æ¯”")
    
    # æ ¼å¼åŒ–æ•°å€¼æ˜¾ç¤º
    display_df = results_df.copy()
    numeric_cols = ['Training Time (s)', 'Train RÂ²', 'Test RÂ²', 'Train RMSE', 'Test RMSE', 'Train MAE', 'Test MAE']
    
    for col in numeric_cols:
        if col in display_df.columns:
            if col == 'Training Time (s)':
                display_df[col] = display_df[col].apply(lambda x: f"{x:.3f}s")
            else:
                display_df[col] = display_df[col].apply(lambda x: f"{x:.4f}")
    
    # æ·»åŠ äº¤å‰éªŒè¯ç»“æœæ ¼å¼åŒ–
    cv_cols = ['CV RÂ² Mean', 'CV RÂ² Std']
    for col in cv_cols:
        if col in display_df.columns:
            display_df[col] = display_df[col].apply(lambda x: f"{x:.4f}")
    
    st.dataframe(display_df, use_container_width=True)
    
    # æ‰¾å‡ºæœ€ä½³æ¨¡å‹
    best_model_idx = results_df['Test RÂ²'].idxmax()
    best_model = results_df.loc[best_model_idx, 'Model']
    best_r2 = results_df.loc[best_model_idx, 'Test RÂ²']
    
    st.success(f"ğŸ† æœ€ä½³æ¨¡å‹: **{best_model}** (æµ‹è¯•é›† RÂ² = {best_r2:.4f})")
    
    # æ˜¾ç¤ºå¯è§†åŒ–
    show_model_performance_visualization(results_df, detailed_results, is_multioutput)


def show_status_message(message, message_type="info"):
    """
    æ˜¾ç¤ºçŠ¶æ€æ¶ˆæ¯
    
    Args:
        message: æ¶ˆæ¯å†…å®¹
        message_type: æ¶ˆæ¯ç±»å‹ ("info", "success", "warning", "error")
    """
    if message_type == "info":
        st.info(message)
    elif message_type == "success":
        st.success(message)
    elif message_type == "warning":
        st.warning(message)
    elif message_type == "error":
        st.error(message)
    else:
        st.write(message)


def check_data_prerequisites(need_labels=False, need_preprocessing=True):
    """
    æ£€æŸ¥æ•°æ®å‰ç½®æ¡ä»¶
    
    Args:
        need_labels: æ˜¯å¦éœ€è¦æ ‡ç­¾æ•°æ®
        need_preprocessing: æ˜¯å¦éœ€è¦é¢„å¤„ç†å®Œæˆ
    
    Returns:
        bool: æ˜¯å¦æ»¡è¶³æ¡ä»¶
    """
    # æ£€æŸ¥æ•°æ®æ˜¯å¦åŠ è½½
    if not hasattr(st.session_state, 'data_loaded') or not st.session_state.data_loaded:
        show_status_message("è¯·å…ˆåŠ è½½æ•°æ®", "warning")
        return False
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦æ ‡ç­¾æ•°æ®
    if need_labels:
        if not hasattr(st.session_state, 'y') or st.session_state.y is None:
            show_status_message("æ­¤åŠŸèƒ½éœ€è¦æ ‡ç­¾æ•°æ®ï¼Œè¯·åœ¨æ•°æ®åŠ è½½é¡µé¢è¾“å…¥æ ‡ç­¾", "warning")
            return False
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦é¢„å¤„ç†å®Œæˆ
    if need_preprocessing:
        if not hasattr(st.session_state, 'preprocessing_done') or not st.session_state.preprocessing_done:
            show_status_message("è¯·å…ˆå®Œæˆæ•°æ®é¢„å¤„ç†", "warning")
            return False
    
    return True


def get_current_data():
    """
    è·å–å½“å‰ä½¿ç”¨çš„æ•°æ®
    
    Returns:
        tuple: (X, wavenumbers, info_message)
    """
    if hasattr(st.session_state, 'feature_selected') and st.session_state.feature_selected:
        X = st.session_state.X_final
        wavenumbers = st.session_state.wavenumbers_final
        info = f"âœ… ä½¿ç”¨ç‰¹å¾é€‰æ‹©åçš„æ•°æ®ï¼Œç‰¹å¾æ•°é‡: {X.shape[1]}"
    elif hasattr(st.session_state, 'preprocessing_done') and st.session_state.preprocessing_done:
        X = st.session_state.X_preprocessed
        wavenumbers = st.session_state.wavenumbers_preprocessed
        info = f"â„¹ï¸ ä½¿ç”¨é¢„å¤„ç†åçš„å…¨éƒ¨ç‰¹å¾ï¼Œç‰¹å¾æ•°é‡: {X.shape[1]}"
    else:
        X = st.session_state.X
        wavenumbers = st.session_state.wavenumbers
        info = f"âš ï¸ ä½¿ç”¨åŸå§‹æ•°æ®ï¼Œç‰¹å¾æ•°é‡: {X.shape[1]}"
    # ç¡®ä¿è¿”å›numpyæ•°ç»„
    if isinstance(X, pd.DataFrame):
        X = X.values
    if isinstance(wavenumbers, pd.Series):
        wavenumbers = wavenumbers.values
    
    return X, wavenumbers, info


def safe_execute(func, error_message="æ“ä½œå¤±è´¥"):
    """
    å®‰å…¨æ‰§è¡Œå‡½æ•°ï¼Œæ•è·å¼‚å¸¸
    
    Args:
        func: è¦æ‰§è¡Œçš„å‡½æ•°
        error_message: é”™è¯¯æ¶ˆæ¯
    
    Returns:
        å‡½æ•°æ‰§è¡Œç»“æœæˆ–None
    """
    try:
        return func()
    except Exception as e:
        st.error(f"{error_message}: {str(e)}")
        with st.expander("æŸ¥çœ‹è¯¦ç»†é”™è¯¯ä¿¡æ¯"):
            st.code(traceback.format_exc())
        return None


# ====================================
# ä¸»å‡½æ•°
# ====================================

def main_with_trend_analysis():
    """å¸¦è¶‹åŠ¿åˆ†æçš„ä¸»å‡½æ•°"""
    # è®¾ç½®é¡µé¢é…ç½®
    st.set_page_config(
        page_title="å’¸æ•°å…‰è°±æ•°æ®åˆ†æä¸é¢„æµ‹",
        page_icon="ğŸ”¬",
        layout="wide",
        initial_sidebar_state="expanded"
    )
    
    # è®¾ç½®é¡µé¢æ ·å¼
    set_page_style()
    
    # åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
    init_session_state()
    
    # ä¾§è¾¹æ å¯¼èˆª
    st.sidebar.title("ğŸ”¬ å’¸æ•°å…‰è°±æ•°æ®åˆ†æä¸é¢„æµ‹")
    st.sidebar.markdown("---")
    
    # æ›´æ–°é¡µé¢å­—å…¸ï¼Œæ·»åŠ è¶‹åŠ¿åˆ†æ
    pages = {
        "1. æ•°æ®åŠ è½½ä¸æ ‡ç­¾è¾“å…¥": show_data_loading_page,
        "2. æ•°æ®é¢„å¤„ç†": show_preprocessing_page,
        "3. ç‰¹å¾æå–ä¸å¯è§†åŒ–": show_feature_extraction_page,
        "4. è¶‹åŠ¿åˆ†æ": show_trend_analysis_page,  # æ–°å¢é¡µé¢
        "5. æ•°æ®é›†åˆ’åˆ†": show_data_split_page,
        "6. æ¨¡å‹è®­ç»ƒä¸è¯„ä¼°": show_model_training_page,
        "7. ç›²æ ·é¢„æµ‹": show_blind_prediction_page
    }
    
    # é¡µé¢é€‰æ‹©
    selection = st.sidebar.radio("å¯¼èˆª", list(pages.keys()))
    
    # æ˜¾ç¤ºæ•°æ®åŠ è½½çŠ¶æ€
    st.sidebar.markdown("---")
    if hasattr(st.session_state, 'data_loaded') and st.session_state.data_loaded:
        st.sidebar.success("âœ… æ•°æ®å·²åŠ è½½")
        if hasattr(st.session_state, 'X'):
            st.sidebar.write(f"ğŸ“Š å…‰è°±æ•°æ®: {st.session_state.X.shape}")
            if hasattr(st.session_state, 'y') and st.session_state.y is not None:
                st.sidebar.write(f"ğŸ·ï¸ æ ‡ç­¾æ•°æ®: {st.session_state.y.shape}")
                st.sidebar.write(f"ğŸ¯ ç›®æ ‡å˜é‡: {', '.join(st.session_state.selected_cols)}")
            else:
                st.sidebar.info("ğŸ” æ— æ ‡ç­¾æ•°æ® - å¯è¿›è¡Œè¶‹åŠ¿åˆ†æ")
    else:
        st.sidebar.warning("âš ï¸ è¯·å…ˆåŠ è½½æ•°æ®")
    
    # æ˜¾ç¤ºé¢„å¤„ç†çŠ¶æ€
    if hasattr(st.session_state, 'preprocessing_done') and st.session_state.preprocessing_done:
        st.sidebar.success("âœ… æ•°æ®é¢„å¤„ç†å®Œæˆ")
    
    # æ˜¾ç¤ºæ¨¡å‹è®­ç»ƒçŠ¶æ€
    if hasattr(st.session_state, 'trained_models') and st.session_state.trained_models:
        st.sidebar.success(f"âœ… å·²è®­ç»ƒ {len(st.session_state.trained_models)} ä¸ªæ¨¡å‹")
    
    # æ˜¾ç¤ºä½œè€…ä¿¡æ¯
    st.sidebar.markdown("---")
    st.sidebar.info(
        """
        **å’¸æ•°å…‰è°±æ•°æ®åˆ†æä¸é¢„æµ‹åº”ç”¨ v2.1**
        
        æ–°å¢åŠŸèƒ½ï¼š
        - ğŸ” **è¶‹åŠ¿åˆ†æ**: ä¸»å‰¯äº§ç‰©æµ“åº¦è¶‹åŠ¿
        - ğŸ“Š **PCAåˆ†æ**: é™ç»´å’Œæ¨¡å¼è¯†åˆ«
        - ğŸ§ª **æˆåˆ†åˆ†è§£**: NMF/ICAåˆ†ç¦»åŒ–å­¦æˆåˆ†
        - ğŸ“ˆ **æ—¶é—´è¶‹åŠ¿**: å·¥è‰ºè¿‡ç¨‹ç›‘æ§
        - ğŸ” **èšç±»åˆ†æ**: æ ·æœ¬åˆ†ç»„å’Œå¼‚å¸¸æ£€æµ‹
        - ğŸ“‹ **ç»¼åˆæŠ¥å‘Š**: è‡ªåŠ¨ç”Ÿæˆåˆ†ææŠ¥å‘Š
        
        **é€‚ç”¨åœºæ™¯**ï¼š
        - æœ‰æ ‡ç­¾æ•°æ®ï¼šå®šé‡é¢„æµ‹æ¨¡å‹
        - æ— æ ‡ç­¾æ•°æ®ï¼šè¶‹åŠ¿åˆ†æå’Œæˆåˆ†è¯†åˆ«
        - å·¥è‰ºç›‘æ§ï¼šå®æ—¶è¶‹åŠ¿è·Ÿè¸ª
        - ååº”è¿‡ç¨‹ï¼šä¸»å‰¯äº§ç‰©åˆ†æ
        """
    )
    
    # æ˜¾ç¤ºé€‰å®šçš„é¡µé¢
    page_func = pages[selection]
    page_func()


# åœ¨åŸä»£ç æœ€åæ›¿æ¢mainå‡½æ•°è°ƒç”¨
if __name__ == "__main__":
    main_with_trend_analysis()  # ä½¿ç”¨æ–°çš„mainå‡½æ•°
